{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b87dc04c",
   "metadata": {},
   "source": [
    "# Tier 1: Correlation Analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** 2497f608-0610-4ae4-bc4e-27d77a0f0af0\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 1: Correlation Analysis,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** 2497f608-0610-4ae4-bc4e-27d77a0f0af0\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858e6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Essential Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 1: Correlation Analysis - Libraries Loaded Successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Available Correlation Methods:\")\n",
    "print(\"• Pearson Correlation - Linear relationships (parametric)\")\n",
    "print(\"• Spearman Correlation - Monotonic relationships (non-parametric)\")\n",
    "print(\"• Kendall's Tau - Rank-based relationships (robust)\")\n",
    "print(\"• Partial Correlation - Controlling for confounders\")\n",
    "print(\"• Point-Biserial - Continuous vs Binary variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Comprehensive Sample Dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create realistic economic/business dataset\n",
    "n_samples = 1000\n",
    "\n",
    "# Base economic indicators\n",
    "gdp_growth = np.random.normal(2.5, 1.2, n_samples)\n",
    "unemployment = 8 - 0.5 * gdp_growth + np.random.normal(0, 0.8, n_samples)\n",
    "inflation = 2 + 0.3 * gdp_growth + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# Consumer and business metrics\n",
    "consumer_confidence = 50 + 5 * gdp_growth - 2 * unemployment + np.random.normal(0, 5, n_samples)\n",
    "stock_returns = 0.08 + 0.4 * gdp_growth - 0.2 * unemployment + np.random.normal(0, 0.15, n_samples)\n",
    "interest_rates = 3 + 0.6 * inflation + np.random.normal(0, 0.3, n_samples)\n",
    "\n",
    "# Retail and housing\n",
    "retail_sales = 100 + 10 * consumer_confidence/10 + 5 * gdp_growth + np.random.normal(0, 8, n_samples)\n",
    "housing_prices = 250000 + 15000 * gdp_growth - 8000 * interest_rates + np.random.normal(0, 20000, n_samples)\n",
    "\n",
    "# Technology and innovation metrics\n",
    "tech_investment = 50 + 8 * gdp_growth + 0.1 * stock_returns * 1000 + np.random.normal(0, 10, n_samples)\n",
    "productivity = 100 + 2 * tech_investment/10 + np.random.normal(0, 5, n_samples)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    " 'GDP_Growth': gdp_growth,\n",
    " 'Unemployment_Rate': unemployment,\n",
    " 'Inflation_Rate': inflation,\n",
    " 'Consumer_Confidence': consumer_confidence,\n",
    " 'Stock_Returns': stock_returns * 100, # Convert to percentage\n",
    " 'Interest_Rates': interest_rates,\n",
    " 'Retail_Sales_Index': retail_sales,\n",
    " 'Housing_Price_Index': housing_prices / 1000, # In thousands\n",
    " 'Tech_Investment': tech_investment,\n",
    " 'Productivity_Index': productivity\n",
    "})\n",
    "\n",
    "# Ensure realistic bounds\n",
    "df['Unemployment_Rate'] = np.clip(df['Unemployment_Rate'], 1, 15)\n",
    "df['Inflation_Rate'] = np.clip(df['Inflation_Rate'], -2, 8)\n",
    "df['Consumer_Confidence'] = np.clip(df['Consumer_Confidence'], 0, 100)\n",
    "df['Interest_Rates'] = np.clip(df['Interest_Rates'], 0, 10)\n",
    "\n",
    "print(\" Economic Dataset Generated Successfully!\")\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(df.head())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PEARSON CORRELATION ANALYSIS\n",
    "print(\" 1. PEARSON CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate Pearson correlation matrix\n",
    "pearson_corr = df.corr(method='pearson')\n",
    "\n",
    "print(\"Pearson Correlation Matrix:\")\n",
    "print(pearson_corr.round(3))\n",
    "\n",
    "# Create enhanced correlation heatmap\n",
    "fig = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=(\"Pearson Correlation Heatmap\", \"Correlation Strength Distribution\"),\n",
    " specs=[[{\"type\": \"heatmap\"}, {\"type\": \"histogram\"}]]\n",
    ")\n",
    "\n",
    "# Correlation heatmap\n",
    "fig.add_trace(\n",
    " go.Heatmap(\n",
    " z=pearson_corr.values,\n",
    " x=pearson_corr.columns,\n",
    " y=pearson_corr.columns,\n",
    " colorscale='RdBu_r',\n",
    " zmid=0,\n",
    " text=pearson_corr.round(3).values,\n",
    " texttemplate=\"%{text}\",\n",
    " textfont={\"size\": 10},\n",
    " hoverongaps=False\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Correlation distribution\n",
    "corr_values = pearson_corr.values[np.triu_indices_from(pearson_corr.values, k=1)]\n",
    "fig.add_trace(\n",
    " go.Histogram(\n",
    " x=corr_values,\n",
    " nbinsx=20,\n",
    " name=\"Correlation Distribution\",\n",
    " marker_color=\"steelblue\",\n",
    " opacity=0.7\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    " title=\"Pearson Correlation Analysis\",\n",
    " height=500,\n",
    " showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Identify strongest correlations\n",
    "def get_correlation_pairs(corr_matrix, threshold=0.5):\n",
    " \"\"\"Extract correlation pairs above threshold\"\"\"\n",
    " pairs = []\n",
    " for i in range(len(corr_matrix.columns)):\n",
    " for j in range(i+1, len(corr_matrix.columns)):\n",
    " corr_val = corr_matrix.iloc[i, j]\n",
    " if abs(corr_val) >= threshold:\n",
    " pairs.append({\n",
    " 'Variable_1': corr_matrix.columns[i],\n",
    " 'Variable_2': corr_matrix.columns[j],\n",
    " 'Correlation': corr_val,\n",
    " 'Strength': 'Strong' if abs(corr_val) >= 0.7 else 'Moderate'\n",
    " })\n",
    " return pd.DataFrame(pairs).sort_values('Correlation', key=abs, ascending=False)\n",
    "\n",
    "strong_correlations = get_correlation_pairs(pearson_corr, threshold=0.5)\n",
    "print(f\"\\n Strong Correlations (|r| >= 0.5):\")\n",
    "print(strong_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SPEARMAN RANK CORRELATION\n",
    "print(\"\\n 2. SPEARMAN RANK CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "spearman_corr = df.corr(method='spearman')\n",
    "\n",
    "# Compare Pearson vs Spearman\n",
    "comparison_data = []\n",
    "for i in range(len(df.columns)):\n",
    " for j in range(i+1, len(df.columns)):\n",
    " var1, var2 = df.columns[i], df.columns[j]\n",
    " pearson_val = pearson_corr.iloc[i, j]\n",
    " spearman_val = spearman_corr.iloc[i, j]\n",
    "\n",
    " comparison_data.append({\n",
    " 'Variable_Pair': f\"{var1} vs {var2}\",\n",
    " 'Pearson': pearson_val,\n",
    " 'Spearman': spearman_val,\n",
    " 'Difference': abs(pearson_val - spearman_val)\n",
    " })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Difference', ascending=False)\n",
    "\n",
    "# Visualize Pearson vs Spearman comparison\n",
    "fig = make_subplots(\n",
    " rows=2, cols=2,\n",
    " subplot_titles=(\"Spearman Correlation Matrix\", \"Pearson vs Spearman Scatter\",\n",
    " \"Correlation Method Differences\", \"Non-linear Relationship Example\"),\n",
    " specs=[[{\"type\": \"heatmap\"}, {\"type\": \"scatter\"}],\n",
    " [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    ")\n",
    "\n",
    "# Spearman heatmap\n",
    "fig.add_trace(\n",
    " go.Heatmap(\n",
    " z=spearman_corr.values,\n",
    " x=spearman_corr.columns,\n",
    " y=spearman_corr.columns,\n",
    " colorscale='Viridis',\n",
    " text=spearman_corr.round(3).values,\n",
    " texttemplate=\"%{text}\",\n",
    " textfont={\"size\": 8}\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Pearson vs Spearman scatter\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=comparison_df['Pearson'],\n",
    " y=comparison_df['Spearman'],\n",
    " mode='markers+text',\n",
    " text=comparison_df['Variable_Pair'].str[:15] + '...',\n",
    " textposition=\"top center\",\n",
    " marker=dict(size=8, color=comparison_df['Difference'],\n",
    " colorscale='Reds', showscale=True),\n",
    " name=\"Correlations\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Add diagonal line\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=[-1, 1], y=[-1, 1],\n",
    " mode='lines',\n",
    " line=dict(dash='dash', color='gray'),\n",
    " name=\"Perfect Agreement\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Differences bar chart\n",
    "top_diffs = comparison_df.head(8)\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=top_diffs['Difference'],\n",
    " y=top_diffs['Variable_Pair'],\n",
    " orientation='h',\n",
    " marker_color='coral',\n",
    " name=\"Correlation Differences\"\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# Example of non-linear relationship\n",
    "# Create quadratic relationship for demonstration\n",
    "x_nonlinear = np.linspace(-3, 3, 100)\n",
    "y_nonlinear = x_nonlinear**2 + np.random.normal(0, 0.5, 100)\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=x_nonlinear,\n",
    " y=y_nonlinear,\n",
    " mode='markers',\n",
    " marker=dict(color='purple', size=6),\n",
    " name=\"Quadratic Relationship\"\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    " title=\"Spearman vs Pearson Correlation Comparison\",\n",
    " height=800,\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Top differences between Pearson and Spearman correlations:\")\n",
    "print(comparison_df.head(5))\n",
    "\n",
    "# Statistical significance testing\n",
    "print(f\"\\n Statistical Significance Testing:\")\n",
    "sample_vars = ['GDP_Growth', 'Unemployment_Rate', 'Consumer_Confidence']\n",
    "for i in range(len(sample_vars)):\n",
    " for j in range(i+1, len(sample_vars)):\n",
    " var1, var2 = sample_vars[i], sample_vars[j]\n",
    "\n",
    " # Pearson correlation with p-value\n",
    " pearson_r, pearson_p = pearsonr(df[var1], df[var2])\n",
    "\n",
    " # Spearman correlation with p-value\n",
    " spearman_r, spearman_p = spearmanr(df[var1], df[var2])\n",
    "\n",
    " print(f\"{var1} vs {var2}:\")\n",
    " print(f\" Pearson: r={pearson_r:.3f}, p={pearson_p:.4f}\")\n",
    " print(f\" Spearman: ρ={spearman_r:.3f}, p={spearman_p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d215c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. KENDALL'S TAU CORRELATION\n",
    "print(\"\\n 3. KENDALL'S TAU CORRELATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate Kendall's tau\n",
    "kendall_corr = df.corr(method='kendall')\n",
    "\n",
    "# Compare all three methods\n",
    "method_comparison = pd.DataFrame({\n",
    " 'Variable_Pair': comparison_df['Variable_Pair'],\n",
    " 'Pearson': comparison_df['Pearson'],\n",
    " 'Spearman': comparison_df['Spearman'],\n",
    " 'Kendall': [kendalltau(df[pair.split(' vs ')[0]], df[pair.split(' vs ')[1]])[0]\n",
    " for pair in comparison_df['Variable_Pair']]\n",
    "})\n",
    "\n",
    "# Create comprehensive comparison plot\n",
    "fig = make_subplots(\n",
    " rows=2, cols=2,\n",
    " subplot_titles=(\"Three Correlation Methods Comparison\", \"Method Correlations\",\n",
    " \"Kendall's Tau Matrix\", \"Correlation Method Stability\"),\n",
    " specs=[[{\"type\": \"scatter\"}, {\"type\": \"bar\"}],\n",
    " [{\"type\": \"heatmap\"}, {\"type\": \"box\"}]]\n",
    ")\n",
    "\n",
    "# 3D-like scatter showing all three methods\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=method_comparison['Pearson'],\n",
    " y=method_comparison['Spearman'],\n",
    " text=method_comparison['Variable_Pair'],\n",
    " mode='markers+text',\n",
    " marker=dict(\n",
    " size=abs(method_comparison['Kendall']) * 20,\n",
    " color=method_comparison['Kendall'],\n",
    " colorscale='RdYlBu_r',\n",
    " showscale=True,\n",
    " colorbar=dict(title=\"Kendall's Tau\")\n",
    " ),\n",
    " name=\"All Methods\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Method correlations bar chart\n",
    "method_corrs = [\n",
    " stats.pearsonr(method_comparison['Pearson'], method_comparison['Spearman'])[0],\n",
    " stats.pearsonr(method_comparison['Pearson'], method_comparison['Kendall'])[0],\n",
    " stats.pearsonr(method_comparison['Spearman'], method_comparison['Kendall'])[0]\n",
    "]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=['Pearson-Spearman', 'Pearson-Kendall', 'Spearman-Kendall'],\n",
    " y=method_corrs,\n",
    " marker_color=['skyblue', 'lightcoral', 'lightgreen'],\n",
    " name=\"Method Correlations\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Kendall's tau heatmap\n",
    "fig.add_trace(\n",
    " go.Heatmap(\n",
    " z=kendall_corr.values,\n",
    " x=kendall_corr.columns,\n",
    " y=kendall_corr.columns,\n",
    " colorscale='Plasma',\n",
    " text=kendall_corr.round(3).values,\n",
    " texttemplate=\"%{text}\",\n",
    " textfont={\"size\": 8}\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# Box plot of correlation values by method\n",
    "correlation_values = []\n",
    "methods = []\n",
    "for method in ['Pearson', 'Spearman', 'Kendall']:\n",
    " values = method_comparison[method].values\n",
    " correlation_values.extend(values)\n",
    " methods.extend([method] * len(values))\n",
    "\n",
    "for i, method in enumerate(['Pearson', 'Spearman', 'Kendall']):\n",
    " fig.add_trace(\n",
    " go.Box(\n",
    " y=method_comparison[method],\n",
    " name=method,\n",
    " boxpoints='outliers',\n",
    " marker_color=['blue', 'red', 'green'][i]\n",
    " ),\n",
    " row=2, col=2\n",
    " )\n",
    "\n",
    "fig.update_layout(\n",
    " title=\"Comprehensive Correlation Method Analysis\",\n",
    " height=800,\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Correlation between different methods:\")\n",
    "print(f\"Pearson-Spearman correlation: {method_corrs[0]:.3f}\")\n",
    "print(f\"Pearson-Kendall correlation: {method_corrs[1]:.3f}\")\n",
    "print(f\"Spearman-Kendall correlation: {method_corrs[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e429e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. INTERACTIVE CORRELATION EXPLORER\n",
    "print(\"\\n 4. INTERACTIVE CORRELATION EXPLORER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create interactive correlation matrix with drill-down capability\n",
    "def create_interactive_correlation_matrix(dataframe, method='pearson'):\n",
    " \"\"\"Create an interactive correlation matrix with detailed hover information\"\"\"\n",
    "\n",
    " # Calculate correlation matrix\n",
    " corr_matrix = dataframe.corr(method=method)\n",
    "\n",
    " # Create mask for upper triangle (optional)\n",
    " mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    " # Create hover text with detailed statistics\n",
    " hover_text = []\n",
    " for i in range(len(corr_matrix.columns)):\n",
    " hover_text.append([])\n",
    " for j in range(len(corr_matrix.columns)):\n",
    " if i != j:\n",
    " var1, var2 = corr_matrix.columns[i], corr_matrix.columns[j]\n",
    " corr_val = corr_matrix.iloc[i, j]\n",
    "\n",
    " # Calculate additional statistics\n",
    " x_data, y_data = dataframe[var1], dataframe[var2]\n",
    " r_squared = corr_val ** 2\n",
    "\n",
    " hover_info = (\n",
    " f\"Variables: {var1} vs {var2}<br>\"\n",
    " f\"Correlation: {corr_val:.4f}<br>\"\n",
    " f\"R-squared: {r_squared:.4f}<br>\"\n",
    " f\"Sample size: {len(x_data)}<br>\"\n",
    " f\"Strength: {'Strong' if abs(corr_val) >= 0.7 else 'Moderate' if abs(corr_val) >= 0.3 else 'Weak'}\"\n",
    " )\n",
    " else:\n",
    " hover_info = f\"Variable: {corr_matrix.columns[i]}<br>Perfect correlation: 1.000\"\n",
    "\n",
    " hover_text[i].append(hover_info)\n",
    "\n",
    " # Create the heatmap\n",
    " fig = go.Figure(data=go.Heatmap(\n",
    " z=corr_matrix.values,\n",
    " x=corr_matrix.columns,\n",
    " y=corr_matrix.columns,\n",
    " colorscale='RdBu_r',\n",
    " zmid=0,\n",
    " text=corr_matrix.round(3).values,\n",
    " texttemplate=\"%{text}\",\n",
    " textfont={\"size\": 10},\n",
    " hovertemplate='%{hovertext}<extra></extra>',\n",
    " hovertext=hover_text\n",
    " ))\n",
    "\n",
    " fig.update_layout(\n",
    " title=f\"Interactive {method.title()} Correlation Matrix<br><sub>Hover for detailed statistics</sub>\",\n",
    " xaxis_title=\"Variables\",\n",
    " yaxis_title=\"Variables\",\n",
    " width=800,\n",
    " height=600\n",
    " )\n",
    "\n",
    " return fig\n",
    "\n",
    "# Create interactive matrices for all methods\n",
    "for method in ['pearson', 'spearman', 'kendall']:\n",
    " fig = create_interactive_correlation_matrix(df, method)\n",
    " fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cfc3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. CORRELATION SIGNIFICANCE AND CONFIDENCE INTERVALS\n",
    "print(\"\\n 5. CORRELATION SIGNIFICANCE & CONFIDENCE INTERVALS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def correlation_with_confidence(x, y, method='pearson', confidence=0.95):\n",
    " \"\"\"Calculate correlation with confidence intervals\"\"\"\n",
    " n = len(x)\n",
    "\n",
    " if method == 'pearson':\n",
    " r, p_value = pearsonr(x, y)\n",
    " elif method == 'spearman':\n",
    " r, p_value = spearmanr(x, y)\n",
    " elif method == 'kendall':\n",
    " r, p_value = kendalltau(x, y)\n",
    "\n",
    " # Fisher's z-transformation for confidence intervals (works best for Pearson)\n",
    " if method == 'pearson' and abs(r) < 0.999:\n",
    " z = np.arctanh(r)\n",
    " se = 1 / np.sqrt(n - 3)\n",
    " alpha = 1 - confidence\n",
    " z_critical = stats.norm.ppf(1 - alpha/2)\n",
    "\n",
    " z_lower = z - z_critical * se\n",
    " z_upper = z + z_critical * se\n",
    "\n",
    " ci_lower = np.tanh(z_lower)\n",
    " ci_upper = np.tanh(z_upper)\n",
    " else:\n",
    " # Approximate CI for non-parametric methods\n",
    " se = 1 / np.sqrt(n - 3)\n",
    " alpha = 1 - confidence\n",
    " z_critical = stats.norm.ppf(1 - alpha/2)\n",
    "\n",
    " ci_lower = r - z_critical * se\n",
    " ci_upper = r + z_critical * se\n",
    "\n",
    " return {\n",
    " 'correlation': r,\n",
    " 'p_value': p_value,\n",
    " 'ci_lower': ci_lower,\n",
    " 'ci_upper': ci_upper,\n",
    " 'significant': p_value < 0.05\n",
    " }\n",
    "\n",
    "# Analyze key relationships with confidence intervals\n",
    "key_relationships = [\n",
    " ('GDP_Growth', 'Unemployment_Rate'),\n",
    " ('GDP_Growth', 'Consumer_Confidence'),\n",
    " ('Interest_Rates', 'Housing_Price_Index'),\n",
    " ('Tech_Investment', 'Productivity_Index'),\n",
    " ('Inflation_Rate', 'Interest_Rates')\n",
    "]\n",
    "\n",
    "significance_results = []\n",
    "\n",
    "for var1, var2 in key_relationships:\n",
    " x, y = df[var1], df[var2]\n",
    "\n",
    " # Calculate for all three methods\n",
    " for method in ['pearson', 'spearman', 'kendall']:\n",
    " result = correlation_with_confidence(x, y, method)\n",
    " significance_results.append({\n",
    " 'Variable_1': var1,\n",
    " 'Variable_2': var2,\n",
    " 'Method': method.title(),\n",
    " 'Correlation': result['correlation'],\n",
    " 'P_Value': result['p_value'],\n",
    " 'CI_Lower': result['ci_lower'],\n",
    " 'CI_Upper': result['ci_upper'],\n",
    " 'Significant': result['significant'],\n",
    " 'CI_Width': result['ci_upper'] - result['ci_lower']\n",
    " })\n",
    "\n",
    "significance_df = pd.DataFrame(significance_results)\n",
    "\n",
    "# Visualize confidence intervals\n",
    "fig = make_subplots(\n",
    " rows=2, cols=2,\n",
    " subplot_titles=(\"Correlation Confidence Intervals\", \"P-Value Distribution\",\n",
    " \"Significance by Method\", \"Confidence Interval Widths\"),\n",
    " specs=[[{\"type\": \"scatter\"}, {\"type\": \"histogram\"}],\n",
    " [{\"type\": \"bar\"}, {\"type\": \"box\"}]]\n",
    ")\n",
    "\n",
    "# Confidence interval plot\n",
    "for i, method in enumerate(['Pearson', 'Spearman', 'Kendall']):\n",
    " method_data = significance_df[significance_df['Method'] == method]\n",
    "\n",
    " fig.add_trace(\n",
    " go.Scatter(\n",
    " x=method_data['Correlation'],\n",
    " y=range(len(method_data)),\n",
    " error_x=dict(\n",
    " type='data',\n",
    " symmetric=False,\n",
    " array=method_data['CI_Upper'] - method_data['Correlation'],\n",
    " arrayminus=method_data['Correlation'] - method_data['CI_Lower']\n",
    " ),\n",
    " mode='markers',\n",
    " name=f\"{method} CI\",\n",
    " marker=dict(size=8),\n",
    " text=method_data['Variable_1'] + ' vs ' + method_data['Variable_2'],\n",
    " hovertemplate=\"<b>%{text}</b><br>Correlation: %{x:.3f}<br>CI: [%{error_x.arrayminus:.3f}, %{error_x.array:.3f}]<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    " )\n",
    "\n",
    "# P-value distribution\n",
    "fig.add_trace(\n",
    " go.Histogram(\n",
    " x=significance_df['P_Value'],\n",
    " nbinsx=20,\n",
    " name=\"P-Values\",\n",
    " marker_color=\"lightblue\",\n",
    " opacity=0.7\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Add significance threshold line\n",
    "fig.add_vline(x=0.05, line_dash=\"dash\", line_color=\"red\", row=1, col=2)\n",
    "\n",
    "# Significance by method\n",
    "sig_by_method = significance_df.groupby('Method')['Significant'].sum()\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=sig_by_method.index,\n",
    " y=sig_by_method.values,\n",
    " marker_color=['blue', 'red', 'green'],\n",
    " name=\"Significant Correlations\"\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# CI width by method\n",
    "for method in ['Pearson', 'Spearman', 'Kendall']:\n",
    " method_data = significance_df[significance_df['Method'] == method]\n",
    " fig.add_trace(\n",
    " go.Box(\n",
    " y=method_data['CI_Width'],\n",
    " name=method,\n",
    " boxpoints='outliers'\n",
    " ),\n",
    " row=2, col=2\n",
    " )\n",
    "\n",
    "fig.update_layout(\n",
    " title=\"Correlation Statistical Significance Analysis\",\n",
    " height=800,\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Print significance summary\n",
    "print(\" Significance Summary:\")\n",
    "print(significance_df.groupby(['Method', 'Significant']).size().unstack(fill_value=0))\n",
    "\n",
    "print(f\"\\n Key Findings:\")\n",
    "significant_strong = significance_df[\n",
    " (significance_df['Significant']) &\n",
    " (abs(significance_df['Correlation']) > 0.5)\n",
    "]\n",
    "print(f\"• {len(significant_strong)} relationships show strong significant correlations\")\n",
    "print(f\"• Average p-value: {significance_df['P_Value'].mean():.4f}\")\n",
    "print(f\"• Most precise method (narrowest CI): {significance_df.groupby('Method')['CI_Width'].mean().idxmin()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. PARTIAL CORRELATION ANALYSIS\n",
    "print(\"\\n 6. PARTIAL CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def partial_correlation(data, x_var, y_var, control_vars):\n",
    " \"\"\"\n",
    " Calculate partial correlation between x_var and y_var, controlling for control_vars\n",
    " \"\"\"\n",
    " # Fit linear regression to remove effect of control variables\n",
    " X_control = data[control_vars].values\n",
    "\n",
    " # Residuals after removing control variable effects\n",
    " reg_x = LinearRegression().fit(X_control, data[x_var])\n",
    " residuals_x = data[x_var] - reg_x.predict(X_control)\n",
    "\n",
    " reg_y = LinearRegression().fit(X_control, data[y_var])\n",
    " residuals_y = data[y_var] - reg_y.predict(X_control)\n",
    "\n",
    " # Correlation of residuals is the partial correlation\n",
    " partial_corr = np.corrcoef(residuals_x, residuals_y)[0, 1]\n",
    "\n",
    " return partial_corr, residuals_x, residuals_y\n",
    "\n",
    "# Example: GDP Growth vs Consumer Confidence, controlling for Unemployment\n",
    "target_relationships = [\n",
    " ('GDP_Growth', 'Consumer_Confidence', ['Unemployment_Rate']),\n",
    " ('Stock_Returns', 'Consumer_Confidence', ['GDP_Growth', 'Inflation_Rate']),\n",
    " ('Housing_Price_Index', 'Interest_Rates', ['GDP_Growth', 'Inflation_Rate']),\n",
    " ('Tech_Investment', 'Productivity_Index', ['GDP_Growth'])\n",
    "]\n",
    "\n",
    "partial_results = []\n",
    "\n",
    "fig = make_subplots(\n",
    " rows=2, cols=2,\n",
    " subplot_titles=[f\"{x} vs {y}<br>Controlling for {', '.join(controls)}\"\n",
    " for x, y, controls in target_relationships],\n",
    " specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    " [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]]\n",
    ")\n",
    "\n",
    "for idx, (x_var, y_var, control_vars) in enumerate(target_relationships):\n",
    " # Calculate simple correlation\n",
    " simple_corr = df[x_var].corr(df[y_var])\n",
    "\n",
    " # Calculate partial correlation\n",
    " partial_corr, residuals_x, residuals_y = partial_correlation(df, x_var, y_var, control_vars)\n",
    "\n",
    " partial_results.append({\n",
    " 'X_Variable': x_var,\n",
    " 'Y_Variable': y_var,\n",
    " 'Control_Variables': ', '.join(control_vars),\n",
    " 'Simple_Correlation': simple_corr,\n",
    " 'Partial_Correlation': partial_corr,\n",
    " 'Difference': simple_corr - partial_corr,\n",
    " 'Control_Effect': abs(simple_corr - partial_corr)\n",
    " })\n",
    "\n",
    " # Plot partial correlation (residuals)\n",
    " row = (idx // 2) + 1\n",
    " col = (idx % 2) + 1\n",
    "\n",
    " fig.add_trace(\n",
    " go.Scatter(\n",
    " x=residuals_x,\n",
    " y=residuals_y,\n",
    " mode='markers',\n",
    " marker=dict(size=4, opacity=0.6),\n",
    " name=f\"Partial r={partial_corr:.3f}\",\n",
    " text=f\"Simple r={simple_corr:.3f}<br>Partial r={partial_corr:.3f}\",\n",
    " hovertemplate=\"<b>Partial Correlation</b><br>%{text}<br>X residual: %{x:.2f}<br>Y residual: %{y:.2f}<extra></extra>\"\n",
    " ),\n",
    " row=row, col=col\n",
    " )\n",
    "\n",
    " # Add trend line for partial correlation\n",
    " z = np.polyfit(residuals_x, residuals_y, 1)\n",
    " x_trend = np.linspace(residuals_x.min(), residuals_x.max(), 100)\n",
    " y_trend = np.poly1d(z)(x_trend)\n",
    "\n",
    " fig.add_trace(\n",
    " go.Scatter(\n",
    " x=x_trend,\n",
    " y=y_trend,\n",
    " mode='lines',\n",
    " line=dict(dash='dash', color='red'),\n",
    " name=f\"Trend (r={partial_corr:.3f})\",\n",
    " showlegend=False\n",
    " ),\n",
    " row=row, col=col\n",
    " )\n",
    "\n",
    "fig.update_layout(\n",
    " title=\"Partial Correlation Analysis - Residual Plots\",\n",
    " height=600,\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Display partial correlation results\n",
    "partial_df = pd.DataFrame(partial_results)\n",
    "print(\" Partial Correlation Results:\")\n",
    "print(partial_df.round(3))\n",
    "\n",
    "# Visualize the comparison\n",
    "fig2 = go.Figure()\n",
    "\n",
    "x_pos = range(len(partial_df))\n",
    "fig2.add_trace(go.Bar(\n",
    " x=x_pos,\n",
    " y=partial_df['Simple_Correlation'],\n",
    " name='Simple Correlation',\n",
    " marker_color='lightblue',\n",
    " text=partial_df['Simple_Correlation'].round(3),\n",
    " textposition='auto'\n",
    "))\n",
    "\n",
    "fig2.add_trace(go.Bar(\n",
    " x=x_pos,\n",
    " y=partial_df['Partial_Correlation'],\n",
    " name='Partial Correlation',\n",
    " marker_color='lightcoral',\n",
    " text=partial_df['Partial_Correlation'].round(3),\n",
    " textposition='auto'\n",
    "))\n",
    "\n",
    "fig2.update_layout(\n",
    " title=\"Simple vs Partial Correlations Comparison\",\n",
    " xaxis_title=\"Variable Pairs\",\n",
    " yaxis_title=\"Correlation Coefficient\",\n",
    " xaxis=dict(\n",
    " tickmode='array',\n",
    " tickvals=x_pos,\n",
    " ticktext=[f\"{row.X_Variable} vs<br>{row.Y_Variable}\" for _, row in partial_df.iterrows()]\n",
    " ),\n",
    " barmode='group',\n",
    " height=500\n",
    ")\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "print(f\"\\n Partial Correlation Insights:\")\n",
    "print(f\"• Largest control effect: {partial_df.loc[partial_df['Control_Effect'].idxmax(), 'X_Variable']} vs {partial_df.loc[partial_df['Control_Effect'].idxmax(), 'Y_Variable']}\")\n",
    "print(f\"• Average control effect: {partial_df['Control_Effect'].mean():.3f}\")\n",
    "print(f\"• Cases where partial > simple: {sum(partial_df['Partial_Correlation'] > partial_df['Simple_Correlation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050601f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. SPURIOUS CORRELATION DETECTION\n",
    "print(\"\\n 7. SPURIOUS CORRELATION DETECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create examples of potentially spurious correlations\n",
    "np.random.seed(123)\n",
    "\n",
    "# Time series that might show spurious correlation\n",
    "time = np.arange(100)\n",
    "trend1 = 2 * time + np.random.normal(0, 10, 100) # Linear trend\n",
    "trend2 = 1.5 * time + np.random.normal(0, 8, 100) # Similar trend\n",
    "random1 = np.cumsum(np.random.normal(0, 1, 100)) # Random walk\n",
    "random2 = np.cumsum(np.random.normal(0, 1, 100)) # Another random walk\n",
    "\n",
    "# Add some seasonal patterns\n",
    "seasonal1 = 20 * np.sin(2 * np.pi * time / 12) + trend1\n",
    "seasonal2 = 15 * np.cos(2 * np.pi * time / 12) + trend2\n",
    "\n",
    "spurious_data = pd.DataFrame({\n",
    " 'Time': time,\n",
    " 'Trend_Series_1': trend1,\n",
    " 'Trend_Series_2': trend2,\n",
    " 'Random_Walk_1': random1,\n",
    " 'Random_Walk_2': random2,\n",
    " 'Seasonal_1': seasonal1,\n",
    " 'Seasonal_2': seasonal2\n",
    "})\n",
    "\n",
    "# Calculate correlations\n",
    "spurious_corr = spurious_data.drop('Time', axis=1).corr()\n",
    "\n",
    "fig = make_subplots(\n",
    " rows=2, cols=3,\n",
    " subplot_titles=(\"Trending Series\", \"Random Walks\", \"Seasonal Series\",\n",
    " \"Detrended Series\", \"Differenced Random Walks\", \"Correlation Matrix\"),\n",
    " specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    " [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"heatmap\"}]]\n",
    ")\n",
    "\n",
    "# Plot original series\n",
    "series_pairs = [\n",
    " ('Trend_Series_1', 'Trend_Series_2', 1, 1),\n",
    " ('Random_Walk_1', 'Random_Walk_2', 1, 2),\n",
    " ('Seasonal_1', 'Seasonal_2', 1, 3)\n",
    "]\n",
    "\n",
    "for var1, var2, row, col in series_pairs:\n",
    " # Original series\n",
    " fig.add_trace(\n",
    " go.Scatter(\n",
    " x=spurious_data[var1],\n",
    " y=spurious_data[var2],\n",
    " mode='markers',\n",
    " marker=dict(size=4, opacity=0.7),\n",
    " name=f\"{var1} vs {var2}\",\n",
    " text=f\"r = {spurious_data[var1].corr(spurious_data[var2]):.3f}\",\n",
    " hovertemplate=\"<b>%{text}</b><br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>\"\n",
    " ),\n",
    " row=row, col=col\n",
    " )\n",
    "\n",
    "# Detrending and differencing to remove spurious correlations\n",
    "from scipy import signal\n",
    "\n",
    "# Detrend the trending series\n",
    "detrended_1 = signal.detrend(spurious_data['Trend_Series_1'])\n",
    "detrended_2 = signal.detrend(spurious_data['Trend_Series_2'])\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=detrended_1,\n",
    " y=detrended_2,\n",
    " mode='markers',\n",
    " marker=dict(size=4, color='red', opacity=0.7),\n",
    " name=f\"Detrended r={np.corrcoef(detrended_1, detrended_2)[0,1]:.3f}\",\n",
    " text=f\"Detrended r = {np.corrcoef(detrended_1, detrended_2)[0,1]:.3f}\",\n",
    " hovertemplate=\"<b>%{text}</b><br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>\"\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# Difference the random walks\n",
    "diff_random1 = np.diff(spurious_data['Random_Walk_1'])\n",
    "diff_random2 = np.diff(spurious_data['Random_Walk_2'])\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=diff_random1,\n",
    " y=diff_random2,\n",
    " mode='markers',\n",
    " marker=dict(size=4, color='green', opacity=0.7),\n",
    " name=f\"Differenced r={np.corrcoef(diff_random1, diff_random2)[0,1]:.3f}\",\n",
    " text=f\"Differenced r = {np.corrcoef(diff_random1, diff_random2)[0,1]:.3f}\",\n",
    " hovertemplate=\"<b>%{text}</b><br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>\"\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "# Correlation matrix\n",
    "fig.add_trace(\n",
    " go.Heatmap(\n",
    " z=spurious_corr.values,\n",
    " x=spurious_corr.columns,\n",
    " y=spurious_corr.columns,\n",
    " colorscale='RdBu_r',\n",
    " zmid=0,\n",
    " text=spurious_corr.round(3).values,\n",
    " texttemplate=\"%{text}\",\n",
    " textfont={\"size\": 8}\n",
    " ),\n",
    " row=2, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    " title=\"Spurious Correlation Detection and Correction\",\n",
    " height=600,\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Spurious correlation tests\n",
    "print(\" Spurious Correlation Analysis:\")\n",
    "print(\"Original correlations (potentially spurious):\")\n",
    "original_corrs = {\n",
    " 'Trending Series': spurious_data['Trend_Series_1'].corr(spurious_data['Trend_Series_2']),\n",
    " 'Random Walks': spurious_data['Random_Walk_1'].corr(spurious_data['Random_Walk_2']),\n",
    " 'Seasonal Series': spurious_data['Seasonal_1'].corr(spurious_data['Seasonal_2'])\n",
    "}\n",
    "\n",
    "print(\"After correction:\")\n",
    "corrected_corrs = {\n",
    " 'Detrended Series': np.corrcoef(detrended_1, detrended_2)[0,1],\n",
    " 'Differenced Random Walks': np.corrcoef(diff_random1, diff_random2)[0,1],\n",
    " 'Deseasonalized': np.corrcoef(\n",
    " signal.detrend(spurious_data['Seasonal_1']),\n",
    " signal.detrend(spurious_data['Seasonal_2'])\n",
    " )[0,1]\n",
    "}\n",
    "\n",
    "for name, corr in original_corrs.items():\n",
    " print(f\"• {name}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nAfter removing trends/patterns:\")\n",
    "for name, corr in corrected_corrs.items():\n",
    " print(f\"• {name}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf11816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. BUSINESS INSIGHTS AND INTERPRETATION\n",
    "print(\"\\n 8. BUSINESS INSIGHTS & INTERPRETATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create business-focused correlation insights\n",
    "business_insights = []\n",
    "\n",
    "# Define business-relevant thresholds and interpretations\n",
    "def interpret_correlation(r, var1, var2):\n",
    " \"\"\"Provide business interpretation of correlation\"\"\"\n",
    " abs_r = abs(r)\n",
    "\n",
    " strength = \"Very Strong\" if abs_r >= 0.8 else \"Strong\" if abs_r >= 0.6 else \"Moderate\" if abs_r >= 0.4 else \"Weak\" if abs_r >= 0.2 else \"Very Weak\"\n",
    " direction = \"Positive\" if r > 0 else \"Negative\"\n",
    "\n",
    " # Business implications\n",
    " if abs_r >= 0.6:\n",
    " reliability = \"High - suitable for strategic decisions\"\n",
    " action = \"Primary focus for policy/strategy\"\n",
    " elif abs_r >= 0.4:\n",
    " reliability = \"Moderate - consider in planning\"\n",
    " action = \"Secondary consideration\"\n",
    " else:\n",
    " reliability = \"Low - monitor but don't rely on\"\n",
    " action = \"Investigate other factors\"\n",
    "\n",
    " return {\n",
    " 'strength': strength,\n",
    " 'direction': direction,\n",
    " 'reliability': reliability,\n",
    " 'action': action,\n",
    " 'business_impact': get_business_meaning(var1, var2, r)\n",
    " }\n",
    "\n",
    "def get_business_meaning(var1, var2, correlation):\n",
    " \"\"\"Generate business-specific interpretations\"\"\"\n",
    " relationships = {\n",
    " ('GDP_Growth', 'Unemployment_Rate'): \"Economic health indicator - inverse relationship expected\",\n",
    " ('GDP_Growth', 'Consumer_Confidence'): \"Economic sentiment alignment - positive relationship\",\n",
    " ('Interest_Rates', 'Housing_Price_Index'): \"Monetary policy impact on real estate\",\n",
    " ('Tech_Investment', 'Productivity_Index'): \"Innovation-productivity nexus\",\n",
    " ('Inflation_Rate', 'Interest_Rates'): \"Central bank policy response mechanism\"\n",
    " }\n",
    "\n",
    " key = (var1, var2) if (var1, var2) in relationships else (var2, var1)\n",
    " base_meaning = relationships.get(key, \"Economic relationship\")\n",
    "\n",
    " if abs(correlation) >= 0.6:\n",
    " strength_comment = \"Strong predictive relationship\"\n",
    " elif abs(correlation) >= 0.4:\n",
    " strength_comment = \"Moderate predictive power\"\n",
    " else:\n",
    " strength_comment = \"Limited predictive value\"\n",
    "\n",
    " return f\"{base_meaning}. {strength_comment}.\"\n",
    "\n",
    "# Analyze key business relationships\n",
    "key_business_pairs = [\n",
    " ('GDP_Growth', 'Unemployment_Rate'),\n",
    " ('GDP_Growth', 'Consumer_Confidence'),\n",
    " ('Interest_Rates', 'Housing_Price_Index'),\n",
    " ('Tech_Investment', 'Productivity_Index'),\n",
    " ('Inflation_Rate', 'Interest_Rates'),\n",
    " ('Consumer_Confidence', 'Retail_Sales_Index'),\n",
    " ('Stock_Returns', 'Consumer_Confidence')\n",
    "]\n",
    "\n",
    "for var1, var2 in key_business_pairs:\n",
    " correlation = df[var1].corr(df[var2])\n",
    " interpretation = interpret_correlation(correlation, var1, var2)\n",
    "\n",
    " business_insights.append({\n",
    " 'Relationship': f\"{var1} ↔ {var2}\",\n",
    " 'Correlation': correlation,\n",
    " 'Strength': interpretation['strength'],\n",
    " 'Direction': interpretation['direction'],\n",
    " 'Business_Reliability': interpretation['reliability'],\n",
    " 'Recommended_Action': interpretation['action'],\n",
    " 'Business_Meaning': interpretation['business_impact']\n",
    " })\n",
    "\n",
    "insights_df = pd.DataFrame(business_insights)\n",
    "\n",
    "# Create business dashboard\n",
    "fig = make_subplots(\n",
    " rows=2, cols=2,\n",
    " subplot_titles=(\"Correlation Strength Distribution\", \"Business Action Matrix\",\n",
    " \"Reliability vs Correlation\", \"Key Economic Relationships\"),\n",
    " specs=[[{\"type\": \"histogram\"}, {\"type\": \"scatter\"}],\n",
    " [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# Correlation strength distribution\n",
    "fig.add_trace(\n",
    " go.Histogram(\n",
    " x=abs(insights_df['Correlation']),\n",
    " nbinsx=15,\n",
    " name=\"Correlation Strengths\",\n",
    " marker_color=\"lightblue\",\n",
    " opacity=0.7\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Business action matrix\n",
    "action_colors = {\n",
    " 'Primary focus for policy/strategy': 'red',\n",
    " 'Secondary consideration': 'orange',\n",
    " 'Investigate other factors': 'gray'\n",
    "}\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=insights_df['Correlation'],\n",
    " y=range(len(insights_df)),\n",
    " mode='markers+text',\n",
    " marker=dict(\n",
    " size=abs(insights_df['Correlation']) * 30,\n",
    " color=[action_colors[action] for action in insights_df['Recommended_Action']],\n",
    " opacity=0.8\n",
    " ),\n",
    " text=insights_df['Relationship'],\n",
    " textposition=\"middle right\",\n",
    " name=\"Action Priority\",\n",
    " hovertemplate=\"<b>%{text}</b><br>Correlation: %{x:.3f}<br>Action: %{customdata}<extra></extra>\",\n",
    " customdata=insights_df['Recommended_Action']\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Reliability analysis (correlation vs strength)\n",
    "strength_numeric = {\n",
    " 'Very Strong': 5, 'Strong': 4, 'Moderate': 3, 'Weak': 2, 'Very Weak': 1\n",
    "}\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=abs(insights_df['Correlation']),\n",
    " y=[strength_numeric[s] for s in insights_df['Strength']],\n",
    " mode='markers',\n",
    " marker=dict(size=10, opacity=0.7, color='purple'),\n",
    " text=insights_df['Relationship'],\n",
    " name=\"Strength Assessment\",\n",
    " hovertemplate=\"<b>%{text}</b><br>|Correlation|: %{x:.3f}<br>Strength: %{customdata}<extra></extra>\",\n",
    " customdata=insights_df['Strength']\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# Key relationships bar chart\n",
    "top_relationships = insights_df.nlargest(7, 'Correlation', keep='all')\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=top_relationships['Correlation'],\n",
    " y=range(len(top_relationships)),\n",
    " orientation='h',\n",
    " marker_color='steelblue',\n",
    " text=top_relationships['Relationship'],\n",
    " textposition='auto',\n",
    " name=\"Top Correlations\"\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    " title=\"Business Correlation Analysis Dashboard\",\n",
    " height=700,\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\" Business Insights Summary:\")\n",
    "print(insights_df[['Relationship', 'Correlation', 'Strength', 'Recommended_Action']].round(3))\n",
    "\n",
    "print(f\"\\n Strategic Recommendations:\")\n",
    "high_priority = insights_df[insights_df['Recommended_Action'] == 'Primary focus for policy/strategy']\n",
    "print(f\"• {len(high_priority)} relationships require primary strategic focus\")\n",
    "print(f\"• Strongest relationship: {insights_df.loc[insights_df['Correlation'].abs().idxmax(), 'Relationship']} (r={insights_df['Correlation'].abs().max():.3f})\")\n",
    "\n",
    "moderate_priority = insights_df[insights_df['Recommended_Action'] == 'Secondary consideration']\n",
    "print(f\"• {len(moderate_priority)} relationships are secondary considerations\")\n",
    "\n",
    "low_priority = insights_df[insights_df['Recommended_Action'] == 'Investigate other factors']\n",
    "print(f\"• {len(low_priority)} relationships need further investigation\")\n",
    "\n",
    "print(f\"\\n Business Intelligence:\")\n",
    "for _, row in high_priority.iterrows():\n",
    " print(f\"• {row['Relationship']}: {row['Business_Meaning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a2797",
   "metadata": {},
   "source": [
    "# LEARNING SUMMARY: Correlation Analysis\n",
    "\n",
    "## Key Concepts Mastered\n",
    "\n",
    "### 1. **Correlation Methods**\n",
    "- **Pearson Correlation**: Measures linear relationships, assumes normal distribution\n",
    "- **Spearman Correlation**: Rank-based, captures monotonic relationships, non-parametric\n",
    "- **Kendall's Tau**: Also rank-based, more robust to outliers than Spearman\n",
    "\n",
    "### 2. **Statistical Significance**\n",
    "- P-values indicate whether correlations are statistically significant\n",
    "- Confidence intervals provide range of plausible correlation values\n",
    "- Sample size affects both significance and precision\n",
    "\n",
    "### 3. **Advanced Techniques**\n",
    "- **Partial Correlation**: Controls for confounding variables\n",
    "- **Spurious Correlation**: Relationships that appear meaningful but aren't causal\n",
    "- **Detrending**: Removes artificial correlations due to common trends\n",
    "\n",
    "## Business Applications\n",
    "\n",
    "### Strategic Decision Making\n",
    "- **Strong correlations (|r| > 0.6)**: Primary focus for strategic planning\n",
    "- **Moderate correlations (0.3 < |r| < 0.6)**: Secondary considerations\n",
    "- **Weak correlations (|r| < 0.3)**: Monitor but investigate other factors\n",
    "\n",
    "### Risk Management\n",
    "- Understanding correlation helps in:\n",
    " - Portfolio diversification\n",
    " - Risk factor identification\n",
    " - Scenario planning\n",
    " - Economic forecasting\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Tier 2: Predictive Models** - Use correlation insights for feature selection\n",
    "2. **Causality Analysis** - Move beyond correlation to understand causation\n",
    "3. **Multivariate Analysis** - Explore relationships among multiple variables simultaneously\n",
    "4. **Time Series Analysis** - Understand how correlations change over time\n",
    "\n",
    "## Pro Tips\n",
    "\n",
    "- Always check for outliers that might inflate correlations\n",
    "- Consider non-linear relationships that correlation might miss\n",
    "- Use domain knowledge to interpret correlation strength\n",
    "- Be cautious of spurious correlations in time series data\n",
    "- Combine multiple correlation methods for robust analysis\n",
    "\n",
    "**Remember**: *Correlation does not imply causation, but it's the first step in understanding relationships in your data!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}