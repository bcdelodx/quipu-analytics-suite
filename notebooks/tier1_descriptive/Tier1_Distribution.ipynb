{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 1: Distribution Analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** 25bbbb63-5a6a-47cd-92e0-1453484beff0\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 1: Distribution Analysis,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** 25bbbb63-5a6a-47cd-92e0-1453484beff0\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfea566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Essential Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm, lognorm, gamma, beta, uniform, expon\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 1: Distribution Analysis - Libraries Loaded Successfully!\")\n",
    "print(\"=\" * 65)\n",
    "print(\"Available Distribution Analysis Techniques:\")\n",
    "print(\"• Histogram Analysis - Frequency distribution visualization\")\n",
    "print(\"• Density Plots - Smooth probability density estimation\")\n",
    "print(\"• Box Plots - Quartile and outlier analysis\")\n",
    "print(\"• Violin Plots - Distribution shape and density\")\n",
    "print(\"• Q-Q Plots - Normality testing and comparison\")\n",
    "print(\"• Distribution Fitting - Statistical model selection\")\n",
    "print(\"• Comparative Analysis - Multi-group distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7851ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Comprehensive Business Dataset for Distribution Analysis\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_business_distribution_dataset(n_samples=2000):\n",
    " \"\"\"Generate realistic business dataset with various distribution patterns\"\"\"\n",
    "\n",
    " # Sales revenue (Log-normal distribution - common in business)\n",
    " sales_revenue = np.random.lognormal(mean=10.5, sigma=0.8, size=n_samples)\n",
    "\n",
    " # Customer satisfaction (Beta distribution - bounded 0-10)\n",
    " satisfaction_raw = np.random.beta(a=2, b=0.5, size=n_samples)\n",
    " customer_satisfaction = satisfaction_raw * 10\n",
    "\n",
    " # Employee performance (Normal distribution)\n",
    " employee_performance = np.random.normal(loc=75, scale=12, size=n_samples)\n",
    " employee_performance = np.clip(employee_performance, 0, 100)\n",
    "\n",
    " # Website response time (Exponential distribution)\n",
    " response_time = np.random.exponential(scale=1.5, size=n_samples)\n",
    "\n",
    " # Product prices (Gamma distribution)\n",
    " product_prices = np.random.gamma(shape=2, scale=50, size=n_samples)\n",
    "\n",
    " # Customer age (Normal with truncation)\n",
    " customer_age = np.random.normal(loc=45, scale=15, size=n_samples)\n",
    " customer_age = np.clip(customer_age, 18, 80)\n",
    "\n",
    " # Transaction amounts (Pareto distribution - 80/20 rule)\n",
    " transaction_base = np.random.pareto(a=1.16, size=n_samples) + 1\n",
    " transaction_amounts = transaction_base * 100\n",
    "\n",
    " # Marketing campaign results (Uniform distribution)\n",
    " campaign_effectiveness = np.random.uniform(low=0, high=100, size=n_samples)\n",
    "\n",
    " # Bimodal distribution for customer segments\n",
    " segment_1 = np.random.normal(loc=30, scale=8, size=n_samples//2)\n",
    " segment_2 = np.random.normal(loc=70, scale=10, size=n_samples//2)\n",
    " customer_lifetime_value = np.concatenate([segment_1, segment_2])\n",
    " np.random.shuffle(customer_lifetime_value)\n",
    "\n",
    " # Categorical variables\n",
    " regions = np.random.choice(['North', 'South', 'East', 'West'], n_samples, p=[0.3, 0.25, 0.25, 0.2])\n",
    " business_types = np.random.choice(['B2B', 'B2C', 'Enterprise'], n_samples, p=[0.4, 0.45, 0.15])\n",
    " customer_segments = np.random.choice(['Premium', 'Standard', 'Basic'], n_samples, p=[0.2, 0.5, 0.3])\n",
    "\n",
    " # Create seasonal patterns in some variables\n",
    " time_component = np.arange(n_samples) / n_samples * 4 * np.pi # 2 full cycles\n",
    " seasonal_sales = sales_revenue * (1 + 0.2 * np.sin(time_component))\n",
    "\n",
    " return pd.DataFrame({\n",
    " 'sales_revenue': sales_revenue,\n",
    " 'seasonal_sales': seasonal_sales,\n",
    " 'customer_satisfaction': customer_satisfaction,\n",
    " 'employee_performance': employee_performance,\n",
    " 'response_time': response_time,\n",
    " 'product_prices': product_prices,\n",
    " 'customer_age': customer_age,\n",
    " 'transaction_amounts': transaction_amounts,\n",
    " 'campaign_effectiveness': campaign_effectiveness,\n",
    " 'customer_lifetime_value': customer_lifetime_value,\n",
    " 'region': regions,\n",
    " 'business_type': business_types,\n",
    " 'customer_segment': customer_segments\n",
    " })\n",
    "\n",
    "# Generate dataset\n",
    "print(\" Generating business distribution dataset...\")\n",
    "df = generate_business_distribution_dataset(2000)\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(df.head())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c44d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BASIC DISTRIBUTION VISUALIZATION\n",
    "print(\" 1. BASIC DISTRIBUTION VISUALIZATION\")\n",
    "print(\"=\" * 38)\n",
    "\n",
    "# Create comprehensive distribution overview\n",
    "numerical_vars = ['sales_revenue', 'customer_satisfaction', 'employee_performance',\n",
    " 'response_time', 'product_prices', 'customer_age']\n",
    "\n",
    "fig = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=[var.replace('_', ' ').title() for var in numerical_vars],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "positions = [(1,1), (1,2), (2,1), (2,2), (3,1), (3,2)]\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "\n",
    "for i, var in enumerate(numerical_vars):\n",
    " row, col = positions[i]\n",
    "\n",
    " # Histogram\n",
    " fig.add_trace(\n",
    " go.Histogram(\n",
    " x=df[var],\n",
    " nbinsx=30,\n",
    " name=var,\n",
    " marker_color=colors[i],\n",
    " opacity=0.7,\n",
    " showlegend=False,\n",
    " hovertemplate=f\"<b>{var}</b><br>Value: %{{x:.2f}}<br>Count: %{{y}}<extra></extra>\"\n",
    " ),\n",
    " row=row, col=col\n",
    " )\n",
    "\n",
    "fig.update_layout(\n",
    " title=\"Distribution Overview: Key Business Variables\",\n",
    " height=800,\n",
    " showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Statistical summary with distribution characteristics\n",
    "print(\" Distribution Characteristics Summary:\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "for var in numerical_vars:\n",
    " data = df[var]\n",
    "\n",
    " # Basic statistics\n",
    " mean_val = data.mean()\n",
    " median_val = data.median()\n",
    " std_val = data.std()\n",
    " skewness = stats.skew(data)\n",
    " kurtosis = stats.kurtosis(data)\n",
    "\n",
    " # Distribution shape analysis\n",
    " if abs(skewness) < 0.5:\n",
    " skew_desc = \"approximately symmetric\"\n",
    " elif skewness > 0.5:\n",
    " skew_desc = \"right-skewed (positive)\"\n",
    " else:\n",
    " skew_desc = \"left-skewed (negative)\"\n",
    "\n",
    " if kurtosis > 0:\n",
    " kurt_desc = \"heavy-tailed (leptokurtic)\"\n",
    " elif kurtosis < 0:\n",
    " kurt_desc = \"light-tailed (platykurtic)\"\n",
    " else:\n",
    " kurt_desc = \"normal tails (mesokurtic)\"\n",
    "\n",
    " print(f\"\\n{var.replace('_', ' ').title()}:\")\n",
    " print(f\" • Mean: {mean_val:.2f} | Median: {median_val:.2f}\")\n",
    " print(f\" • Standard Deviation: {std_val:.2f}\")\n",
    " print(f\" • Skewness: {skewness:.3f} ({skew_desc})\")\n",
    " print(f\" • Kurtosis: {kurtosis:.3f} ({kurt_desc})\")\n",
    " print(f\" • Range: [{data.min():.2f}, {data.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ADVANCED DISTRIBUTION PLOTS\n",
    "print(\"\\n 2. ADVANCED DISTRIBUTION PLOTS\")\n",
    "print(\"=\" * 36)\n",
    "\n",
    "# 2.1 Density Plots with Multiple Variables\n",
    "print(\"2.1 Kernel Density Estimation:\")\n",
    "\n",
    "# Create density plots for key variables\n",
    "fig_density = go.Figure()\n",
    "\n",
    "key_vars = ['sales_revenue', 'customer_satisfaction', 'employee_performance']\n",
    "colors_density = ['blue', 'red', 'green']\n",
    "\n",
    "for i, var in enumerate(key_vars):\n",
    " # Calculate KDE\n",
    " from scipy.stats import gaussian_kde\n",
    "\n",
    " data = df[var]\n",
    " kde = gaussian_kde(data)\n",
    " x_range = np.linspace(data.min(), data.max(), 200)\n",
    " density = kde(x_range)\n",
    "\n",
    " fig_density.add_trace(\n",
    " go.Scatter(\n",
    " x=x_range,\n",
    " y=density,\n",
    " mode='lines',\n",
    " fill='tonexty' if i > 0 else 'tozeroy',\n",
    " name=var.replace('_', ' ').title(),\n",
    " line=dict(color=colors_density[i], width=3),\n",
    " opacity=0.6,\n",
    " hovertemplate=f\"<b>{var}</b><br>Value: %{{x:.2f}}<br>Density: %{{y:.4f}}<extra></extra>\"\n",
    " )\n",
    " )\n",
    "\n",
    "fig_density.update_layout(\n",
    " title=\"Kernel Density Estimation: Distribution Comparison\",\n",
    " xaxis_title=\"Value (Standardized Scale)\",\n",
    " yaxis_title=\"Density\",\n",
    " height=500\n",
    ")\n",
    "fig_density.show()\n",
    "\n",
    "# 2.2 Box Plots with Outlier Analysis\n",
    "print(\"\\n2.2 Box Plot Analysis:\")\n",
    "\n",
    "# Standardize data for comparison\n",
    "scaler = StandardScaler()\n",
    "standardized_data = pd.DataFrame(\n",
    " scaler.fit_transform(df[key_vars]),\n",
    " columns=key_vars\n",
    ")\n",
    "\n",
    "fig_box = go.Figure()\n",
    "\n",
    "for i, var in enumerate(key_vars):\n",
    " fig_box.add_trace(\n",
    " go.Box(\n",
    " y=standardized_data[var],\n",
    " name=var.replace('_', ' ').title(),\n",
    " boxpoints='outliers',\n",
    " marker_color=colors_density[i],\n",
    " hovertemplate=f\"<b>{var}</b><br>Value: %{{y:.2f}}<br>Quartile Info:<br>%{{text}}<extra></extra>\"\n",
    " )\n",
    " )\n",
    "\n",
    "fig_box.update_layout(\n",
    " title=\"Box Plot Analysis: Distribution Comparison (Standardized)\",\n",
    " yaxis_title=\"Standardized Value\",\n",
    " height=500\n",
    ")\n",
    "fig_box.show()\n",
    "\n",
    "# 2.3 Violin Plots\n",
    "print(\"\\n2.3 Violin Plot Analysis:\")\n",
    "\n",
    "fig_violin = go.Figure()\n",
    "\n",
    "for i, var in enumerate(key_vars):\n",
    " fig_violin.add_trace(\n",
    " go.Violin(\n",
    " y=standardized_data[var],\n",
    " name=var.replace('_', ' ').title(),\n",
    " box_visible=True,\n",
    " meanline_visible=True,\n",
    " fillcolor=colors_density[i],\n",
    " opacity=0.6,\n",
    " line_color='black'\n",
    " )\n",
    " )\n",
    "\n",
    "fig_violin.update_layout(\n",
    " title=\"Violin Plots: Distribution Shape and Density\",\n",
    " yaxis_title=\"Standardized Value\",\n",
    " height=500\n",
    ")\n",
    "fig_violin.show()\n",
    "\n",
    "# Outlier Analysis\n",
    "print(\"\\n Outlier Analysis Summary:\")\n",
    "for var in key_vars:\n",
    " data = df[var]\n",
    " Q1 = data.quantile(0.25)\n",
    " Q3 = data.quantile(0.75)\n",
    " IQR = Q3 - Q1\n",
    " lower_bound = Q1 - 1.5 * IQR\n",
    " upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    " outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    " outlier_percentage = (len(outliers) / len(data)) * 100\n",
    "\n",
    " print(f\"• {var.replace('_', ' ').title()}: {len(outliers)} outliers ({outlier_percentage:.1f}%)\")\n",
    " if len(outliers) > 0:\n",
    " print(f\" Range: [{outliers.min():.2f}, {outliers.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3272ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. NORMALITY TESTING AND Q-Q PLOTS\n",
    "print(\"\\n 3. NORMALITY TESTING AND Q-Q PLOTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def perform_normality_tests(data, variable_name):\n",
    " \"\"\"Perform comprehensive normality testing\"\"\"\n",
    "\n",
    " # Shapiro-Wilk test (sample size <= 5000)\n",
    " if len(data) <= 5000:\n",
    " shapiro_stat, shapiro_p = stats.shapiro(data)\n",
    " else:\n",
    " shapiro_stat, shapiro_p = None, None\n",
    "\n",
    " # Kolmogorov-Smirnov test\n",
    " ks_stat, ks_p = stats.kstest(data, 'norm', args=(data.mean(), data.std()))\n",
    "\n",
    " # Anderson-Darling test\n",
    " ad_stat, ad_critical, ad_significance = stats.anderson(data, dist='norm')\n",
    "\n",
    " # Jarque-Bera test\n",
    " jb_stat, jb_p = stats.jarque_bera(data)\n",
    "\n",
    " return {\n",
    " 'variable': variable_name,\n",
    " 'shapiro_stat': shapiro_stat,\n",
    " 'shapiro_p': shapiro_p,\n",
    " 'ks_stat': ks_stat,\n",
    " 'ks_p': ks_p,\n",
    " 'ad_stat': ad_stat,\n",
    " 'jb_stat': jb_stat,\n",
    " 'jb_p': jb_p,\n",
    " 'is_normal': all([\n",
    " shapiro_p > 0.05 if shapiro_p is not None else True,\n",
    " ks_p > 0.05,\n",
    " jb_p > 0.05\n",
    " ])\n",
    " }\n",
    "\n",
    "# Test normality for all numerical variables\n",
    "normality_results = []\n",
    "for var in numerical_vars:\n",
    " result = perform_normality_tests(df[var], var)\n",
    " normality_results.append(result)\n",
    "\n",
    "# Create Q-Q plots for normality assessment\n",
    "fig_qq = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=[f\"Q-Q Plot: {var.replace('_', ' ').title()}\" for var in numerical_vars]\n",
    ")\n",
    "\n",
    "for i, var in enumerate(numerical_vars):\n",
    " row = (i // 2) + 1\n",
    " col = (i % 2) + 1\n",
    "\n",
    " # Generate Q-Q plot data\n",
    " data = df[var]\n",
    " standardized_data = (data - data.mean()) / data.std()\n",
    "\n",
    " # Theoretical quantiles (normal distribution)\n",
    " n = len(data)\n",
    " theoretical_quantiles = stats.norm.ppf(np.arange(1, n + 1) / (n + 1))\n",
    "\n",
    " # Sort sample quantiles\n",
    " sample_quantiles = np.sort(standardized_data)\n",
    "\n",
    " # Add Q-Q plot\n",
    " fig_qq.add_trace(\n",
    " go.Scatter(\n",
    " x=theoretical_quantiles,\n",
    " y=sample_quantiles,\n",
    " mode='markers',\n",
    " marker=dict(size=4, opacity=0.6),\n",
    " name=var,\n",
    " showlegend=False,\n",
    " hovertemplate=f\"<b>{var}</b><br>Theoretical: %{{x:.2f}}<br>Sample: %{{y:.2f}}<extra></extra>\"\n",
    " ),\n",
    " row=row, col=col\n",
    " )\n",
    "\n",
    " # Add reference line (perfect normal distribution)\n",
    " min_val = min(theoretical_quantiles.min(), sample_quantiles.min())\n",
    " max_val = max(theoretical_quantiles.max(), sample_quantiles.max())\n",
    "\n",
    " fig_qq.add_trace(\n",
    " go.Scatter(\n",
    " x=[min_val, max_val],\n",
    " y=[min_val, max_val],\n",
    " mode='lines',\n",
    " line=dict(color='red', dash='dash'),\n",
    " name='Perfect Normal',\n",
    " showlegend=(i == 0)\n",
    " ),\n",
    " row=row, col=col\n",
    " )\n",
    "\n",
    "fig_qq.update_layout(\n",
    " title=\"Q-Q Plots: Normality Assessment\",\n",
    " height=800\n",
    ")\n",
    "fig_qq.show()\n",
    "\n",
    "# Display normality test results\n",
    "print(\" Normality Test Results:\")\n",
    "print(\"=\" * 26)\n",
    "\n",
    "normality_df = pd.DataFrame(normality_results)\n",
    "print(\"Variable | Shapiro-Wilk p | K-S p | Jarque-Bera p | Is Normal?\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for _, row in normality_df.iterrows():\n",
    " var_name = row['variable'].replace('_', ' ').title()[:12].ljust(12)\n",
    " shapiro_p = f\"{row['shapiro_p']:.4f}\" if row['shapiro_p'] is not None else \"N/A\"\n",
    " ks_p = f\"{row['ks_p']:.4f}\"\n",
    " jb_p = f\"{row['jb_p']:.4f}\"\n",
    " is_normal = \"\" if row['is_normal'] else \"\"\n",
    "\n",
    " print(f\"{var_name} | {shapiro_p:>13} | {ks_p:>5} | {jb_p:>11} | {is_normal:>9}\")\n",
    "\n",
    "print(f\"\\n Normality Summary:\")\n",
    "normal_vars = [r['variable'] for r in normality_results if r['is_normal']]\n",
    "non_normal_vars = [r['variable'] for r in normality_results if not r['is_normal']]\n",
    "\n",
    "print(f\"• Normal distributions: {len(normal_vars)} variables\")\n",
    "if normal_vars:\n",
    " print(f\" - {', '.join([v.replace('_', ' ').title() for v in normal_vars])}\")\n",
    "\n",
    "print(f\"• Non-normal distributions: {len(non_normal_vars)} variables\")\n",
    "if non_normal_vars:\n",
    " print(f\" - {', '.join([v.replace('_', ' ').title() for v in non_normal_vars])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1588b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DISTRIBUTION FITTING AND MODEL SELECTION\n",
    "print(\"\\n 4. DISTRIBUTION FITTING AND MODEL SELECTION\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "def fit_multiple_distributions(data, distributions=None):\n",
    " \"\"\"Fit multiple distributions and return best fit based on AIC/BIC\"\"\"\n",
    "\n",
    " if distributions is None:\n",
    " distributions = [\n",
    " ('Normal', stats.norm),\n",
    " ('Log-Normal', stats.lognorm),\n",
    " ('Exponential', stats.expon),\n",
    " ('Gamma', stats.gamma),\n",
    " ('Beta', stats.beta),\n",
    " ('Uniform', stats.uniform)\n",
    " ]\n",
    "\n",
    " results = []\n",
    "\n",
    " for name, distribution in distributions:\n",
    " try:\n",
    " # Fit distribution\n",
    " if name == 'Beta':\n",
    " # Beta distribution needs data scaled to [0,1]\n",
    " data_scaled = (data - data.min()) / (data.max() - data.min())\n",
    " params = distribution.fit(data_scaled)\n",
    " # Scale back for evaluation\n",
    " fitted_data = data_scaled\n",
    " else:\n",
    " params = distribution.fit(data)\n",
    " fitted_data = data\n",
    "\n",
    " # Calculate log-likelihood\n",
    " log_likelihood = np.sum(distribution.logpdf(fitted_data, *params))\n",
    "\n",
    " # Calculate AIC and BIC\n",
    " k = len(params) # number of parameters\n",
    " n = len(data) # sample size\n",
    " aic = 2 * k - 2 * log_likelihood\n",
    " bic = k * np.log(n) - 2 * log_likelihood\n",
    "\n",
    " # Kolmogorov-Smirnov test\n",
    " if name == 'Beta':\n",
    " ks_stat, ks_p = stats.kstest(fitted_data,\n",
    " lambda x: distribution.cdf(x, *params))\n",
    " else:\n",
    " ks_stat, ks_p = stats.kstest(data,\n",
    " lambda x: distribution.cdf(x, *params))\n",
    "\n",
    " results.append({\n",
    " 'distribution': name,\n",
    " 'parameters': params,\n",
    " 'log_likelihood': log_likelihood,\n",
    " 'aic': aic,\n",
    " 'bic': bic,\n",
    " 'ks_statistic': ks_stat,\n",
    " 'ks_p_value': ks_p\n",
    " })\n",
    "\n",
    " except Exception as e:\n",
    " print(f\"Failed to fit {name}: {e}\")\n",
    " continue\n",
    "\n",
    " # Sort by AIC (lower is better)\n",
    " results.sort(key=lambda x: x['aic'])\n",
    " return results\n",
    "\n",
    "# Fit distributions for key variables\n",
    "variables_to_fit = ['sales_revenue', 'response_time', 'customer_satisfaction']\n",
    "\n",
    "fitting_results = {}\n",
    "for var in variables_to_fit:\n",
    " print(f\"\\n Fitting distributions for {var.replace('_', ' ').title()}:\")\n",
    "\n",
    " data = df[var].values\n",
    " # Remove any infinite or NaN values\n",
    " data = data[np.isfinite(data)]\n",
    "\n",
    " results = fit_multiple_distributions(data)\n",
    " fitting_results[var] = results\n",
    "\n",
    " print(\"Rank | Distribution | AIC | BIC | K-S p-value\")\n",
    " print(\"-\" * 50)\n",
    "\n",
    " for i, result in enumerate(results[:5]): # Show top 5\n",
    " rank = i + 1\n",
    " dist_name = result['distribution'][:12].ljust(12)\n",
    " aic = f\"{result['aic']:.1f}\"\n",
    " bic = f\"{result['bic']:.1f}\"\n",
    " ks_p = f\"{result['ks_p_value']:.4f}\"\n",
    "\n",
    " print(f\"{rank:>4} | {dist_name} | {aic:>7} | {bic:>7} | {ks_p:>11}\")\n",
    "\n",
    "# Visualize best-fitting distributions\n",
    "fig_fits = make_subplots(\n",
    " rows=1, cols=3,\n",
    " subplot_titles=[var.replace('_', ' ').title() for var in variables_to_fit]\n",
    ")\n",
    "\n",
    "for i, var in enumerate(variables_to_fit):\n",
    " col = i + 1\n",
    " data = df[var].values\n",
    " data = data[np.isfinite(data)]\n",
    "\n",
    " # Plot histogram\n",
    " fig_fits.add_trace(\n",
    " go.Histogram(\n",
    " x=data,\n",
    " nbinsx=30,\n",
    " histnorm='probability density',\n",
    " name=f'{var} Data',\n",
    " marker_color='lightblue',\n",
    " opacity=0.7,\n",
    " showlegend=(i == 0)\n",
    " ),\n",
    " row=1, col=col\n",
    " )\n",
    "\n",
    " # Plot best-fitting distribution\n",
    " best_fit = fitting_results[var][0]\n",
    " dist_name = best_fit['distribution']\n",
    " params = best_fit['parameters']\n",
    "\n",
    " # Generate distribution curve\n",
    " if dist_name == 'Beta':\n",
    " data_scaled = (data - data.min()) / (data.max() - data.min())\n",
    " x_range = np.linspace(0, 1, 200)\n",
    " y_range = getattr(stats, dist_name.lower().replace('-', '')).pdf(x_range, *params)\n",
    " # Scale back x_range\n",
    " x_range = x_range * (data.max() - data.min()) + data.min()\n",
    " else:\n",
    " x_range = np.linspace(data.min(), data.max(), 200)\n",
    " y_range = getattr(stats, dist_name.lower().replace('-', '')).pdf(x_range, *params)\n",
    "\n",
    " fig_fits.add_trace(\n",
    " go.Scatter(\n",
    " x=x_range,\n",
    " y=y_range,\n",
    " mode='lines',\n",
    " name=f'Best Fit: {dist_name}',\n",
    " line=dict(color='red', width=3),\n",
    " showlegend=(i == 0)\n",
    " ),\n",
    " row=1, col=col\n",
    " )\n",
    "\n",
    "fig_fits.update_layout(\n",
    " title=\"Distribution Fitting: Best Models vs Actual Data\",\n",
    " height=500\n",
    ")\n",
    "fig_fits.show()\n",
    "\n",
    "# Summary of best fits\n",
    "print(f\"\\n Best Distribution Fits Summary:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "for var in variables_to_fit:\n",
    " best_fit = fitting_results[var][0]\n",
    " var_name = var.replace('_', ' ').title()\n",
    " dist_name = best_fit['distribution']\n",
    " aic = best_fit['aic']\n",
    " ks_p = best_fit['ks_p_value']\n",
    "\n",
    " goodness = \"Excellent\" if ks_p > 0.1 else \"Good\" if ks_p > 0.05 else \"Fair\" if ks_p > 0.01 else \"Poor\"\n",
    "\n",
    " print(f\"• {var_name}: {dist_name} distribution\")\n",
    " print(f\" - AIC: {aic:.1f}\")\n",
    " print(f\" - Goodness of fit: {goodness} (p={ks_p:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. GROUP COMPARISONS AND CATEGORICAL ANALYSIS\n",
    "print(\"\\n 5. GROUP COMPARISONS AND CATEGORICAL ANALYSIS\")\n",
    "print(\"=\" * 52)\n",
    "\n",
    "# 5.1 Distribution by categorical variables\n",
    "print(\"5.1 Regional Distribution Analysis:\")\n",
    "\n",
    "# Sales revenue by region\n",
    "fig_region = go.Figure()\n",
    "\n",
    "regions = df['region'].unique()\n",
    "colors_region = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "for i, region in enumerate(regions):\n",
    " region_data = df[df['region'] == region]['sales_revenue']\n",
    "\n",
    " fig_region.add_trace(\n",
    " go.Histogram(\n",
    " x=region_data,\n",
    " name=region,\n",
    " opacity=0.7,\n",
    " nbinsx=25,\n",
    " histnorm='probability density',\n",
    " marker_color=colors_region[i]\n",
    " )\n",
    " )\n",
    "\n",
    "fig_region.update_layout(\n",
    " title=\"Sales Revenue Distribution by Region\",\n",
    " xaxis_title=\"Sales Revenue ($)\",\n",
    " yaxis_title=\"Probability Density\",\n",
    " barmode='overlay',\n",
    " height=500\n",
    ")\n",
    "fig_region.show()\n",
    "\n",
    "# 5.2 Business type comparison\n",
    "print(\"\\n5.2 Business Type Distribution Analysis:\")\n",
    "\n",
    "# Create side-by-side box plots\n",
    "fig_business = go.Figure()\n",
    "\n",
    "business_types = df['business_type'].unique()\n",
    "colors_business = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "for i, btype in enumerate(business_types):\n",
    " btype_data = df[df['business_type'] == btype]['customer_satisfaction']\n",
    "\n",
    " fig_business.add_trace(\n",
    " go.Box(\n",
    " y=btype_data,\n",
    " name=btype,\n",
    " marker_color=colors_business[i],\n",
    " boxpoints='outliers'\n",
    " )\n",
    " )\n",
    "\n",
    "fig_business.update_layout(\n",
    " title=\"Customer Satisfaction Distribution by Business Type\",\n",
    " yaxis_title=\"Customer Satisfaction (1-10)\",\n",
    " height=500\n",
    ")\n",
    "fig_business.show()\n",
    "\n",
    "# 5.3 Statistical comparison tests\n",
    "print(\"\\n5.3 Statistical Comparison Tests:\")\n",
    "\n",
    "# ANOVA test for regional differences in sales\n",
    "regional_groups = [df[df['region'] == region]['sales_revenue'] for region in regions]\n",
    "f_stat, p_value = stats.f_oneway(*regional_groups)\n",
    "\n",
    "print(f\"Regional Sales Revenue ANOVA:\")\n",
    "print(f\"• F-statistic: {f_stat:.3f}\")\n",
    "print(f\"• P-value: {p_value:.6f}\")\n",
    "print(f\"• Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "\n",
    "# Post-hoc analysis (Tukey's HSD)\n",
    "if p_value < 0.05:\n",
    " from scipy.stats import tukey_hsd\n",
    "\n",
    " # Prepare data for Tukey's test\n",
    " sales_data = []\n",
    " region_labels = []\n",
    "\n",
    " for region in regions:\n",
    " region_sales = df[df['region'] == region]['sales_revenue']\n",
    " sales_data.extend(region_sales)\n",
    " region_labels.extend([region] * len(region_sales))\n",
    "\n",
    " # Perform Tukey's HSD test\n",
    " print(f\"\\nTukey's HSD Post-hoc Analysis:\")\n",
    " regional_means = df.groupby('region')['sales_revenue'].mean().sort_values(ascending=False)\n",
    " print(\"Regional Sales Ranking (highest to lowest):\")\n",
    " for i, (region, mean_sales) in enumerate(regional_means.items(), 1):\n",
    " print(f\" {i}. {region}: ${mean_sales:,.0f}\")\n",
    "\n",
    "# Customer segment analysis\n",
    "print(f\"\\n5.4 Customer Segment Analysis:\")\n",
    "\n",
    "# Compare customer lifetime value across segments\n",
    "segments = df['customer_segment'].unique()\n",
    "segment_stats = df.groupby('customer_segment')['customer_lifetime_value'].agg([\n",
    " 'count', 'mean', 'std', 'median'\n",
    "]).round(2)\n",
    "\n",
    "print(\"Customer Lifetime Value by Segment:\")\n",
    "print(segment_stats)\n",
    "\n",
    "# Create violin plot for segment comparison\n",
    "fig_segments = go.Figure()\n",
    "\n",
    "for i, segment in enumerate(segments):\n",
    " segment_data = df[df['customer_segment'] == segment]['customer_lifetime_value']\n",
    "\n",
    " fig_segments.add_trace(\n",
    " go.Violin(\n",
    " y=segment_data,\n",
    " name=segment,\n",
    " box_visible=True,\n",
    " meanline_visible=True,\n",
    " fillcolor=colors_business[i % len(colors_business)],\n",
    " opacity=0.6\n",
    " )\n",
    " )\n",
    "\n",
    "fig_segments.update_layout(\n",
    " title=\"Customer Lifetime Value Distribution by Segment\",\n",
    " yaxis_title=\"Customer Lifetime Value ($)\",\n",
    " height=500\n",
    ")\n",
    "fig_segments.show()\n",
    "\n",
    "# Kruskal-Wallis test (non-parametric alternative to ANOVA)\n",
    "segment_groups = [df[df['customer_segment'] == seg]['customer_lifetime_value'] for seg in segments]\n",
    "kw_stat, kw_p = stats.kruskal(*segment_groups)\n",
    "\n",
    "print(f\"\\nCustomer Segment Lifetime Value Comparison (Kruskal-Wallis):\")\n",
    "print(f\"• H-statistic: {kw_stat:.3f}\")\n",
    "print(f\"• P-value: {kw_p:.6f}\")\n",
    "print(f\"• Significant difference: {'Yes' if kw_p < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e66c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. BIMODAL AND MULTIMODAL DISTRIBUTION ANALYSIS\n",
    "print(\"\\n 6. BIMODAL AND MULTIMODAL DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 52)\n",
    "\n",
    "# Analyze the bimodal customer lifetime value distribution\n",
    "def detect_multimodality(data, method='dip'):\n",
    " \"\"\"Detect multimodality in data\"\"\"\n",
    "\n",
    " # Hartigan's dip test for unimodality\n",
    " try:\n",
    " from diptest import diptest\n",
    " dip_stat, dip_p = diptest(data)\n",
    " is_multimodal_dip = dip_p < 0.05\n",
    " except ImportError:\n",
    " print(\"diptest not available, using alternative method\")\n",
    " dip_stat, dip_p = None, None\n",
    " is_multimodal_dip = None\n",
    "\n",
    " # Alternative: Count peaks in KDE\n",
    " from scipy.stats import gaussian_kde\n",
    " from scipy.signal import find_peaks\n",
    "\n",
    " kde = gaussian_kde(data)\n",
    " x_range = np.linspace(data.min(), data.max(), 1000)\n",
    " density = kde(x_range)\n",
    "\n",
    " # Find peaks\n",
    " peaks, _ = find_peaks(density, height=0.001, distance=50)\n",
    " n_peaks = len(peaks)\n",
    "\n",
    " return {\n",
    " 'dip_statistic': dip_stat,\n",
    " 'dip_p_value': dip_p,\n",
    " 'is_multimodal_dip': is_multimodal_dip,\n",
    " 'n_peaks': n_peaks,\n",
    " 'is_multimodal_peaks': n_peaks > 1,\n",
    " 'peak_locations': x_range[peaks] if n_peaks > 0 else []\n",
    " }\n",
    "\n",
    "# Analyze customer lifetime value for multimodality\n",
    "clv_data = df['customer_lifetime_value'].values\n",
    "multimodal_results = detect_multimodality(clv_data)\n",
    "\n",
    "print(\" Multimodality Analysis - Customer Lifetime Value:\")\n",
    "print(f\"• Number of peaks detected: {multimodal_results['n_peaks']}\")\n",
    "print(f\"• Is multimodal (peaks): {'Yes' if multimodal_results['is_multimodal_peaks'] else 'No'}\")\n",
    "\n",
    "if multimodal_results['dip_p_value'] is not None:\n",
    " print(f\"• Dip test p-value: {multimodal_results['dip_p_value']:.6f}\")\n",
    " print(f\"• Is multimodal (dip test): {'Yes' if multimodal_results['is_multimodal_dip'] else 'No'}\")\n",
    "\n",
    "if multimodal_results['peak_locations'].size > 0:\n",
    " print(f\"• Peak locations: {multimodal_results['peak_locations'].round(2)}\")\n",
    "\n",
    "# Visualize multimodal distribution\n",
    "fig_multimodal = go.Figure()\n",
    "\n",
    "# Histogram\n",
    "fig_multimodal.add_trace(\n",
    " go.Histogram(\n",
    " x=clv_data,\n",
    " nbinsx=40,\n",
    " histnorm='probability density',\n",
    " name='Customer LTV',\n",
    " marker_color='lightblue',\n",
    " opacity=0.7\n",
    " )\n",
    ")\n",
    "\n",
    "# KDE overlay\n",
    "from scipy.stats import gaussian_kde\n",
    "kde = gaussian_kde(clv_data)\n",
    "x_range = np.linspace(clv_data.min(), clv_data.max(), 1000)\n",
    "density = kde(x_range)\n",
    "\n",
    "fig_multimodal.add_trace(\n",
    " go.Scatter(\n",
    " x=x_range,\n",
    " y=density,\n",
    " mode='lines',\n",
    " name='KDE',\n",
    " line=dict(color='red', width=3)\n",
    " )\n",
    ")\n",
    "\n",
    "# Mark peaks\n",
    "if multimodal_results['peak_locations'].size > 0:\n",
    " peak_densities = kde(multimodal_results['peak_locations'])\n",
    "\n",
    " fig_multimodal.add_trace(\n",
    " go.Scatter(\n",
    " x=multimodal_results['peak_locations'],\n",
    " y=peak_densities,\n",
    " mode='markers',\n",
    " name='Detected Peaks',\n",
    " marker=dict(color='green', size=12, symbol='star')\n",
    " )\n",
    " )\n",
    "\n",
    "fig_multimodal.update_layout(\n",
    " title=\"Multimodal Distribution Analysis: Customer Lifetime Value\",\n",
    " xaxis_title=\"Customer Lifetime Value ($)\",\n",
    " yaxis_title=\"Probability Density\",\n",
    " height=500\n",
    ")\n",
    "fig_multimodal.show()\n",
    "\n",
    "# Mixture model fitting for bimodal distribution\n",
    "print(\"\\n6.1 Gaussian Mixture Model Analysis:\")\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fit Gaussian Mixture Models with different numbers of components\n",
    "n_components_range = range(1, 6)\n",
    "aic_scores = []\n",
    "bic_scores = []\n",
    "log_likelihoods = []\n",
    "\n",
    "clv_reshaped = clv_data.reshape(-1, 1)\n",
    "\n",
    "for n_components in n_components_range:\n",
    " gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    " gmm.fit(clv_reshaped)\n",
    "\n",
    " aic_scores.append(gmm.aic(clv_reshaped))\n",
    " bic_scores.append(gmm.bic(clv_reshaped))\n",
    " log_likelihoods.append(gmm.score(clv_reshaped))\n",
    "\n",
    "# Find optimal number of components\n",
    "optimal_components_aic = n_components_range[np.argmin(aic_scores)]\n",
    "optimal_components_bic = n_components_range[np.argmin(bic_scores)]\n",
    "\n",
    "print(f\"• Optimal components (AIC): {optimal_components_aic}\")\n",
    "print(f\"• Optimal components (BIC): {optimal_components_bic}\")\n",
    "\n",
    "# Fit final model with optimal components\n",
    "optimal_n = optimal_components_bic # BIC is more conservative\n",
    "final_gmm = GaussianMixture(n_components=optimal_n, random_state=42)\n",
    "final_gmm.fit(clv_reshaped)\n",
    "\n",
    "# Get component parameters\n",
    "print(f\"\\nGaussian Mixture Model Components (n={optimal_n}):\")\n",
    "for i in range(optimal_n):\n",
    " mean = final_gmm.means_[i][0]\n",
    " std = np.sqrt(final_gmm.covariances_[i][0][0])\n",
    " weight = final_gmm.weights_[i]\n",
    "\n",
    " print(f\"• Component {i+1}: Mean={mean:.2f}, Std={std:.2f}, Weight={weight:.3f}\")\n",
    "\n",
    "# Predict component memberships\n",
    "component_labels = final_gmm.predict(clv_reshaped)\n",
    "component_probs = final_gmm.predict_proba(clv_reshaped)\n",
    "\n",
    "# Visualize mixture components\n",
    "fig_mixture = go.Figure()\n",
    "\n",
    "# Original data\n",
    "fig_mixture.add_trace(\n",
    " go.Histogram(\n",
    " x=clv_data,\n",
    " nbinsx=40,\n",
    " histnorm='probability density',\n",
    " name='Original Data',\n",
    " marker_color='lightgray',\n",
    " opacity=0.5\n",
    " )\n",
    ")\n",
    "\n",
    "# Plot each component\n",
    "colors_comp = ['red', 'blue', 'green', 'purple', 'orange']\n",
    "x_plot = np.linspace(clv_data.min(), clv_data.max(), 1000)\n",
    "\n",
    "for i in range(optimal_n):\n",
    " mean = final_gmm.means_[i][0]\n",
    " std = np.sqrt(final_gmm.covariances_[i][0][0])\n",
    " weight = final_gmm.weights_[i]\n",
    "\n",
    " component_density = weight * stats.norm.pdf(x_plot, mean, std)\n",
    "\n",
    " fig_mixture.add_trace(\n",
    " go.Scatter(\n",
    " x=x_plot,\n",
    " y=component_density,\n",
    " mode='lines',\n",
    " name=f'Component {i+1}',\n",
    " line=dict(color=colors_comp[i], width=2, dash='dash')\n",
    " )\n",
    " )\n",
    "\n",
    "# Overall mixture\n",
    "mixture_density = np.sum([\n",
    " final_gmm.weights_[i] * stats.norm.pdf(x_plot, final_gmm.means_[i][0],\n",
    " np.sqrt(final_gmm.covariances_[i][0][0]))\n",
    " for i in range(optimal_n)\n",
    "], axis=0)\n",
    "\n",
    "fig_mixture.add_trace(\n",
    " go.Scatter(\n",
    " x=x_plot,\n",
    " y=mixture_density,\n",
    " mode='lines',\n",
    " name='Mixture Model',\n",
    " line=dict(color='black', width=3)\n",
    " )\n",
    ")\n",
    "\n",
    "fig_mixture.update_layout(\n",
    " title=f\"Gaussian Mixture Model: {optimal_n} Components\",\n",
    " xaxis_title=\"Customer Lifetime Value ($)\",\n",
    " yaxis_title=\"Probability Density\",\n",
    " height=500\n",
    ")\n",
    "fig_mixture.show()\n",
    "\n",
    "# Business interpretation of components\n",
    "if optimal_n > 1:\n",
    " print(f\"\\n Business Interpretation:\")\n",
    " component_means = [final_gmm.means_[i][0] for i in range(optimal_n)]\n",
    " sorted_components = np.argsort(component_means)\n",
    "\n",
    " component_names = ['Low-Value', 'Mid-Value', 'High-Value', 'Premium', 'Ultra-Premium'][:optimal_n]\n",
    "\n",
    " for i, comp_idx in enumerate(sorted_components):\n",
    " mean_val = final_gmm.means_[comp_idx][0]\n",
    " weight = final_gmm.weights_[comp_idx]\n",
    " n_customers = int(weight * len(df))\n",
    "\n",
    " print(f\"• {component_names[i]} Customers: {n_customers:,} customers ({weight:.1%})\")\n",
    " print(f\" - Average LTV: ${mean_val:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\n",
    "print(\"\\n 7. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "def generate_distribution_insights(dataframe, fitting_results, multimodal_results):\n",
    " \"\"\"Generate comprehensive business insights from distribution analysis\"\"\"\n",
    "\n",
    " insights = {\n",
    " 'distribution_characteristics': [],\n",
    " 'risk_assessment': [],\n",
    " 'optimization_opportunities': [],\n",
    " 'customer_segmentation': [],\n",
    " 'operational_insights': []\n",
    " }\n",
    "\n",
    " # Distribution characteristics insights\n",
    " sales_revenue_stats = dataframe['sales_revenue'].describe()\n",
    " sales_cv = dataframe['sales_revenue'].std() / dataframe['sales_revenue'].mean()\n",
    "\n",
    " insights['distribution_characteristics'].append(\n",
    " f\"Sales revenue shows high variability (CV={sales_cv:.2f}) indicating diverse business performance\"\n",
    " )\n",
    "\n",
    " # Customer satisfaction analysis\n",
    " satisfaction_skew = stats.skew(dataframe['customer_satisfaction'])\n",
    " if satisfaction_skew > 0.5:\n",
    " insights['optimization_opportunities'].append(\n",
    " \"Customer satisfaction is right-skewed - focus on improving low-satisfaction outliers\"\n",
    " )\n",
    " elif satisfaction_skew < -0.5:\n",
    " insights['optimization_opportunities'].append(\n",
    " \"Customer satisfaction is left-skewed - most customers are highly satisfied\"\n",
    " )\n",
    "\n",
    " # Response time analysis\n",
    " if 'response_time' in fitting_results:\n",
    " best_fit = fitting_results['response_time'][0]\n",
    " if best_fit['distribution'] == 'Exponential':\n",
    " insights['operational_insights'].append(\n",
    " \"Response time follows exponential distribution - typical of system performance metrics\"\n",
    " )\n",
    "\n",
    " # Multimodal customer analysis\n",
    " if multimodal_results['is_multimodal_peaks']:\n",
    " insights['customer_segmentation'].append(\n",
    " f\"Customer lifetime value shows {multimodal_results['n_peaks']} distinct segments - \"\n",
    " \"strong evidence for targeted marketing strategies\"\n",
    " )\n",
    "\n",
    " # Risk assessment\n",
    " revenue_outliers = dataframe['sales_revenue'].quantile(0.95)\n",
    " high_revenue_prop = (dataframe['sales_revenue'] > revenue_outliers).mean()\n",
    "\n",
    " insights['risk_assessment'].append(\n",
    " f\"Top 5% of businesses generate disproportionate revenue - \"\n",
    " f\"concentration risk in {high_revenue_prop:.1%} of customer base\"\n",
    " )\n",
    "\n",
    " return insights\n",
    "\n",
    "# Generate insights\n",
    "distribution_insights = generate_distribution_insights(df, fitting_results, multimodal_results)\n",
    "\n",
    "# Create comprehensive business dashboard\n",
    "fig_dashboard = make_subplots(\n",
    " rows=2, cols=2,\n",
    " subplot_titles=(\n",
    " \"Revenue Distribution Risk Profile\",\n",
    " \"Customer Satisfaction Performance\",\n",
    " \"Operational Efficiency Metrics\",\n",
    " \"Customer Segment Value Analysis\"\n",
    " ),\n",
    " specs=[[{\"type\": \"scatter\"}, {\"type\": \"bar\"}],\n",
    " [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# 1. Revenue Distribution Risk Profile\n",
    "revenue_percentiles = np.percentile(df['sales_revenue'], [10, 25, 50, 75, 90, 95, 99])\n",
    "percentile_labels = ['P10', 'P25', 'P50', 'P75', 'P90', 'P95', 'P99']\n",
    "\n",
    "fig_dashboard.add_trace(\n",
    " go.Scatter(\n",
    " x=percentile_labels,\n",
    " y=revenue_percentiles,\n",
    " mode='markers+lines',\n",
    " marker=dict(size=10, color='red'),\n",
    " line=dict(color='red', width=3),\n",
    " name='Revenue Percentiles'\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Customer Satisfaction Performance\n",
    "satisfaction_ranges = pd.cut(df['customer_satisfaction'],\n",
    " bins=[0, 5, 7, 8.5, 10],\n",
    " labels=['Poor', 'Fair', 'Good', 'Excellent'])\n",
    "satisfaction_counts = satisfaction_ranges.value_counts()\n",
    "\n",
    "fig_dashboard.add_trace(\n",
    " go.Bar(\n",
    " x=satisfaction_counts.index,\n",
    " y=satisfaction_counts.values,\n",
    " marker_color=['red', 'orange', 'lightgreen', 'darkgreen'],\n",
    " name='Satisfaction Distribution'\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Operational Efficiency Metrics\n",
    "response_time_bins = pd.cut(df['response_time'],\n",
    " bins=[0, 1, 2, 3, np.inf],\n",
    " labels=['Excellent (<1s)', 'Good (1-2s)', 'Fair (2-3s)', 'Poor (>3s)'])\n",
    "response_counts = response_time_bins.value_counts()\n",
    "\n",
    "fig_dashboard.add_trace(\n",
    " go.Bar(\n",
    " x=response_counts.index,\n",
    " y=response_counts.values,\n",
    " marker_color=['darkgreen', 'lightgreen', 'orange', 'red'],\n",
    " name='Response Time Distribution'\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Customer Segment Value Analysis\n",
    "if multimodal_results['n_peaks'] > 1:\n",
    " # Use mixture model components\n",
    " component_labels = final_gmm.predict(df['customer_lifetime_value'].values.reshape(-1, 1))\n",
    " component_counts = pd.Series(component_labels).value_counts().sort_index()\n",
    " component_names = [f'Segment {i+1}' for i in range(len(component_counts))]\n",
    "\n",
    " fig_dashboard.add_trace(\n",
    " go.Bar(\n",
    " x=component_names,\n",
    " y=component_counts.values,\n",
    " marker_color=['lightblue', 'blue', 'darkblue'][:len(component_counts)],\n",
    " name='Customer Segments'\n",
    " ),\n",
    " row=2, col=2\n",
    " )\n",
    "\n",
    "fig_dashboard.update_layout(\n",
    " title=\"Distribution Analysis Business Dashboard\",\n",
    " height=700,\n",
    " showlegend=False\n",
    ")\n",
    "fig_dashboard.show()\n",
    "\n",
    "# Display strategic insights\n",
    "print(\" Strategic Business Insights:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for category, insights_list in distribution_insights.items():\n",
    " if insights_list:\n",
    " print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    " for i, insight in enumerate(insights_list, 1):\n",
    " print(f\" {i}. {insight}\")\n",
    "\n",
    "# Risk and opportunity assessment\n",
    "print(f\"\\n Risk and Opportunity Assessment:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Revenue concentration analysis\n",
    "revenue_80th = df['sales_revenue'].quantile(0.8)\n",
    "top_20_percent = df[df['sales_revenue'] >= revenue_80th]\n",
    "revenue_concentration = top_20_percent['sales_revenue'].sum() / df['sales_revenue'].sum()\n",
    "\n",
    "print(f\"• Revenue Concentration: Top 20% of businesses generate {revenue_concentration:.1%} of total revenue\")\n",
    "\n",
    "# Customer satisfaction risk\n",
    "low_satisfaction = (df['customer_satisfaction'] < 6).mean()\n",
    "print(f\"• Customer Risk: {low_satisfaction:.1%} of customers have satisfaction scores below 6/10\")\n",
    "\n",
    "# Operational efficiency\n",
    "slow_responses = (df['response_time'] > 3).mean()\n",
    "print(f\"• Operational Risk: {slow_responses:.1%} of responses exceed 3-second threshold\")\n",
    "\n",
    "# Growth opportunities\n",
    "if multimodal_results['n_peaks'] > 1:\n",
    " low_value_segment = (component_labels == 0).mean() # Assuming component 0 is lowest\n",
    " print(f\"• Growth Opportunity: {low_value_segment:.1%} of customers in low-value segment - potential for upselling\")\n",
    "\n",
    "print(f\"\\n Strategic Recommendations:\")\n",
    "print(\"1. Implement customer success program for low-satisfaction segments\")\n",
    "print(\"2. Develop premium services for high-value customer segments\")\n",
    "print(\"3. Optimize system performance to reduce response time variability\")\n",
    "print(\"4. Create targeted marketing campaigns based on identified customer segments\")\n",
    "print(\"5. Monitor revenue concentration to reduce dependency on top customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ddde78",
   "metadata": {},
   "source": [
    "# LEARNING SUMMARY: Distribution Analysis\n",
    "\n",
    "## Key Concepts Mastered\n",
    "\n",
    "### 1. **Distribution Fundamentals**\n",
    "- **Shape Analysis**: Understanding skewness, kurtosis, and distribution characteristics\n",
    "- **Central Tendency**: Mean vs median relationships in different distributions\n",
    "- **Variability Measures**: Standard deviation, coefficient of variation, and range analysis\n",
    "- **Outlier Detection**: Statistical methods for identifying extreme values\n",
    "\n",
    "### 2. **Statistical Testing**\n",
    "- **Normality Tests**: Shapiro-Wilk, Kolmogorov-Smirnov, Jarque-Bera, Anderson-Darling\n",
    "- **Q-Q Plots**: Visual assessment of distribution fit against theoretical distributions\n",
    "- **Distribution Fitting**: AIC/BIC model selection for optimal distribution choice\n",
    "- **Goodness of Fit**: Statistical validation of distribution models\n",
    "\n",
    "### 3. **Advanced Distribution Analysis**\n",
    "- **Multimodal Detection**: Identifying multiple peaks and customer segments\n",
    "- **Mixture Models**: Gaussian mixture modeling for complex distributions\n",
    "- **Group Comparisons**: ANOVA, Kruskal-Wallis for categorical analysis\n",
    "- **Business Intelligence**: Converting statistical insights into actionable strategies\n",
    "\n",
    "## Business Applications\n",
    "\n",
    "### Risk Assessment\n",
    "- **Revenue Concentration**: Understanding dependency on high-value customers\n",
    "- **Performance Variability**: Identifying operational inconsistencies\n",
    "- **Customer Risk**: Quantifying satisfaction and retention risks\n",
    "- **Quality Control**: Using distribution analysis for process monitoring\n",
    "\n",
    "### Strategic Planning\n",
    "- Distribution analysis enables:\n",
    " - Customer segmentation through multimodal analysis\n",
    " - Resource allocation based on distribution characteristics\n",
    " - Performance benchmarking against theoretical models\n",
    " - Risk mitigation through outlier identification\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Tier 2: Predictive Modeling** - Use distribution insights for forecasting\n",
    "2. **Hypothesis Testing** - Build on distribution knowledge for statistical inference\n",
    "3. **Time Series Analysis** - Understand how distributions evolve over time\n",
    "4. **Multivariate Analysis** - Extend to joint distributions and correlations\n",
    "\n",
    "## Pro Tips\n",
    "\n",
    "- Always visualize distributions before applying statistical tests\n",
    "- Use multiple methods to confirm distribution characteristics\n",
    "- Consider business context when interpreting statistical results\n",
    "- Multimodal distributions often reveal hidden customer segments\n",
    "- Non-normal distributions may require specialized analytical approaches\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "- **Assumption Violations**: Applying normal-distribution methods to non-normal data\n",
    "- **Sample Size Effects**: Small samples can lead to unreliable distribution fitting\n",
    "- **Outlier Influence**: Extreme values can distort distribution shape analysis\n",
    "- **Over-interpretation**: Statistical significance doesn't always mean practical significance\n",
    "\n",
    "**Remember**: *Distribution analysis is the foundation for understanding your data's story - let the shape guide your analytical strategy!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}