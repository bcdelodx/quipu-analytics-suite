{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54c55df",
   "metadata": {},
   "source": [
    "# Tier 1: Descriptive Statistics\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "\n",
    "---\n",
    "\n",
    "> **Citation:**\n",
    "> Brandon Deloatch, \"Tier 1: Descriptive Statistics,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook is provided \"as-is\" for educational and research purposes. Users assume full responsibility for any results or applications derived from it.*\n",
    "\n",
    "---\n",
    "\n",
    "## Comprehensive Descriptive Statistics and Exploratory Analytics\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master fundamental descriptive statistics calculations\n",
    "- Understand measures of central tendency and variability\n",
    "- Learn distribution characteristics and data summarization\n",
    "- Apply statistical measures to real-world datasets\n",
    "\n",
    "**Cross-References:**\n",
    "- **Foundation For:** `Tier1_Distribution.ipynb` (distribution analysis)\n",
    "- **Complements:** `Tier1_Correlation.ipynb` (relationship analysis)\n",
    "- **Advanced:** `Tier3_Statistics.ipynb` (advanced statistical methods)\n",
    "\n",
    "**Key Applications:**\n",
    "- Data quality assessment and validation\n",
    "- Business intelligence and reporting\n",
    "- Research data summarization\n",
    "- Exploratory data analysis foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5e4bf",
   "metadata": {},
   "source": [
    "# Tier 1: Descriptive Statistics\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "\n",
    "---\n",
    "\n",
    "> **Citation:**\n",
    "> Brandon Deloatch, \"Tier 1: Descriptive Statistics,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook is provided \"as-is\" for educational and research purposes. Users assume full responsibility for any results or applications derived from it.*\n",
    "\n",
    "---\n",
    "\n",
    "## Comprehensive Descriptive Statistics and Exploratory Analytics\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master fundamental descriptive statistics calculations\n",
    "- Understand measures of central tendency and variability\n",
    "- Learn distribution characteristics and data summarization\n",
    "- Apply statistical measures to real-world datasets\n",
    "\n",
    "**Cross-References:**\n",
    "- **Foundation For:** `Tier1_Distribution.ipynb` (distribution analysis)\n",
    "- **Complements:** `Tier1_Correlation.ipynb` (relationship analysis)\n",
    "- **Advanced:** `Tier3_Statistics.ipynb` (advanced statistical methods)\n",
    "\n",
    "**Key Applications:**\n",
    "- Data quality assessment and validation\n",
    "- Business intelligence and reporting\n",
    "- Research data summarization\n",
    "- Exploratory data analysis foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tier 1: Descriptive & Exploratory Analytics\n",
    "# ============================================\n",
    "# Professional implementation with comprehensive visualizations and synthetic data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Tier 1: Descriptive & Exploratory Analytics\")\n",
    "print(\"=\" * 50)\n",
    "print(\"CROSS-REFERENCES:\")\n",
    "print(\"• Prerequisites: None (Entry point to analytics suite)\")\n",
    "print(\"• Next Steps: Tier1_Pivot.ipynb (data aggregation)\")\n",
    "print(\"• Next Steps: Tier1_Scatter.ipynb (relationship analysis)\")\n",
    "print(\"• Feeds Into: ALL subsequent notebooks (foundational concepts)\")\n",
    "print(\"• Compare With: Advanced statistical analysis in Tier3_Statistics.ipynb\")\n",
    "print(\"• Full Guide: See CROSS_REFERENCE_GUIDE.md for complete learning paths\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Purpose: Understand data structure, distributions, and relationships\")\n",
    "print(\"Techniques: Summary stats, distributions, correlations, pivot analysis\")\n",
    "print(\"Output: Comprehensive data profiling with professional visualizations\")\n",
    "print()\n",
    "\n",
    "def generate_synthetic_dataset(n_samples=1000, seed=42):\n",
    " \"\"\"Generate realistic synthetic business dataset for analysis.\"\"\"\n",
    " np.random.seed(seed)\n",
    "\n",
    " # Generate correlated business metrics\n",
    " base_sales = np.random.normal(50000, 15000, n_samples)\n",
    " base_sales = np.maximum(base_sales, 10000) # Minimum sales\n",
    "\n",
    " # Create related variables with realistic relationships\n",
    " data = {\n",
    " 'sales_amount': base_sales,\n",
    " 'marketing_spend': base_sales * 0.05 + np.random.normal(0, 500, n_samples),\n",
    " 'customer_satisfaction': 7 + (base_sales - 50000) / 20000 + np.random.normal(0, 0.8, n_samples),\n",
    " 'product_price': 100 + (base_sales - 50000) / 1000 + np.random.normal(0, 10, n_samples),\n",
    " 'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples, p=[0.3, 0.25, 0.25, 0.2]),\n",
    " 'product_category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Books'], n_samples, p=[0.4, 0.3, 0.2, 0.1]),\n",
    " 'quarter': np.random.choice(['Q1', 'Q2', 'Q3', 'Q4'], n_samples),\n",
    " 'employee_count': np.random.poisson(15, n_samples) + 5,\n",
    " 'years_in_business': np.random.exponential(5, n_samples),\n",
    " }\n",
    "\n",
    " # Ensure realistic ranges\n",
    " data['customer_satisfaction'] = np.clip(data['customer_satisfaction'], 1, 10)\n",
    " data['marketing_spend'] = np.maximum(data['marketing_spend'], 500)\n",
    " data['product_price'] = np.maximum(data['product_price'], 20)\n",
    " data['years_in_business'] = np.clip(data['years_in_business'], 0.5, 20)\n",
    "\n",
    " # Add some categorical derived variables\n",
    " data['business_size'] = pd.cut(data['employee_count'],\n",
    " bins=[0, 10, 25, 50, np.inf],\n",
    " labels=['Small', 'Medium', 'Large', 'Enterprise'])\n",
    "\n",
    " data['performance_tier'] = pd.cut(data['sales_amount'],\n",
    " bins=3,\n",
    " labels=['Low', 'Medium', 'High'])\n",
    "\n",
    " return pd.DataFrame(data)\n",
    "\n",
    "# Generate and load data\n",
    "print(\"Generating synthetic business dataset...\")\n",
    "df = generate_synthetic_dataset(1000)\n",
    "print(f\"Generated dataset with {len(df)} records and {len(df.columns)} variables\")\n",
    "print()\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Overview:\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1024:.1f} KB\")\n",
    "print()\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "print()\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c18518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. COMPREHENSIVE SUMMARY STATISTICS\n",
    "# ====================================\n",
    "\n",
    "print(\"COMPREHENSIVE SUMMARY STATISTICS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Basic descriptive statistics\n",
    "print(\"\\n1.1 Numerical Variables Summary:\")\n",
    "print(\"-\" * 35)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "summary_stats = df[numeric_cols].describe()\n",
    "print(summary_stats.round(2))\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\n1.2 Additional Statistical Measures:\")\n",
    "print(\"-\" * 38)\n",
    "additional_stats = pd.DataFrame({\n",
    " 'Skewness': df[numeric_cols].skew(),\n",
    " 'Kurtosis': df[numeric_cols].kurtosis(),\n",
    " 'Variance': df[numeric_cols].var(),\n",
    " 'Std_Dev': df[numeric_cols].std(),\n",
    " 'Range': df[numeric_cols].max() - df[numeric_cols].min(),\n",
    " 'IQR': df[numeric_cols].quantile(0.75) - df[numeric_cols].quantile(0.25),\n",
    " 'Missing_Count': df[numeric_cols].isnull().sum(),\n",
    " 'Missing_Pct': (df[numeric_cols].isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "print(additional_stats.round(3))\n",
    "\n",
    "# Categorical variables summary\n",
    "print(\"\\n1.3 Categorical Variables Summary:\")\n",
    "print(\"-\" * 37)\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    " print(f\"\\n{col.upper()}:\")\n",
    " value_counts = df[col].value_counts()\n",
    " percentages = (df[col].value_counts(normalize=True) * 100).round(2)\n",
    " summary_cat = pd.DataFrame({\n",
    " 'Count': value_counts,\n",
    " 'Percentage': percentages\n",
    " })\n",
    " print(summary_cat)\n",
    "\n",
    "print(\"\\n Summary statistics complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DISTRIBUTION ANALYSIS WITH PROFESSIONAL VISUALIZATIONS\n",
    "# ==========================================================\n",
    "\n",
    "print(\"DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Create subplots for multiple distribution visualizations\n",
    "fig = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=[\n",
    " 'Sales Amount Distribution', 'Customer Satisfaction Distribution',\n",
    " 'Marketing Spend Distribution', 'Product Price Distribution',\n",
    " 'Employee Count Distribution', 'Years in Business Distribution'\n",
    " ],\n",
    " specs=[[{\"secondary_y\": True}, {\"secondary_y\": True}],\n",
    " [{\"secondary_y\": True}, {\"secondary_y\": True}],\n",
    " [{\"secondary_y\": True}, {\"secondary_y\": True}]]\n",
    ")\n",
    "\n",
    "# Variables to analyze\n",
    "dist_vars = ['sales_amount', 'customer_satisfaction', 'marketing_spend',\n",
    " 'product_price', 'employee_count', 'years_in_business']\n",
    "\n",
    "positions = [(1,1), (1,2), (2,1), (2,2), (3,1), (3,2)]\n",
    "\n",
    "for var, (row, col) in zip(dist_vars, positions):\n",
    " # Histogram\n",
    " fig.add_trace(\n",
    " go.Histogram(x=df[var], name=f'{var} Histogram',\n",
    " opacity=0.7, nbinsx=30,\n",
    " marker_color='lightblue'),\n",
    " row=row, col=col\n",
    " )\n",
    "\n",
    " # Add mean line\n",
    " mean_val = df[var].mean()\n",
    " fig.add_vline(x=mean_val, line_dash=\"dash\", line_color=\"red\",\n",
    " annotation_text=f\"Mean: {mean_val:.1f}\",\n",
    " row=row, col=col)\n",
    "\n",
    "fig.update_layout(height=900, title_text=\"Distribution Analysis Dashboard\",\n",
    " showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# Statistical normality tests\n",
    "print(\"\\n2.1 Distribution Shape Analysis:\")\n",
    "print(\"-\" * 32)\n",
    "from scipy import stats\n",
    "\n",
    "normality_results = []\n",
    "for var in dist_vars:\n",
    " # Shapiro-Wilk test (small samples)\n",
    " if len(df[var]) < 5000:\n",
    " stat, p_value = stats.shapiro(df[var].dropna())\n",
    " test_name = \"Shapiro-Wilk\"\n",
    " else:\n",
    " # Kolmogorov-Smirnov test (large samples)\n",
    " stat, p_value = stats.kstest(df[var].dropna(), 'norm')\n",
    " test_name = \"Kolmogorov-Smirnov\"\n",
    "\n",
    " is_normal = \"Yes\" if p_value > 0.05 else \"No\"\n",
    " normality_results.append({\n",
    " 'Variable': var,\n",
    " 'Test': test_name,\n",
    " 'Statistic': round(stat, 4),\n",
    " 'P-Value': round(p_value, 4),\n",
    " 'Normal?': is_normal,\n",
    " 'Skewness': round(df[var].skew(), 3),\n",
    " 'Kurtosis': round(df[var].kurtosis(), 3)\n",
    " })\n",
    "\n",
    "normality_df = pd.DataFrame(normality_results)\n",
    "print(normality_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n Distribution analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3308aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CORRELATION ANALYSIS AND HEATMAPS\n",
    "# ====================================\n",
    "\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 22)\n",
    "\n",
    "# Calculate correlation matrices\n",
    "numeric_data = df[numeric_cols]\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "# Create interactive correlation heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    " z=correlation_matrix.values,\n",
    " x=correlation_matrix.columns,\n",
    " y=correlation_matrix.columns,\n",
    " colorscale='RdBu',\n",
    " zmid=0,\n",
    " text=correlation_matrix.round(3).values,\n",
    " texttemplate=\"%{text}\",\n",
    " textfont={\"size\": 10},\n",
    " hoverongaps=False\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    " title=\"Correlation Matrix Heatmap\",\n",
    " width=800, height=600,\n",
    " xaxis_title=\"Variables\",\n",
    " yaxis_title=\"Variables\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Correlation strength analysis\n",
    "print(\"\\n3.1 Strong Correlations (|r| > 0.5):\")\n",
    "print(\"-\" * 35)\n",
    "strong_corrs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    " for j in range(i+1, len(correlation_matrix.columns)):\n",
    " corr_val = correlation_matrix.iloc[i, j]\n",
    " if abs(corr_val) > 0.5:\n",
    " strong_corrs.append({\n",
    " 'Variable_1': correlation_matrix.columns[i],\n",
    " 'Variable_2': correlation_matrix.columns[j],\n",
    " 'Correlation': round(corr_val, 3),\n",
    " 'Strength': 'Strong' if abs(corr_val) > 0.7 else 'Moderate'\n",
    " })\n",
    "\n",
    "if strong_corrs:\n",
    " strong_corr_df = pd.DataFrame(strong_corrs)\n",
    " strong_corr_df = strong_corr_df.sort_values('Correlation', key=abs, ascending=False)\n",
    " print(strong_corr_df.to_string(index=False))\n",
    "else:\n",
    " print(\"No strong correlations found (|r| > 0.5)\")\n",
    "\n",
    "# Pairplot for key relationships\n",
    "print(\"\\n3.2 Creating pairplot for key variables...\")\n",
    "key_vars = ['sales_amount', 'marketing_spend', 'customer_satisfaction', 'product_price']\n",
    "pairplot_data = df[key_vars + ['region']].copy()\n",
    "\n",
    "# Create pairplot with Plotly\n",
    "from plotly.graph_objects import Scatter\n",
    "fig_pair = make_subplots(rows=len(key_vars), cols=len(key_vars),\n",
    " subplot_titles=[f\"{x} vs {y}\" for x in key_vars for y in key_vars])\n",
    "\n",
    "for i, var1 in enumerate(key_vars):\n",
    " for j, var2 in enumerate(key_vars):\n",
    " if i == j:\n",
    " # Diagonal: histogram\n",
    " fig_pair.add_trace(\n",
    " go.Histogram(x=df[var1], name=var1, showlegend=False),\n",
    " row=i+1, col=j+1\n",
    " )\n",
    " else:\n",
    " # Off-diagonal: scatter plot\n",
    " fig_pair.add_trace(\n",
    " go.Scatter(x=df[var2], y=df[var1], mode='markers',\n",
    " name=f'{var1} vs {var2}', showlegend=False,\n",
    " marker=dict(size=4, opacity=0.6)),\n",
    " row=i+1, col=j+1\n",
    " )\n",
    "\n",
    "fig_pair.update_layout(height=800, title_text=\"Pairplot of Key Variables\")\n",
    "fig_pair.show()\n",
    "\n",
    "print(\"\\n Correlation analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baab053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PIVOT TABLES AND CROSS-TABULATION ANALYSIS\n",
    "# ==============================================\n",
    "\n",
    "print(\"PIVOT TABLES & CROSS-TABULATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# 4.1 Sales analysis by region and category\n",
    "print(\"\\n4.1 Sales Analysis by Region and Product Category:\")\n",
    "print(\"-\" * 50)\n",
    "pivot_sales = df.pivot_table(\n",
    " values=['sales_amount', 'marketing_spend', 'customer_satisfaction'],\n",
    " index='region',\n",
    " columns='product_category',\n",
    " aggfunc={'sales_amount': 'mean', 'marketing_spend': 'sum', 'customer_satisfaction': 'mean'}\n",
    ")\n",
    "\n",
    "print(\"Average Sales by Region and Category:\")\n",
    "print(pivot_sales['sales_amount'].round(0))\n",
    "print(\"\\nTotal Marketing Spend by Region and Category:\")\n",
    "print(pivot_sales['marketing_spend'].round(0))\n",
    "\n",
    "# Interactive pivot visualization\n",
    "fig = px.sunburst(\n",
    " df,\n",
    " path=['region', 'product_category'],\n",
    " values='sales_amount',\n",
    " title=\"Sales Distribution: Region → Product Category\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 4.2 Cross-tabulation analysis\n",
    "print(\"\\n4.2 Cross-Tabulation Analysis:\")\n",
    "print(\"-\" * 31)\n",
    "\n",
    "# Business size vs Performance tier\n",
    "crosstab = pd.crosstab(df['business_size'], df['performance_tier'],\n",
    " margins=True, normalize='index')\n",
    "print(\"Business Size vs Performance Tier (Row Percentages):\")\n",
    "print(crosstab.round(3))\n",
    "\n",
    "# Visualization of cross-tab\n",
    "fig = px.density_heatmap(\n",
    " df, x='business_size', y='performance_tier',\n",
    " title=\"Business Size vs Performance Tier Distribution\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 4.3 Aggregated metrics by categories\n",
    "print(\"\\n4.3 Key Metrics by Categories:\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# Group by region\n",
    "region_summary = df.groupby('region').agg({\n",
    " 'sales_amount': ['count', 'mean', 'median', 'std'],\n",
    " 'customer_satisfaction': 'mean',\n",
    " 'marketing_spend': 'sum',\n",
    " 'employee_count': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "region_summary.columns = ['_'.join(col).strip() for col in region_summary.columns]\n",
    "print(\"Regional Summary:\")\n",
    "print(region_summary)\n",
    "\n",
    "# Group by product category\n",
    "category_summary = df.groupby('product_category').agg({\n",
    " 'sales_amount': ['count', 'mean', 'sum'],\n",
    " 'product_price': 'mean',\n",
    " 'customer_satisfaction': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "category_summary.columns = ['_'.join(col).strip() for col in category_summary.columns]\n",
    "print(\"\\nProduct Category Summary:\")\n",
    "print(category_summary)\n",
    "\n",
    "print(\"\\n Pivot analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}