{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 6: Statistical Anomaly Detection\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** 13497a73-018e-433b-8716-385139202e87\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 6: Statistical Anomaly Detection,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** 13497a73-018e-433b-8716-385139202e87\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f38f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.covariance import EllipticEnvelope, MinCovDet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Tier 6: Statistical Anomaly Detection - Libraries Loaded!\")\n",
    "print(\"=\" * 58)\n",
    "print(\"Statistical Anomaly Detection Methods:\")\n",
    "print(\"• Z-score and Modified Z-score outlier detection\")\n",
    "print(\"• Interquartile Range (IQR) method\")\n",
    "print(\"• Multivariate Mahalanobis distance\")\n",
    "print(\"• Robust covariance estimation (Minimum Covariance Determinant)\")\n",
    "print(\"• Time series seasonal decomposition anomalies\")\n",
    "print(\"• Statistical process control (SPC) methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive anomaly detection datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Financial transactions dataset with fraud\n",
    "n_transactions = 5000\n",
    "n_fraudulent = 200\n",
    "\n",
    "# Normal transactions\n",
    "normal_transactions = []\n",
    "for i in range(n_transactions - n_fraudulent):\n",
    " # Transaction patterns vary by time of day and day of week\n",
    " hour = np.random.randint(0, 24)\n",
    " day_of_week = np.random.randint(0, 7)\n",
    "\n",
    " # Business hours have higher amounts\n",
    " if 9 <= hour <= 17 and day_of_week < 5: # Business hours\n",
    " amount_base = np.random.lognormal(6, 1) # ~$400 average\n",
    " frequency_factor = 2.0\n",
    " elif 18 <= hour <= 22: # Evening shopping\n",
    " amount_base = np.random.lognormal(5, 0.8) # ~$150 average\n",
    " frequency_factor = 1.5\n",
    " else: # Late night/early morning\n",
    " amount_base = np.random.lognormal(4, 0.5) # ~$55 average\n",
    " frequency_factor = 0.3\n",
    "\n",
    " normal_transactions.append({\n",
    " 'transaction_id': f'TXN_{i:06d}',\n",
    " 'amount': amount_base,\n",
    " 'hour': hour,\n",
    " 'day_of_week': day_of_week,\n",
    " 'merchant_risk_score': np.random.beta(2, 8), # Most merchants low risk\n",
    " 'user_velocity': np.random.poisson(frequency_factor),\n",
    " 'geographic_risk': np.random.exponential(0.1),\n",
    " 'is_fraud': 0\n",
    " })\n",
    "\n",
    "# Fraudulent transactions (anomalies)\n",
    "fraudulent_transactions = []\n",
    "for i in range(n_fraudulent):\n",
    " # Fraud patterns: unusual amounts, times, and risk scores\n",
    " hour = np.random.choice([2, 3, 4, 23], p=[0.3, 0.3, 0.2, 0.2]) # Suspicious hours\n",
    "\n",
    " # Fraudulent amounts are either very small (testing) or very large (theft)\n",
    " if np.random.random() < 0.3: # Small test transactions\n",
    " amount = np.random.uniform(1, 10)\n",
    " else: # Large fraudulent transactions\n",
    " amount = np.random.lognormal(8, 1.5) # ~$3000 average\n",
    "\n",
    " fraudulent_transactions.append({\n",
    " 'transaction_id': f'FRAUD_{i:06d}',\n",
    " 'amount': amount,\n",
    " 'hour': hour,\n",
    " 'day_of_week': np.random.randint(0, 7),\n",
    " 'merchant_risk_score': np.random.beta(8, 2), # High risk merchants\n",
    " 'user_velocity': np.random.poisson(0.1), # Very low or very high velocity\n",
    " 'geographic_risk': np.random.exponential(0.05), # Higher geographic risk\n",
    " 'is_fraud': 1\n",
    " })\n",
    "\n",
    "# Combine datasets\n",
    "all_transactions = normal_transactions + fraudulent_transactions\n",
    "fraud_df = pd.DataFrame(all_transactions).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Financial Fraud Dataset Created:\")\n",
    "print(f\"Total transactions: {len(fraud_df)}\")\n",
    "print(f\"Fraudulent transactions: {len(fraudulent_transactions)} ({len(fraudulent_transactions)/len(fraud_df)*100:.1f}%)\")\n",
    "print(f\"Amount range: ${fraud_df['amount'].min():.2f} - ${fraud_df['amount'].max():.2f}\")\n",
    "\n",
    "# 2. Manufacturing quality control dataset\n",
    "n_products = 3000\n",
    "n_defective = 150\n",
    "\n",
    "# Normal product measurements\n",
    "quality_data = []\n",
    "\n",
    "for i in range(n_products - n_defective):\n",
    " # Normal manufacturing process with controlled variation\n",
    " dimension_1 = np.random.normal(100, 2) # Target: 100mm ±2mm\n",
    " dimension_2 = np.random.normal(50, 1) # Target: 50mm ±1mm\n",
    " weight = np.random.normal(500, 10) # Target: 500g ±10g\n",
    " surface_roughness = np.random.gamma(2, 0.5) # Low roughness\n",
    "\n",
    " quality_data.append({\n",
    " 'product_id': f'PROD_{i:06d}',\n",
    " 'dimension_1': dimension_1,\n",
    " 'dimension_2': dimension_2,\n",
    " 'weight': weight,\n",
    " 'surface_roughness': surface_roughness,\n",
    " 'temperature': np.random.normal(25, 2), # Room temperature\n",
    " 'is_defective': 0\n",
    " })\n",
    "\n",
    "# Defective products (anomalies)\n",
    "for i in range(n_defective):\n",
    " # Defective products have out-of-spec measurements\n",
    " if np.random.random() < 0.4: # Dimension defects\n",
    " dimension_1 = np.random.choice([np.random.normal(85, 3), np.random.normal(115, 3)])\n",
    " dimension_2 = np.random.normal(50, 1)\n",
    " elif np.random.random() < 0.3: # Weight defects\n",
    " dimension_1 = np.random.normal(100, 2)\n",
    " dimension_2 = np.random.normal(50, 1)\n",
    " else: # Multiple defects\n",
    " dimension_1 = np.random.normal(100, 5)\n",
    " dimension_2 = np.random.choice([np.random.normal(40, 2), np.random.normal(60, 2)])\n",
    "\n",
    " weight = np.random.choice([np.random.normal(450, 20), np.random.normal(550, 20)])\n",
    " surface_roughness = np.random.gamma(8, 1) # High roughness\n",
    "\n",
    " quality_data.append({\n",
    " 'product_id': f'DEFECT_{i:06d}',\n",
    " 'dimension_1': dimension_1,\n",
    " 'dimension_2': dimension_2,\n",
    " 'weight': weight,\n",
    " 'surface_roughness': surface_roughness,\n",
    " 'temperature': np.random.normal(25, 2),\n",
    " 'is_defective': 1\n",
    " })\n",
    "\n",
    "quality_df = pd.DataFrame(quality_data).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nManufacturing Quality Dataset Created:\")\n",
    "print(f\"Total products: {len(quality_df)}\")\n",
    "print(f\"Defective products: {n_defective} ({n_defective/len(quality_df)*100:.1f}%)\")\n",
    "\n",
    "# 3. Time series with seasonal anomalies\n",
    "dates = pd.date_range('2023-01-01', periods=365, freq='D')\n",
    "n_days = len(dates)\n",
    "\n",
    "# Base seasonal pattern\n",
    "day_of_year = np.arange(1, n_days + 1)\n",
    "seasonal_component = 50 + 30 * np.sin(2 * np.pi * day_of_year / 365.25) # Annual cycle\n",
    "weekly_component = 10 * np.sin(2 * np.pi * day_of_year / 7) # Weekly cycle\n",
    "trend_component = 0.1 * day_of_year # Slight upward trend\n",
    "noise = np.random.normal(0, 5, n_days)\n",
    "\n",
    "base_values = seasonal_component + weekly_component + trend_component + noise\n",
    "\n",
    "# Add anomalies\n",
    "anomaly_indices = np.random.choice(n_days, size=20, replace=False)\n",
    "anomaly_values = base_values.copy()\n",
    "for idx in anomaly_indices:\n",
    " if np.random.random() < 0.5:\n",
    " anomaly_values[idx] += np.random.normal(50, 15) # Positive anomaly\n",
    " else:\n",
    " anomaly_values[idx] -= np.random.normal(30, 10) # Negative anomaly\n",
    "\n",
    "timeseries_df = pd.DataFrame({\n",
    " 'date': dates,\n",
    " 'value': anomaly_values,\n",
    " 'is_anomaly': [1 if i in anomaly_indices else 0 for i in range(n_days)]\n",
    "})\n",
    "\n",
    "print(f\"\\nTime Series Anomaly Dataset Created:\")\n",
    "print(f\"Total days: {len(timeseries_df)}\")\n",
    "print(f\"Anomalous days: {len(anomaly_indices)} ({len(anomaly_indices)/len(timeseries_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8adcd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. STATISTICAL ANOMALY DETECTION METHODS\n",
    "print(\"1. STATISTICAL ANOMALY DETECTION METHODS\")\n",
    "print(\"=\" * 44)\n",
    "\n",
    "# Z-Score Method\n",
    "def detect_zscore_anomalies(data, threshold=3):\n",
    " \"\"\"Detect anomalies using Z-score method\"\"\"\n",
    " z_scores = np.abs(stats.zscore(data))\n",
    " return z_scores > threshold, z_scores\n",
    "\n",
    "# Modified Z-Score Method (more robust)\n",
    "def detect_modified_zscore_anomalies(data, threshold=3.5):\n",
    " \"\"\"Detect anomalies using Modified Z-score method\"\"\"\n",
    " median = np.median(data)\n",
    " mad = np.median(np.abs(data - median)) # Median Absolute Deviation\n",
    " modified_z_scores = 0.6745 * (data - median) / mad\n",
    " return np.abs(modified_z_scores) > threshold, modified_z_scores\n",
    "\n",
    "# IQR Method\n",
    "def detect_iqr_anomalies(data, k=1.5):\n",
    " \"\"\"Detect anomalies using Interquartile Range method\"\"\"\n",
    " Q1 = np.percentile(data, 25)\n",
    " Q3 = np.percentile(data, 75)\n",
    " IQR = Q3 - Q1\n",
    " lower_bound = Q1 - k * IQR\n",
    " upper_bound = Q3 + k * IQR\n",
    " return (data < lower_bound) | (data > upper_bound), (lower_bound, upper_bound)\n",
    "\n",
    "# Apply methods to fraud detection (transaction amounts)\n",
    "amounts = fraud_df['amount'].values\n",
    "\n",
    "# Z-score anomalies\n",
    "zscore_anomalies, z_scores = detect_zscore_anomalies(amounts)\n",
    "fraud_df['zscore_anomaly'] = zscore_anomalies\n",
    "fraud_df['z_score'] = z_scores\n",
    "\n",
    "# Modified Z-score anomalies\n",
    "mod_zscore_anomalies, mod_z_scores = detect_modified_zscore_anomalies(amounts)\n",
    "fraud_df['mod_zscore_anomaly'] = mod_zscore_anomalies\n",
    "fraud_df['mod_z_score'] = mod_z_scores\n",
    "\n",
    "# IQR anomalies\n",
    "iqr_anomalies, (iqr_lower, iqr_upper) = detect_iqr_anomalies(amounts)\n",
    "fraud_df['iqr_anomaly'] = iqr_anomalies\n",
    "\n",
    "print(\"Fraud Detection Results (Transaction Amounts):\")\n",
    "print(f\"Z-score anomalies: {np.sum(zscore_anomalies)} ({np.sum(zscore_anomalies)/len(amounts)*100:.1f}%)\")\n",
    "print(f\"Modified Z-score anomalies: {np.sum(mod_zscore_anomalies)} ({np.sum(mod_zscore_anomalies)/len(amounts)*100:.1f}%)\")\n",
    "print(f\"IQR anomalies: {np.sum(iqr_anomalies)} ({np.sum(iqr_anomalies)/len(amounts)*100:.1f}%)\")\n",
    "print(f\"IQR bounds: ${iqr_lower:.2f} - ${iqr_upper:.2f}\")\n",
    "\n",
    "# Multivariate anomaly detection using Mahalanobis distance\n",
    "print(f\"\\nMultivariate Anomaly Detection:\")\n",
    "\n",
    "# Select features for multivariate analysis\n",
    "fraud_features = ['amount', 'merchant_risk_score', 'user_velocity', 'geographic_risk']\n",
    "X_fraud = fraud_df[fraud_features].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_fraud_scaled = scaler.fit_transform(X_fraud)\n",
    "\n",
    "# Mahalanobis distance with robust covariance estimation\n",
    "robust_cov = MinCovDet().fit(X_fraud_scaled)\n",
    "mahal_distances = robust_cov.mahalanobis(X_fraud_scaled)\n",
    "\n",
    "# Determine threshold (typically 97.5th percentile)\n",
    "mahal_threshold = np.percentile(mahal_distances, 97.5)\n",
    "mahal_anomalies = mahal_distances > mahal_threshold\n",
    "\n",
    "fraud_df['mahalanobis_distance'] = mahal_distances\n",
    "fraud_df['mahal_anomaly'] = mahal_anomalies\n",
    "\n",
    "print(f\"Mahalanobis distance anomalies: {np.sum(mahal_anomalies)} ({np.sum(mahal_anomalies)/len(X_fraud)*100:.1f}%)\")\n",
    "print(f\"Mahalanobis threshold: {mahal_threshold:.2f}\")\n",
    "\n",
    "# Elliptic Envelope method\n",
    "envelope = EllipticEnvelope(contamination=0.1, random_state=42)\n",
    "envelope_predictions = envelope.fit_predict(X_fraud_scaled)\n",
    "envelope_anomalies = envelope_predictions == -1\n",
    "\n",
    "fraud_df['envelope_anomaly'] = envelope_anomalies\n",
    "\n",
    "print(f\"Elliptic Envelope anomalies: {np.sum(envelope_anomalies)} ({np.sum(envelope_anomalies)/len(X_fraud)*100:.1f}%)\")\n",
    "\n",
    "# Evaluate performance against ground truth\n",
    "methods = {\n",
    " 'Z-score': fraud_df['zscore_anomaly'],\n",
    " 'Modified Z-score': fraud_df['mod_zscore_anomaly'],\n",
    " 'IQR': fraud_df['iqr_anomaly'],\n",
    " 'Mahalanobis': fraud_df['mahal_anomaly'],\n",
    " 'Elliptic Envelope': fraud_df['envelope_anomaly']\n",
    "}\n",
    "\n",
    "print(f\"\\nPerformance Evaluation:\")\n",
    "print(\"Method Precision Recall F1-Score\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "performance_results = {}\n",
    "for method_name, predictions in methods.items():\n",
    " precision = precision_score(fraud_df['is_fraud'], predictions, zero_division=0)\n",
    " recall = recall_score(fraud_df['is_fraud'], predictions, zero_division=0)\n",
    " f1 = f1_score(fraud_df['is_fraud'], predictions, zero_division=0)\n",
    "\n",
    " performance_results[method_name] = {\n",
    " 'precision': precision,\n",
    " 'recall': recall,\n",
    " 'f1': f1\n",
    " }\n",
    "\n",
    " print(f\"{method_name:<20} {precision:.3f} {recall:.3f} {f1:.3f}\")\n",
    "\n",
    "# Find best performing method\n",
    "best_method = max(performance_results.keys(),\n",
    " key=lambda x: performance_results[x]['f1'])\n",
    "print(f\"\\nBest performing method: {best_method} (F1: {performance_results[best_method]['f1']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce84128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TIME SERIES ANOMALY DETECTION\n",
    "print(\"2. TIME SERIES ANOMALY DETECTION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Seasonal decomposition for time series anomalies\n",
    "def seasonal_decompose_anomalies(data, window=30, threshold=2.5):\n",
    " \"\"\"Detect anomalies using seasonal decomposition\"\"\"\n",
    " # Simple seasonal decomposition\n",
    " # Moving average for trend\n",
    " trend = data.rolling(window=window, center=True).mean()\n",
    "\n",
    " # Detrended data\n",
    " detrended = data - trend\n",
    "\n",
    " # Seasonal component (weekly pattern)\n",
    " seasonal_period = 7\n",
    " seasonal = detrended.groupby(detrended.index % seasonal_period).transform('mean')\n",
    "\n",
    " # Residual\n",
    " residual = detrended - seasonal\n",
    "\n",
    " # Anomalies based on residual\n",
    " residual_std = residual.std()\n",
    " anomalies = np.abs(residual) > threshold * residual_std\n",
    "\n",
    " return anomalies, trend, seasonal, residual\n",
    "\n",
    "# Apply to time series data\n",
    "ts_anomalies, ts_trend, ts_seasonal, ts_residual = seasonal_decompose_anomalies(\n",
    " timeseries_df['value'], window=30, threshold=2.5\n",
    ")\n",
    "\n",
    "timeseries_df['detected_anomaly'] = ts_anomalies\n",
    "timeseries_df['trend'] = ts_trend\n",
    "timeseries_df['seasonal'] = ts_seasonal\n",
    "timeseries_df['residual'] = ts_residual\n",
    "\n",
    "# Statistical process control (SPC) method\n",
    "def spc_control_limits(data, window=30):\n",
    " \"\"\"Calculate SPC control limits\"\"\"\n",
    " rolling_mean = data.rolling(window=window).mean()\n",
    " rolling_std = data.rolling(window=window).std()\n",
    "\n",
    " upper_control_limit = rolling_mean + 3 * rolling_std\n",
    " lower_control_limit = rolling_mean - 3 * rolling_std\n",
    "\n",
    " spc_anomalies = (data > upper_control_limit) | (data < lower_control_limit)\n",
    "\n",
    " return spc_anomalies, upper_control_limit, lower_control_limit\n",
    "\n",
    "spc_anomalies, ucl, lcl = spc_control_limits(timeseries_df['value'])\n",
    "timeseries_df['spc_anomaly'] = spc_anomalies\n",
    "timeseries_df['ucl'] = ucl\n",
    "timeseries_df['lcl'] = lcl\n",
    "\n",
    "print(\"Time Series Anomaly Detection Results:\")\n",
    "print(f\"Seasonal decomposition anomalies: {np.sum(ts_anomalies)} ({np.sum(ts_anomalies)/len(timeseries_df)*100:.1f}%)\")\n",
    "print(f\"SPC anomalies: {np.sum(spc_anomalies)} ({np.sum(spc_anomalies)/len(timeseries_df)*100:.1f}%)\")\n",
    "\n",
    "# Evaluate time series methods\n",
    "ts_precision_seasonal = precision_score(timeseries_df['is_anomaly'], timeseries_df['detected_anomaly'], zero_division=0)\n",
    "ts_recall_seasonal = recall_score(timeseries_df['is_anomaly'], timeseries_df['detected_anomaly'], zero_division=0)\n",
    "ts_f1_seasonal = f1_score(timeseries_df['is_anomaly'], timeseries_df['detected_anomaly'], zero_division=0)\n",
    "\n",
    "ts_precision_spc = precision_score(timeseries_df['is_anomaly'], timeseries_df['spc_anomaly'], zero_division=0)\n",
    "ts_recall_spc = recall_score(timeseries_df['is_anomaly'], timeseries_df['spc_anomaly'], zero_division=0)\n",
    "ts_f1_spc = f1_score(timeseries_df['is_anomaly'], timeseries_df['spc_anomaly'], zero_division=0)\n",
    "\n",
    "print(f\"\\nTime Series Performance Evaluation:\")\n",
    "print(\"Method Precision Recall F1-Score\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"Seasonal Decomposition {ts_precision_seasonal:.3f} {ts_recall_seasonal:.3f} {ts_f1_seasonal:.3f}\")\n",
    "print(f\"SPC Control Limits {ts_precision_spc:.3f} {ts_recall_spc:.3f} {ts_f1_spc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. INTERACTIVE STATISTICAL ANOMALY VISUALIZATIONS\n",
    "print(\"3. INTERACTIVE STATISTICAL ANOMALY VISUALIZATIONS\")\n",
    "print(\"=\" * 51)\n",
    "\n",
    "# Create comprehensive statistical anomaly detection dashboard\n",
    "fig = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=[\n",
    " 'Fraud Detection: Transaction Amounts with Statistical Methods',\n",
    " 'Multivariate Anomaly Detection (Mahalanobis Distance)',\n",
    " 'Method Performance Comparison',\n",
    " 'Time Series with Seasonal Decomposition',\n",
    " 'Statistical Process Control (SPC) Chart',\n",
    " 'Manufacturing Quality Control (Multivariate)'\n",
    " ],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# 1. Transaction amounts with statistical thresholds\n",
    "# Normal transactions\n",
    "normal_transactions = fraud_df[fraud_df['is_fraud'] == 0]\n",
    "fraud_transactions = fraud_df[fraud_df['is_fraud'] == 1]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=normal_transactions.index,\n",
    " y=normal_transactions['amount'],\n",
    " mode='markers',\n",
    " name='Normal Transactions',\n",
    " marker=dict(color='blue', size=4, opacity=0.6),\n",
    " hovertemplate='Amount: $%{y:.2f}<br>Z-score: %{customdata:.2f}<extra></extra>',\n",
    " customdata=normal_transactions['z_score']\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=fraud_transactions.index,\n",
    " y=fraud_transactions['amount'],\n",
    " mode='markers',\n",
    " name='Fraudulent Transactions',\n",
    " marker=dict(color='red', size=6, opacity=0.8),\n",
    " hovertemplate='FRAUD: $%{y:.2f}<br>Z-score: %{customdata:.2f}<extra></extra>',\n",
    " customdata=fraud_transactions['z_score']\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Add IQR bounds\n",
    "fig.add_hline(y=iqr_upper, line=dict(color='orange', dash='dash'),\n",
    " annotation_text=f\"IQR Upper: ${iqr_upper:.0f}\", row=1, col=1)\n",
    "fig.add_hline(y=iqr_lower, line=dict(color='orange', dash='dash'),\n",
    " annotation_text=f\"IQR Lower: ${iqr_lower:.0f}\", row=1, col=1)\n",
    "\n",
    "# 2. Mahalanobis distance visualization\n",
    "colors_mahal = ['red' if anomaly else 'blue' for anomaly in fraud_df['mahal_anomaly']]\n",
    "sizes_mahal = [8 if fraud else 4 for fraud in fraud_df['is_fraud']]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=fraud_df['amount'],\n",
    " y=fraud_df['mahalanobis_distance'],\n",
    " mode='markers',\n",
    " name='Mahalanobis Distance',\n",
    " marker=dict(color=colors_mahal, size=sizes_mahal, opacity=0.7),\n",
    " text=[f\"{'FRAUD' if fraud else 'Normal'}\" for fraud in fraud_df['is_fraud']],\n",
    " hovertemplate='%{text}<br>Amount: $%{x:.2f}<br>Mahal Distance: %{y:.2f}<extra></extra>'\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_hline(y=mahal_threshold, line=dict(color='green', dash='dash'),\n",
    " annotation_text=f\"Threshold: {mahal_threshold:.2f}\", row=1, col=2)\n",
    "\n",
    "# 3. Performance comparison bar chart\n",
    "methods_list = list(performance_results.keys())\n",
    "f1_scores = [performance_results[method]['f1'] for method in methods_list]\n",
    "precisions = [performance_results[method]['precision'] for method in methods_list]\n",
    "recalls = [performance_results[method]['recall'] for method in methods_list]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(x=methods_list, y=f1_scores, name='F1-Score',\n",
    " marker_color='green', opacity=0.7),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(x=methods_list, y=precisions, name='Precision',\n",
    " marker_color='blue', opacity=0.7),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(x=methods_list, y=recalls, name='Recall',\n",
    " marker_color='orange', opacity=0.7),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Time series with seasonal decomposition\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=timeseries_df['date'],\n",
    " y=timeseries_df['value'],\n",
    " mode='lines+markers',\n",
    " name='Time Series',\n",
    " line=dict(color='blue', width=2),\n",
    " marker=dict(size=4)\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "# Highlight anomalies\n",
    "anomaly_dates = timeseries_df[timeseries_df['is_anomaly'] == 1]\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=anomaly_dates['date'],\n",
    " y=anomaly_dates['value'],\n",
    " mode='markers',\n",
    " name='True Anomalies',\n",
    " marker=dict(color='red', size=10, symbol='x'),\n",
    " hovertemplate='True Anomaly<br>Date: %{x}<br>Value: %{y:.1f}<extra></extra>'\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "detected_anomalies = timeseries_df[timeseries_df['detected_anomaly'] == True]\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=detected_anomalies['date'],\n",
    " y=detected_anomalies['value'],\n",
    " mode='markers',\n",
    " name='Detected Anomalies',\n",
    " marker=dict(color='orange', size=8, symbol='circle-open'),\n",
    " hovertemplate='Detected Anomaly<br>Date: %{x}<br>Value: %{y:.1f}<extra></extra>'\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "# 5. SPC Control Chart\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=timeseries_df['date'],\n",
    " y=timeseries_df['value'],\n",
    " mode='lines+markers',\n",
    " name='Process Values',\n",
    " line=dict(color='blue', width=2),\n",
    " marker=dict(size=3)\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=timeseries_df['date'],\n",
    " y=timeseries_df['ucl'],\n",
    " mode='lines',\n",
    " name='Upper Control Limit',\n",
    " line=dict(color='red', dash='dash', width=2)\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=timeseries_df['date'],\n",
    " y=timeseries_df['lcl'],\n",
    " mode='lines',\n",
    " name='Lower Control Limit',\n",
    " line=dict(color='red', dash='dash', width=2)\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "# SPC anomalies\n",
    "spc_anomaly_data = timeseries_df[timeseries_df['spc_anomaly'] == True]\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=spc_anomaly_data['date'],\n",
    " y=spc_anomaly_data['value'],\n",
    " mode='markers',\n",
    " name='SPC Anomalies',\n",
    " marker=dict(color='red', size=10, symbol='diamond'),\n",
    " hovertemplate='SPC Anomaly<br>Date: %{x}<br>Value: %{y:.1f}<extra></extra>'\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Manufacturing quality control (2D projection)\n",
    "normal_products = quality_df[quality_df['is_defective'] == 0]\n",
    "defective_products = quality_df[quality_df['is_defective'] == 1]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=normal_products['dimension_1'],\n",
    " y=normal_products['weight'],\n",
    " mode='markers',\n",
    " name='Normal Products',\n",
    " marker=dict(color='green', size=4, opacity=0.6),\n",
    " hovertemplate='Normal<br>Dimension 1: %{x:.1f}mm<br>Weight: %{y:.1f}g<extra></extra>'\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=defective_products['dimension_1'],\n",
    " y=defective_products['weight'],\n",
    " mode='markers',\n",
    " name='Defective Products',\n",
    " marker=dict(color='red', size=6, opacity=0.8),\n",
    " hovertemplate='DEFECTIVE<br>Dimension 1: %{x:.1f}mm<br>Weight: %{y:.1f}g<extra></extra>'\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    " height=1200,\n",
    " title=\"Statistical Anomaly Detection Methods Dashboard\",\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Transaction Index\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Transaction Amount ($)\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Detection Method\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Date\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"Date\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"Dimension 1 (mm)\", row=3, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Transaction Amount ($)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Mahalanobis Distance\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Score\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Value\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Process Value\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Weight (g)\", row=3, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Business insights and applications\n",
    "print(f\"\\nSTATISTICAL ANOMALY DETECTION INSIGHTS:\")\n",
    "\n",
    "# Financial fraud insights\n",
    "total_transaction_value = fraud_df['amount'].sum()\n",
    "fraud_transaction_value = fraud_df[fraud_df['is_fraud'] == 1]['amount'].sum()\n",
    "fraud_percentage = fraud_transaction_value / total_transaction_value\n",
    "\n",
    "print(f\"\\nFinancial Fraud Analysis:\")\n",
    "print(f\"• Total transaction value: ${total_transaction_value:,.0f}\")\n",
    "print(f\"• Fraudulent transaction value: ${fraud_transaction_value:,.0f}\")\n",
    "print(f\"• Fraud value percentage: {fraud_percentage*100:.2f}%\")\n",
    "print(f\"• Best detection method: {best_method} (F1: {performance_results[best_method]['f1']:.3f})\")\n",
    "\n",
    "# Manufacturing quality insights\n",
    "defect_rate = quality_df['is_defective'].mean()\n",
    "total_products_value = len(quality_df) * 100 # $100 per product\n",
    "defect_cost = quality_df['is_defective'].sum() * 500 # $500 cost per defect\n",
    "\n",
    "print(f\"\\nManufacturing Quality Analysis:\")\n",
    "print(f\"• Overall defect rate: {defect_rate*100:.2f}%\")\n",
    "print(f\"• Total production value: ${total_products_value:,.0f}\")\n",
    "print(f\"• Defect cost impact: ${defect_cost:,.0f}\")\n",
    "\n",
    "# ROI calculation\n",
    "fraud_prevention_value = fraud_transaction_value * 0.80 # Prevent 80% of fraud\n",
    "quality_improvement_value = defect_cost * 0.70 # Prevent 70% of defects\n",
    "monitoring_system_cost = 250_000 # Implementation cost\n",
    "annual_operational_cost = 50_000 # Annual monitoring cost\n",
    "\n",
    "total_benefits = fraud_prevention_value + quality_improvement_value\n",
    "net_annual_benefits = total_benefits - annual_operational_cost\n",
    "roi = (net_annual_benefits - monitoring_system_cost) / monitoring_system_cost\n",
    "\n",
    "print(f\"\\nSTATISTICAL ANOMALY DETECTION ROI:\")\n",
    "print(f\"• Fraud prevention value: ${fraud_prevention_value:,.0f}\")\n",
    "print(f\"• Quality improvement value: ${quality_improvement_value:,.0f}\")\n",
    "print(f\"• Total annual benefits: ${total_benefits:,.0f}\")\n",
    "print(f\"• Implementation cost: ${monitoring_system_cost:,.0f}\")\n",
    "print(f\"• Annual operational cost: ${annual_operational_cost:,.0f}\")\n",
    "print(f\"• Net annual benefits: ${net_annual_benefits:,.0f}\")\n",
    "print(f\"• ROI: {roi*100:.0f}%\")\n",
    "print(f\"• Payback period: {monitoring_system_cost/net_annual_benefits*12:.1f} months\")\n",
    "\n",
    "print(f\"\\nCross-Reference Learning Path:\")\n",
    "print(f\"• Foundation: Tier3_Statistics.ipynb (statistical concepts)\")\n",
    "print(f\"• Comparison: Tier6_IsolationForest.ipynb (tree-based anomaly detection)\")\n",
    "print(f\"• Advanced: Tier6_OneClassSVM.ipynb (boundary-based methods)\")\n",
    "print(f\"• Specialized: Advanced_TimeSeriesAnomaly.ipynb (time series focus)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}