{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 6: Isolation Forest\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** 2b64eced-5f5c-439f-bd23-22ac395445be\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 6: Isolation Forest,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** 2b64eced-5f5c-439f-bd23-22ac395445be\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18596eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 6: Isolation Forest - Libraries Loaded!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Isolation Forest Techniques:\")\n",
    "print(\"• Tree-based anomaly detection\")\n",
    "print(\"• Path length anomaly scoring\")\n",
    "print(\"• Contamination parameter optimization\")\n",
    "print(\"• Real-time anomaly monitoring\")\n",
    "print(\"• Multi-dimensional outlier detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Isolation Forest datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Financial transaction data with fraud\n",
    "n_transactions = 10000\n",
    "normal_transactions = pd.DataFrame({\n",
    " 'amount': np.random.lognormal(3, 1, int(n_transactions * 0.95)),\n",
    " 'frequency': np.random.poisson(5, int(n_transactions * 0.95)),\n",
    " 'time_of_day': np.random.normal(12, 4, int(n_transactions * 0.95)) % 24,\n",
    " 'merchant_risk': np.random.beta(2, 5, int(n_transactions * 0.95))\n",
    "})\n",
    "\n",
    "# Add fraudulent transactions (5% contamination)\n",
    "n_fraud = int(n_transactions * 0.05)\n",
    "fraud_transactions = pd.DataFrame({\n",
    " 'amount': np.random.lognormal(5, 1.5, n_fraud), # Larger amounts\n",
    " 'frequency': np.random.poisson(15, n_fraud), # Higher frequency\n",
    " 'time_of_day': np.random.choice([2, 3, 23], n_fraud), # Unusual times\n",
    " 'merchant_risk': np.random.beta(5, 2, n_fraud) # Higher risk\n",
    "})\n",
    "\n",
    "# Combine datasets\n",
    "financial_data = pd.concat([normal_transactions, fraud_transactions], ignore_index=True)\n",
    "financial_labels = np.concatenate([np.zeros(len(normal_transactions)),\n",
    " np.ones(len(fraud_transactions))])\n",
    "\n",
    "# 2. Network traffic data with intrusions\n",
    "n_connections = 5000\n",
    "network_data = pd.DataFrame({\n",
    " 'packet_size': np.random.exponential(1000, n_connections),\n",
    " 'duration': np.random.gamma(2, 30, n_connections),\n",
    " 'port_number': np.random.choice([80, 443, 22, 21], n_connections, p=[0.4, 0.3, 0.2, 0.1]),\n",
    " 'bytes_transferred': np.random.lognormal(8, 2, n_connections)\n",
    "})\n",
    "\n",
    "# Add intrusion patterns (3% contamination)\n",
    "n_intrusions = int(n_connections * 0.03)\n",
    "intrusion_indices = np.random.choice(n_connections, n_intrusions, replace=False)\n",
    "network_labels = np.zeros(n_connections)\n",
    "network_labels[intrusion_indices] = 1\n",
    "\n",
    "# Modify intrusion data\n",
    "network_data.loc[intrusion_indices, 'packet_size'] *= 10 # Larger packets\n",
    "network_data.loc[intrusion_indices, 'duration'] *= 5 # Longer duration\n",
    "network_data.loc[intrusion_indices, 'bytes_transferred'] *= 20 # More data\n",
    "\n",
    "print(\" Isolation Forest Datasets:\")\n",
    "print(f\"Financial transactions: {len(financial_data)} samples, {financial_labels.sum():.0f} fraudulent\")\n",
    "print(f\"Network connections: {len(network_data)} samples, {network_labels.sum():.0f} intrusions\")\n",
    "print(f\"Financial fraud rate: {financial_labels.mean()*100:.1f}%\")\n",
    "print(f\"Network intrusion rate: {network_labels.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ISOLATION FOREST ANOMALY DETECTION\n",
    "print(\" 1. ISOLATION FOREST ANALYSIS\")\n",
    "print(\"=\" * 31)\n",
    "\n",
    "# Apply Isolation Forest to financial data\n",
    "scaler_financial = StandardScaler()\n",
    "financial_scaled = scaler_financial.fit_transform(financial_data)\n",
    "\n",
    "# Test different contamination parameters\n",
    "contamination_rates = [0.01, 0.03, 0.05, 0.1, 0.15]\n",
    "isolation_results = {}\n",
    "\n",
    "for contamination in contamination_rates:\n",
    " iso_forest = IsolationForest(\n",
    " contamination=contamination,\n",
    " random_state=42,\n",
    " n_estimators=100\n",
    " )\n",
    "\n",
    " # Fit and predict\n",
    " anomaly_predictions = iso_forest.fit_predict(financial_scaled)\n",
    " anomaly_scores = iso_forest.decision_function(financial_scaled)\n",
    "\n",
    " # Convert predictions (-1 for anomaly, 1 for normal) to binary\n",
    " binary_predictions = (anomaly_predictions == -1).astype(int)\n",
    "\n",
    " # Calculate metrics\n",
    " precision = np.sum((binary_predictions == 1) & (financial_labels == 1)) / np.sum(binary_predictions == 1) if np.sum(binary_predictions == 1) > 0 else 0\n",
    " recall = np.sum((binary_predictions == 1) & (financial_labels == 1)) / np.sum(financial_labels == 1)\n",
    " f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    " isolation_results[contamination] = {\n",
    " 'predictions': binary_predictions,\n",
    " 'scores': anomaly_scores,\n",
    " 'precision': precision,\n",
    " 'recall': recall,\n",
    " 'f1_score': f1_score\n",
    " }\n",
    "\n",
    " print(f\"Contamination {contamination:.2f}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1_score:.3f}\")\n",
    "\n",
    "# Select best contamination rate\n",
    "best_contamination = max(isolation_results.keys(), key=lambda x: isolation_results[x]['f1_score'])\n",
    "best_model_results = isolation_results[best_contamination]\n",
    "\n",
    "print(f\"\\nBest contamination rate: {best_contamination:.2f}\")\n",
    "print(f\"Best F1-score: {best_model_results['f1_score']:.3f}\")\n",
    "\n",
    "# Analyze anomaly scores distribution\n",
    "normal_scores = best_model_results['scores'][financial_labels == 0]\n",
    "fraud_scores = best_model_results['scores'][financial_labels == 1]\n",
    "\n",
    "print(f\"Normal transaction scores: mean={normal_scores.mean():.3f}, std={normal_scores.std():.3f}\")\n",
    "print(f\"Fraud transaction scores: mean={fraud_scores.mean():.3f}, std={fraud_scores.std():.3f}\")\n",
    "\n",
    "# Feature importance analysis (approximated by feature variance in anomalies)\n",
    "anomaly_indices = best_model_results['predictions'] == 1\n",
    "if np.sum(anomaly_indices) > 0:\n",
    " anomaly_data = financial_data[anomaly_indices]\n",
    " normal_data = financial_data[~anomaly_indices]\n",
    "\n",
    " feature_importance = {}\n",
    " for col in financial_data.columns:\n",
    " anomaly_var = anomaly_data[col].var()\n",
    " normal_var = normal_data[col].var()\n",
    " importance = anomaly_var / (normal_var + 1e-10) # Avoid division by zero\n",
    " feature_importance[col] = importance\n",
    "\n",
    " print(f\"\\nFeature importance for anomaly detection:\")\n",
    " for feature, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):\n",
    " print(f\"• {feature}: {importance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. INTERACTIVE ISOLATION FOREST VISUALIZATIONS\n",
    "print(\" 2. INTERACTIVE ISOLATION FOREST VISUALIZATIONS\")\n",
    "print(\"=\" * 49)\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "fig = make_subplots(\n",
    " rows=2, cols=2,\n",
    " subplot_titles=[\n",
    " 'Contamination Rate Performance',\n",
    " 'Anomaly Score Distribution',\n",
    " 'Transaction Amount vs Frequency (Colored by Anomaly)',\n",
    " 'Feature Comparison: Normal vs Anomalous'\n",
    " ]\n",
    ")\n",
    "\n",
    "# Performance comparison across contamination rates\n",
    "contamination_list = list(isolation_results.keys())\n",
    "f1_scores = [isolation_results[c]['f1_score'] for c in contamination_list]\n",
    "precision_scores = [isolation_results[c]['precision'] for c in contamination_list]\n",
    "recall_scores = [isolation_results[c]['recall'] for c in contamination_list]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(x=contamination_list, y=f1_scores, mode='lines+markers',\n",
    " name='F1-Score', line=dict(color='blue', width=3)),\n",
    " row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    " go.Scatter(x=contamination_list, y=precision_scores, mode='lines+markers',\n",
    " name='Precision', line=dict(color='green', width=2)),\n",
    " row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    " go.Scatter(x=contamination_list, y=recall_scores, mode='lines+markers',\n",
    " name='Recall', line=dict(color='red', width=2)),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Anomaly score distributions\n",
    "fig.add_trace(\n",
    " go.Histogram(x=normal_scores, name='Normal Transactions',\n",
    " opacity=0.7, nbinsx=30, marker_color='blue'),\n",
    " row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    " go.Histogram(x=fraud_scores, name='Fraudulent Transactions',\n",
    " opacity=0.7, nbinsx=30, marker_color='red'),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Transaction scatter plot colored by anomaly detection\n",
    "financial_data['predicted_anomaly'] = best_model_results['predictions']\n",
    "financial_data['anomaly_score'] = best_model_results['scores']\n",
    "\n",
    "# Normal transactions\n",
    "normal_trans = financial_data[financial_data['predicted_anomaly'] == 0]\n",
    "fig.add_trace(\n",
    " go.Scatter(x=normal_trans['amount'], y=normal_trans['frequency'],\n",
    " mode='markers', name='Normal (Predicted)',\n",
    " marker=dict(color='lightblue', size=4, opacity=0.6)),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# Anomalous transactions\n",
    "anomaly_trans = financial_data[financial_data['predicted_anomaly'] == 1]\n",
    "fig.add_trace(\n",
    " go.Scatter(x=anomaly_trans['amount'], y=anomaly_trans['frequency'],\n",
    " mode='markers', name='Anomalous (Predicted)',\n",
    " marker=dict(color='red', size=6, opacity=0.8)),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# Feature comparison box plots\n",
    "features = list(financial_data.columns[:-2]) # Exclude prediction columns\n",
    "for i, feature in enumerate(features):\n",
    " normal_values = financial_data[financial_data['predicted_anomaly'] == 0][feature]\n",
    " anomaly_values = financial_data[financial_data['predicted_anomaly'] == 1][feature]\n",
    "\n",
    " fig.add_trace(\n",
    " go.Box(y=normal_values, name=f'Normal {feature}',\n",
    " marker_color='lightblue', showlegend=False),\n",
    " row=2, col=2\n",
    " )\n",
    " fig.add_trace(\n",
    " go.Box(y=anomaly_values, name=f'Anomaly {feature}',\n",
    " marker_color='red', showlegend=False),\n",
    " row=2, col=2\n",
    " )\n",
    "\n",
    "fig.update_layout(height=800, title=\"Isolation Forest Anomaly Detection Dashboard\")\n",
    "fig.update_xaxes(title_text=\"Contamination Rate\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Anomaly Score\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Transaction Amount ($)\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Features\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Score\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Transaction Frequency\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Feature Values\", row=2, col=2)\n",
    "fig.show()\n",
    "\n",
    "# Real-time monitoring simulation\n",
    "print(f\"\\n REAL-TIME ANOMALY MONITORING SIMULATION:\")\n",
    "\n",
    "# Simulate new incoming transactions\n",
    "new_transactions = pd.DataFrame({\n",
    " 'amount': [50, 10000, 75, 50000, 200], # Mix of normal and suspicious amounts\n",
    " 'frequency': [3, 25, 5, 40, 8], # Mix of frequencies\n",
    " 'time_of_day': [14, 2, 10, 23, 16], # Mix of times\n",
    " 'merchant_risk': [0.2, 0.8, 0.3, 0.9, 0.1] # Mix of risk levels\n",
    "})\n",
    "\n",
    "# Apply the trained model\n",
    "new_scaled = scaler_financial.transform(new_transactions)\n",
    "final_model = IsolationForest(contamination=best_contamination, random_state=42, n_estimators=100)\n",
    "final_model.fit(financial_scaled)\n",
    "\n",
    "new_predictions = final_model.predict(new_scaled)\n",
    "new_scores = final_model.decision_function(new_scaled)\n",
    "\n",
    "print(\"New transaction analysis:\")\n",
    "for i, (pred, score) in enumerate(zip(new_predictions, new_scores)):\n",
    " status = \" ANOMALY\" if pred == -1 else \" NORMAL\"\n",
    " amount = new_transactions.iloc[i]['amount']\n",
    " risk_level = \"HIGH\" if score < -0.1 else \"MEDIUM\" if score < 0 else \"LOW\"\n",
    " print(f\"Transaction {i+1}: ${amount:,.0f} -> {status} (Score: {score:.3f}, Risk: {risk_level})\")\n",
    "\n",
    "# Business impact calculation\n",
    "fraud_detection_rate = best_model_results['recall']\n",
    "false_positive_rate = 1 - best_model_results['precision']\n",
    "avg_fraud_amount = financial_data[financial_labels == 1]['amount'].mean()\n",
    "total_transactions_per_year = 1_000_000\n",
    "fraud_rate = 0.05\n",
    "\n",
    "prevented_fraud_amount = (total_transactions_per_year * fraud_rate *\n",
    " fraud_detection_rate * avg_fraud_amount)\n",
    "false_positive_cost = (total_transactions_per_year * (1 - fraud_rate) *\n",
    " false_positive_rate * 50) # $50 cost per false positive\n",
    "\n",
    "net_savings = prevented_fraud_amount - false_positive_cost\n",
    "implementation_cost = 200_000\n",
    "\n",
    "print(f\"\\n ISOLATION FOREST BUSINESS ROI:\")\n",
    "print(f\"• Annual transactions: {total_transactions_per_year:,}\")\n",
    "print(f\"• Fraud detection rate: {fraud_detection_rate:.1%}\")\n",
    "print(f\"• Average fraud amount: ${avg_fraud_amount:,.0f}\")\n",
    "print(f\"• Prevented fraud losses: ${prevented_fraud_amount:,.0f}\")\n",
    "print(f\"• False positive costs: ${false_positive_cost:,.0f}\")\n",
    "print(f\"• Net annual savings: ${net_savings:,.0f}\")\n",
    "print(f\"• Implementation cost: ${implementation_cost:,.0f}\")\n",
    "print(f\"• ROI: {(net_savings - implementation_cost)/implementation_cost*100:.0f}%\")\n",
    "print(f\"• Payback period: {implementation_cost/net_savings*12:.1f} months\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}