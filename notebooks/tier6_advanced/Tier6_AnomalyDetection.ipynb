{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 6: Advanced Anomaly Detection & Outlier Analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** f8bf2d16-b5d6-4a48-97f4-1d58238ec1c7\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 6: Advanced Anomaly Detection & Outlier Analysis,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** f8bf2d16-b5d6-4a48-97f4-1d58238ec1c7\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81967f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 6: Advanced Anomaly Detection & Outlier Analysis\")\n",
    "print(\"=\" * 55)\n",
    "print(\" CROSS-REFERENCES:\")\n",
    "print(\"• Prerequisites: Tier1_Descriptive.ipynb, Tier2_LogisticRegression.ipynb, Tier5_Classification.ipynb\")\n",
    "print(\"• Builds On: Tier4_SVM.ipynb (one-class SVM), Tier4_KMeans.ipynb (clustering-based)\")\n",
    "print(\"• Complements: Tier6_RealTimeAnalytics.ipynb, Tier3_TimeSeries.ipynb\")\n",
    "print(\"• Advanced: Tier6_DeepLearning.ipynb, Tier6_EnsembleMethods.ipynb\")\n",
    "print(\"=\" * 55)\n",
    "print(\"Anomaly Detection Techniques:\")\n",
    "print(\"• Statistical methods (Z-score, IQR, Grubbs' test)\")\n",
    "print(\"• Machine learning approaches (Isolation Forest, One-Class SVM)\")\n",
    "print(\"• Clustering-based detection (DBSCAN, LOF)\")\n",
    "print(\"• Ensemble methods and real-time detection\")\n",
    "print(\"• Performance evaluation and business impact analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb88f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive synthetic datasets with known anomalies\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_anomaly_datasets():\n",
    " \"\"\"Generate multiple datasets with different types of anomalies.\"\"\"\n",
    "\n",
    " # Dataset 1: Financial Transactions (fraud detection)\n",
    " n_normal = 8000\n",
    " n_fraud = 200\n",
    "\n",
    " # Normal transactions\n",
    " normal_amounts = np.random.lognormal(mean=3, sigma=1, size=n_normal)\n",
    " normal_amounts = np.clip(normal_amounts, 1, 1000) # Realistic transaction amounts\n",
    "\n",
    " normal_features = {\n",
    " 'transaction_amount': normal_amounts,\n",
    " 'account_age_days': np.random.normal(365, 200, n_normal),\n",
    " 'daily_transaction_count': np.random.poisson(3, n_normal),\n",
    " 'time_since_last_transaction': np.random.exponential(2, n_normal),\n",
    " 'merchant_risk_score': np.random.beta(2, 5, n_normal),\n",
    " 'is_weekend': np.random.choice([0, 1], n_normal, p=[0.7, 0.3]),\n",
    " 'customer_id': np.random.randint(1, 2000, n_normal)\n",
    " }\n",
    "\n",
    " # Fraudulent transactions (anomalies)\n",
    " fraud_amounts = np.random.choice([\n",
    " np.random.uniform(2000, 10000, n_fraud//3), # High-value fraud\n",
    " np.random.uniform(0.1, 5, n_fraud//3), # Micro-fraud\n",
    " np.random.lognormal(5, 0.5, n_fraud//3) # Random large amounts\n",
    " ]).flatten()[:n_fraud]\n",
    "\n",
    " fraud_features = {\n",
    " 'transaction_amount': fraud_amounts,\n",
    " 'account_age_days': np.random.choice([\n",
    " np.random.uniform(0, 30, n_fraud//2), # New accounts\n",
    " np.random.uniform(1000, 2000, n_fraud//2) # Old dormant accounts\n",
    " ]).flatten()[:n_fraud],\n",
    " 'daily_transaction_count': np.random.choice([\n",
    " np.random.poisson(15, n_fraud//2), # High frequency\n",
    " np.random.poisson(0.5, n_fraud//2) # Very low frequency\n",
    " ]).flatten()[:n_fraud],\n",
    " 'time_since_last_transaction': np.random.exponential(0.1, n_fraud), # Rapid succession\n",
    " 'merchant_risk_score': np.random.beta(5, 2, n_fraud), # Higher risk merchants\n",
    " 'is_weekend': np.random.choice([0, 1], n_fraud, p=[0.4, 0.6]), # More weekend activity\n",
    " 'customer_id': np.random.randint(1, 2000, n_fraud)\n",
    " }\n",
    "\n",
    " # Combine normal and fraudulent transactions\n",
    " financial_df = pd.DataFrame({\n",
    " key: np.concatenate([normal_features[key], fraud_features[key]])\n",
    " for key in normal_features.keys()\n",
    " })\n",
    " financial_df['is_fraud'] = np.concatenate([np.zeros(n_normal), np.ones(n_fraud)])\n",
    " financial_df = financial_df.sample(frac=1).reset_index(drop=True) # Shuffle\n",
    "\n",
    " # Dataset 2: Manufacturing Quality Control\n",
    " n_normal_parts = 5000\n",
    " n_defective = 150\n",
    "\n",
    " # Normal parts (multivariate normal distribution)\n",
    " normal_temp = np.random.normal(250, 10, n_normal_parts)\n",
    " normal_pressure = np.random.normal(15, 2, n_normal_parts)\n",
    " normal_vibration = np.random.normal(0.5, 0.1, n_normal_parts)\n",
    " normal_thickness = np.random.normal(5.0, 0.1, n_normal_parts)\n",
    "\n",
    " # Defective parts (various anomaly patterns)\n",
    " defective_temp = np.concatenate([\n",
    " np.random.normal(300, 15, n_defective//3), # Overheating\n",
    " np.random.normal(200, 10, n_defective//3), # Underheating\n",
    " np.random.normal(250, 50, n_defective//3) # Unstable temperature\n",
    " ])[:n_defective]\n",
    "\n",
    " defective_pressure = np.concatenate([\n",
    " np.random.normal(25, 5, n_defective//2), # High pressure\n",
    " np.random.normal(5, 2, n_defective//2) # Low pressure\n",
    " ])[:n_defective]\n",
    "\n",
    " defective_vibration = np.random.uniform(1.0, 3.0, n_defective) # High vibration\n",
    " defective_thickness = np.concatenate([\n",
    " np.random.normal(5.5, 0.2, n_defective//2), # Too thick\n",
    " np.random.normal(4.5, 0.2, n_defective//2) # Too thin\n",
    " ])[:n_defective]\n",
    "\n",
    " manufacturing_df = pd.DataFrame({\n",
    " 'temperature': np.concatenate([normal_temp, defective_temp]),\n",
    " 'pressure': np.concatenate([normal_pressure, defective_pressure]),\n",
    " 'vibration': np.concatenate([normal_vibration, defective_vibration]),\n",
    " 'thickness': np.concatenate([normal_thickness, defective_thickness]),\n",
    " 'is_defective': np.concatenate([np.zeros(n_normal_parts), np.ones(n_defective)])\n",
    " })\n",
    " manufacturing_df = manufacturing_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    " return financial_df, manufacturing_df\n",
    "\n",
    "# Generate datasets\n",
    "financial_df, manufacturing_df = generate_anomaly_datasets()\n",
    "\n",
    "print(\" Anomaly Detection Datasets Created:\")\n",
    "print(f\"\\n1. Financial Transactions Dataset:\")\n",
    "print(f\" • Total transactions: {len(financial_df):,}\")\n",
    "print(f\" • Fraudulent transactions: {financial_df['is_fraud'].sum():.0f} ({financial_df['is_fraud'].mean()*100:.1f}%)\")\n",
    "print(f\" • Average transaction: ${financial_df['transaction_amount'].mean():.2f}\")\n",
    "print(f\" • Fraud rate: 1 in {len(financial_df) / financial_df['is_fraud'].sum():.0f} transactions\")\n",
    "\n",
    "print(f\"\\n2. Manufacturing Quality Control:\")\n",
    "print(f\" • Total parts inspected: {len(manufacturing_df):,}\")\n",
    "print(f\" • Defective parts: {manufacturing_df['is_defective'].sum():.0f} ({manufacturing_df['is_defective'].mean()*100:.1f}%)\")\n",
    "print(f\" • Average temperature: {manufacturing_df['temperature'].mean():.1f}°C\")\n",
    "print(f\" • Quality rate: {(1-manufacturing_df['is_defective'].mean())*100:.1f}%\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample Financial Data:\")\n",
    "print(financial_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb14f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. STATISTICAL ANOMALY DETECTION METHODS\n",
    "print(\" 1. STATISTICAL ANOMALY DETECTION METHODS\")\n",
    "print(\"=\" * 43)\n",
    "\n",
    "def statistical_anomaly_detection(data, column, method='zscore', threshold=3):\n",
    " \"\"\"Apply various statistical methods for anomaly detection.\"\"\"\n",
    "\n",
    " if method == 'zscore':\n",
    " z_scores = np.abs(stats.zscore(data[column]))\n",
    " anomalies = z_scores > threshold\n",
    " scores = z_scores\n",
    "\n",
    " elif method == 'iqr':\n",
    " Q1 = data[column].quantile(0.25)\n",
    " Q3 = data[column].quantile(0.75)\n",
    " IQR = Q3 - Q1\n",
    " lower_bound = Q1 - 1.5 * IQR\n",
    " upper_bound = Q3 + 1.5 * IQR\n",
    " anomalies = (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    " scores = np.abs(data[column] - data[column].median()) / IQR\n",
    "\n",
    " elif method == 'grubbs':\n",
    " # Simplified Grubbs test (single outlier)\n",
    " mean_val = data[column].mean()\n",
    " std_val = data[column].std()\n",
    " z_scores = np.abs((data[column] - mean_val) / std_val)\n",
    "\n",
    " n = len(data)\n",
    " t_critical = stats.t.ppf(1 - 0.05/(2*n), n-2) # Two-tailed test\n",
    " grubbs_critical = ((n-1)/np.sqrt(n)) * np.sqrt(t_critical**2/(n-2+t_critical**2))\n",
    "\n",
    " anomalies = z_scores > grubbs_critical\n",
    " scores = z_scores\n",
    "\n",
    " elif method == 'modified_zscore':\n",
    " # Using median absolute deviation (more robust)\n",
    " median = data[column].median()\n",
    " mad = np.median(np.abs(data[column] - median))\n",
    " modified_z_scores = 0.6745 * (data[column] - median) / mad\n",
    " anomalies = np.abs(modified_z_scores) > threshold\n",
    " scores = np.abs(modified_z_scores)\n",
    "\n",
    " return anomalies, scores\n",
    "\n",
    "# Apply statistical methods to financial transaction amounts\n",
    "print(\"Financial Transaction Amount Analysis:\")\n",
    "\n",
    "stat_methods = ['zscore', 'iqr', 'grubbs', 'modified_zscore']\n",
    "stat_results = {}\n",
    "\n",
    "for method in stat_methods:\n",
    " threshold = 3 if method in ['zscore', 'modified_zscore'] else None\n",
    " anomalies, scores = statistical_anomaly_detection(\n",
    " financial_df, 'transaction_amount', method, threshold\n",
    " )\n",
    "\n",
    " stat_results[method] = {\n",
    " 'anomalies': anomalies,\n",
    " 'scores': scores,\n",
    " 'count': anomalies.sum(),\n",
    " 'percentage': anomalies.mean() * 100\n",
    " }\n",
    "\n",
    " # Calculate precision, recall, F1 for fraud detection\n",
    " if 'is_fraud' in financial_df.columns:\n",
    " true_positives = ((anomalies) & (financial_df['is_fraud'] == 1)).sum()\n",
    " false_positives = ((anomalies) & (financial_df['is_fraud'] == 0)).sum()\n",
    " false_negatives = ((~anomalies) & (financial_df['is_fraud'] == 1)).sum()\n",
    "\n",
    " precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    " recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    " f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    " stat_results[method].update({\n",
    " 'precision': precision,\n",
    " 'recall': recall,\n",
    " 'f1': f1\n",
    " })\n",
    "\n",
    " print(f\"\\n{method.upper()} Method:\")\n",
    " print(f\" • Anomalies detected: {anomalies.sum()} ({anomalies.mean()*100:.1f}%)\")\n",
    " if 'precision' in stat_results[method]:\n",
    " print(f\" • Precision: {precision:.3f}\")\n",
    " print(f\" • Recall: {recall:.3f}\")\n",
    " print(f\" • F1-Score: {f1:.3f}\")\n",
    "\n",
    "# Find best statistical method\n",
    "best_stat_method = max(stat_methods, key=lambda x: stat_results[x].get('f1', 0))\n",
    "print(f\"\\n Best Statistical Method: {best_stat_method.upper()} (F1: {stat_results[best_stat_method]['f1']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd93367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MACHINE LEARNING ANOMALY DETECTION\n",
    "print(\" 2. MACHINE LEARNING ANOMALY DETECTION\")\n",
    "print(\"=\" * 41)\n",
    "\n",
    "# Prepare data for ML models\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Financial data preparation\n",
    "financial_features = ['transaction_amount', 'account_age_days', 'daily_transaction_count',\n",
    " 'time_since_last_transaction', 'merchant_risk_score', 'is_weekend']\n",
    "X_financial = financial_df[financial_features].copy()\n",
    "X_financial_scaled = scaler.fit_transform(X_financial)\n",
    "\n",
    "# Manufacturing data preparation\n",
    "manufacturing_features = ['temperature', 'pressure', 'vibration', 'thickness']\n",
    "X_manufacturing = manufacturing_df[manufacturing_features].copy()\n",
    "X_manufacturing_scaled = StandardScaler().fit_transform(X_manufacturing)\n",
    "\n",
    "# Initialize ML anomaly detection models\n",
    "ml_models = {\n",
    " 'Isolation Forest': IsolationForest(contamination=0.1, random_state=42),\n",
    " 'One-Class SVM': OneClassSVM(gamma='scale', nu=0.1),\n",
    " 'Local Outlier Factor': LocalOutlierFactor(contamination=0.1),\n",
    " 'Elliptic Envelope': EllipticEnvelope(contamination=0.1),\n",
    " 'DBSCAN': DBSCAN(eps=0.5, min_samples=5)\n",
    "}\n",
    "\n",
    "def evaluate_ml_anomaly_detection(X, y_true, models, dataset_name):\n",
    " \"\"\"Evaluate multiple ML anomaly detection models.\"\"\"\n",
    "\n",
    " print(f\"\\n{dataset_name} Dataset Evaluation:\")\n",
    " print(\"=\" * (len(dataset_name) + 20))\n",
    "\n",
    " results = {}\n",
    "\n",
    " for model_name, model in models.items():\n",
    " try:\n",
    " if model_name == 'Local Outlier Factor':\n",
    " # LOF returns 1 for inliers, -1 for outliers\n",
    " predictions = model.fit_predict(X)\n",
    " anomaly_mask = predictions == -1\n",
    "\n",
    " elif model_name == 'DBSCAN':\n",
    " # DBSCAN labels: -1 for noise (anomalies), 0+ for clusters\n",
    " predictions = model.fit_predict(X)\n",
    " anomaly_mask = predictions == -1\n",
    "\n",
    " else:\n",
    " # Standard sklearn anomaly detection interface\n",
    " model.fit(X)\n",
    " predictions = model.predict(X)\n",
    " anomaly_mask = predictions == -1\n",
    "\n",
    " # Calculate performance metrics\n",
    " if y_true is not None:\n",
    " true_positives = ((anomaly_mask) & (y_true == 1)).sum()\n",
    " false_positives = ((anomaly_mask) & (y_true == 0)).sum()\n",
    " false_negatives = ((~anomaly_mask) & (y_true == 1)).sum()\n",
    " true_negatives = ((~anomaly_mask) & (y_true == 0)).sum()\n",
    "\n",
    " precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    " recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    " f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    " # Calculate AUC if possible\n",
    " try:\n",
    " if hasattr(model, 'decision_function'):\n",
    " scores = model.decision_function(X)\n",
    " auc = roc_auc_score(y_true, -scores) # Negative because anomalies have lower scores\n",
    " else:\n",
    " auc = None\n",
    " except:\n",
    " auc = None\n",
    " else:\n",
    " precision = recall = f1 = auc = None\n",
    "\n",
    " results[model_name] = {\n",
    " 'anomaly_mask': anomaly_mask,\n",
    " 'anomaly_count': anomaly_mask.sum(),\n",
    " 'anomaly_percentage': anomaly_mask.mean() * 100,\n",
    " 'precision': precision,\n",
    " 'recall': recall,\n",
    " 'f1': f1,\n",
    " 'auc': auc,\n",
    " 'model': model\n",
    " }\n",
    "\n",
    " print(f\"\\n{model_name}:\")\n",
    " print(f\" • Anomalies detected: {anomaly_mask.sum()} ({anomaly_mask.mean()*100:.1f}%)\")\n",
    " if precision is not None:\n",
    " print(f\" • Precision: {precision:.3f}\")\n",
    " print(f\" • Recall: {recall:.3f}\")\n",
    " print(f\" • F1-Score: {f1:.3f}\")\n",
    " if auc is not None:\n",
    " print(f\" • AUC: {auc:.3f}\")\n",
    "\n",
    " except Exception as e:\n",
    " print(f\"\\n{model_name}: Error - {str(e)}\")\n",
    " results[model_name] = None\n",
    "\n",
    " return results\n",
    "\n",
    "# Evaluate models on both datasets\n",
    "financial_results = evaluate_ml_anomaly_detection(\n",
    " X_financial_scaled, financial_df['is_fraud'], ml_models, \"Financial Fraud Detection\"\n",
    ")\n",
    "\n",
    "manufacturing_results = evaluate_ml_anomaly_detection(\n",
    " X_manufacturing_scaled, manufacturing_df['is_defective'], ml_models, \"Manufacturing Quality Control\"\n",
    ")\n",
    "\n",
    "# Find best models for each dataset\n",
    "def find_best_model(results):\n",
    " \"\"\"Find best performing model based on F1 score.\"\"\"\n",
    " valid_results = {k: v for k, v in results.items() if v is not None and v['f1'] is not None}\n",
    " if valid_results:\n",
    " return max(valid_results.keys(), key=lambda x: valid_results[x]['f1'])\n",
    " return None\n",
    "\n",
    "best_financial_model = find_best_model(financial_results)\n",
    "best_manufacturing_model = find_best_model(manufacturing_results)\n",
    "\n",
    "print(f\"\\n\\n BEST PERFORMING MODELS:\")\n",
    "print(\"=\" * 26)\n",
    "if best_financial_model:\n",
    " print(f\"• Financial: {best_financial_model} (F1: {financial_results[best_financial_model]['f1']:.3f})\")\n",
    "if best_manufacturing_model:\n",
    " print(f\"• Manufacturing: {best_manufacturing_model} (F1: {manufacturing_results[best_manufacturing_model]['f1']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ENSEMBLE ANOMALY DETECTION\n",
    "print(\" 3. ENSEMBLE ANOMALY DETECTION\")\n",
    "print(\"=\" * 31)\n",
    "\n",
    "class EnsembleAnomalyDetector:\n",
    " \"\"\"Ensemble anomaly detector combining multiple methods.\"\"\"\n",
    "\n",
    " def __init__(self, models=None, voting='majority', weights=None):\n",
    " self.models = models or [\n",
    " IsolationForest(contamination=0.1, random_state=42),\n",
    " OneClassSVM(gamma='scale', nu=0.1),\n",
    " EllipticEnvelope(contamination=0.1)\n",
    " ]\n",
    " self.voting = voting\n",
    " self.weights = weights or [1.0] * len(self.models)\n",
    " self.fitted_models = []\n",
    "\n",
    " def fit(self, X):\n",
    " \"\"\"Fit all models in the ensemble.\"\"\"\n",
    " self.fitted_models = []\n",
    " for model in self.models:\n",
    " fitted_model = model.fit(X)\n",
    " self.fitted_models.append(fitted_model)\n",
    " return self\n",
    "\n",
    " def predict(self, X):\n",
    " \"\"\"Predict anomalies using ensemble voting.\"\"\"\n",
    " predictions = []\n",
    "\n",
    " for model in self.fitted_models:\n",
    " pred = model.predict(X)\n",
    " # Convert to binary (1 for normal, 0 for anomaly)\n",
    " pred_binary = (pred == 1).astype(int)\n",
    " predictions.append(pred_binary)\n",
    "\n",
    " predictions = np.array(predictions)\n",
    "\n",
    " if self.voting == 'majority':\n",
    " # Majority vote (anomaly if majority says anomaly)\n",
    " ensemble_pred = (np.mean(predictions, axis=0) < 0.5).astype(int)\n",
    " elif self.voting == 'weighted':\n",
    " # Weighted vote\n",
    " weighted_pred = np.average(predictions, axis=0, weights=self.weights)\n",
    " ensemble_pred = (weighted_pred < 0.5).astype(int)\n",
    "\n",
    " return ensemble_pred\n",
    "\n",
    " def get_anomaly_scores(self, X):\n",
    " \"\"\"Get ensemble anomaly scores.\"\"\"\n",
    " scores = []\n",
    "\n",
    " for model in self.fitted_models:\n",
    " if hasattr(model, 'decision_function'):\n",
    " score = -model.decision_function(X) # Negative for anomaly scores\n",
    " elif hasattr(model, 'score_samples'):\n",
    " score = -model.score_samples(X)\n",
    " else:\n",
    " # Fallback to prediction confidence\n",
    " pred = model.predict(X)\n",
    " score = (pred == -1).astype(float)\n",
    " scores.append(score)\n",
    "\n",
    " # Average scores\n",
    " ensemble_scores = np.mean(scores, axis=0)\n",
    " return ensemble_scores\n",
    "\n",
    "# Test ensemble detector on financial data\n",
    "print(\"Financial Data Ensemble Detection:\")\n",
    "\n",
    "ensemble_detector = EnsembleAnomalyDetector(voting='majority')\n",
    "ensemble_detector.fit(X_financial_scaled)\n",
    "ensemble_predictions = ensemble_detector.predict(X_financial_scaled)\n",
    "ensemble_scores = ensemble_detector.get_anomaly_scores(X_financial_scaled)\n",
    "\n",
    "# Evaluate ensemble performance\n",
    "ensemble_anomalies = ensemble_predictions == 1\n",
    "true_positives = ((ensemble_anomalies) & (financial_df['is_fraud'] == 1)).sum()\n",
    "false_positives = ((ensemble_anomalies) & (financial_df['is_fraud'] == 0)).sum()\n",
    "false_negatives = ((~ensemble_anomalies) & (financial_df['is_fraud'] == 1)).sum()\n",
    "\n",
    "ensemble_precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "ensemble_recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "ensemble_f1 = 2 * (ensemble_precision * ensemble_recall) / (ensemble_precision + ensemble_recall) if (ensemble_precision + ensemble_recall) > 0 else 0\n",
    "\n",
    "print(f\"• Ensemble Anomalies detected: {ensemble_anomalies.sum()} ({ensemble_anomalies.mean()*100:.1f}%)\")\n",
    "print(f\"• Ensemble Precision: {ensemble_precision:.3f}\")\n",
    "print(f\"• Ensemble Recall: {ensemble_recall:.3f}\")\n",
    "print(f\"• Ensemble F1-Score: {ensemble_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. COMPREHENSIVE ANOMALY DETECTION DASHBOARD\n",
    "print(\" 4. COMPREHENSIVE ANOMALY DETECTION DASHBOARD\")\n",
    "print(\"=\" * 49)\n",
    "\n",
    "# Create comprehensive anomaly detection dashboard\n",
    "fig = make_subplots(\n",
    " rows=2, cols=3,\n",
    " subplot_titles=[\n",
    " 'Financial Transaction Anomalies',\n",
    " 'Manufacturing Quality Control',\n",
    " 'Statistical vs ML Methods',\n",
    " 'Anomaly Score Distribution',\n",
    " 'Model Performance Comparison',\n",
    " 'ROC Curves'\n",
    " ],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# 1. Financial Transaction Anomalies (Scatter plot)\n",
    "fraud_mask = financial_df['is_fraud'] == 1\n",
    "normal_mask = financial_df['is_fraud'] == 0\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=financial_df[normal_mask]['transaction_amount'],\n",
    " y=financial_df[normal_mask]['account_age_days'],\n",
    " mode='markers',\n",
    " name='Normal Transactions',\n",
    " marker=dict(color='blue', size=4, opacity=0.6)\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=financial_df[fraud_mask]['transaction_amount'],\n",
    " y=financial_df[fraud_mask]['account_age_days'],\n",
    " mode='markers',\n",
    " name='Fraudulent Transactions',\n",
    " marker=dict(color='red', size=6, symbol='x')\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Manufacturing Quality Control\n",
    "defective_mask = manufacturing_df['is_defective'] == 1\n",
    "normal_parts_mask = manufacturing_df['is_defective'] == 0\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=manufacturing_df[normal_parts_mask]['temperature'],\n",
    " y=manufacturing_df[normal_parts_mask]['pressure'],\n",
    " mode='markers',\n",
    " name='Normal Parts',\n",
    " marker=dict(color='green', size=4, opacity=0.6)\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=manufacturing_df[defective_mask]['temperature'],\n",
    " y=manufacturing_df[defective_mask]['pressure'],\n",
    " mode='markers',\n",
    " name='Defective Parts',\n",
    " marker=dict(color='red', size=6, symbol='x')\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Statistical vs ML Methods Comparison\n",
    "methods = []\n",
    "f1_scores = []\n",
    "\n",
    "# Add statistical methods\n",
    "for method, results in stat_results.items():\n",
    " if 'f1' in results:\n",
    " methods.append(f\"Stat: {method}\")\n",
    " f1_scores.append(results['f1'])\n",
    "\n",
    "# Add ML methods\n",
    "for method, results in financial_results.items():\n",
    " if results and results['f1'] is not None:\n",
    " methods.append(f\"ML: {method}\")\n",
    " f1_scores.append(results['f1'])\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=methods,\n",
    " y=f1_scores,\n",
    " name='F1 Scores',\n",
    " marker_color='lightcoral'\n",
    " ),\n",
    " row=1, col=3\n",
    ")\n",
    "\n",
    "# 4. Anomaly Score Distribution\n",
    "if best_financial_model and financial_results[best_financial_model]:\n",
    " # Get scores from the best model\n",
    " best_model = financial_results[best_financial_model]['model']\n",
    " if hasattr(best_model, 'decision_function'):\n",
    " all_scores = -best_model.decision_function(X_financial_scaled)\n",
    " elif hasattr(best_model, 'score_samples'):\n",
    " all_scores = -best_model.score_samples(X_financial_scaled)\n",
    " else:\n",
    " all_scores = ensemble_scores\n",
    "\n",
    " normal_scores = all_scores[financial_df['is_fraud'] == 0]\n",
    " anomaly_scores = all_scores[financial_df['is_fraud'] == 1]\n",
    "\n",
    " fig.add_trace(\n",
    " go.Histogram(\n",
    " x=normal_scores,\n",
    " name='Normal Scores',\n",
    " opacity=0.7,\n",
    " marker_color='blue',\n",
    " nbinsx=30\n",
    " ),\n",
    " row=2, col=1\n",
    " )\n",
    "\n",
    " fig.add_trace(\n",
    " go.Histogram(\n",
    " x=anomaly_scores,\n",
    " name='Anomaly Scores',\n",
    " opacity=0.7,\n",
    " marker_color='red',\n",
    " nbinsx=30\n",
    " ),\n",
    " row=2, col=1\n",
    " )\n",
    "\n",
    "# 5. Model Performance Comparison\n",
    "model_names = []\n",
    "model_f1_scores = []\n",
    "\n",
    "for method, results in financial_results.items():\n",
    " if results and results['f1'] is not None:\n",
    " model_names.append(method)\n",
    " model_f1_scores.append(results['f1'])\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=model_names,\n",
    " y=model_f1_scores,\n",
    " name='Model F1 Scores',\n",
    " marker_color='lightgreen'\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "# 6. ROC Curve\n",
    "if best_financial_model and financial_results[best_financial_model]:\n",
    " y_true = financial_df['is_fraud']\n",
    " try:\n",
    " fpr, tpr, _ = roc_curve(y_true, all_scores)\n",
    "\n",
    " fig.add_trace(\n",
    " go.Scatter(\n",
    " x=fpr,\n",
    " y=tpr,\n",
    " mode='lines',\n",
    " name=f'ROC ({best_financial_model})',\n",
    " line=dict(color='purple', width=2)\n",
    " ),\n",
    " row=2, col=3\n",
    " )\n",
    "\n",
    " # Add diagonal reference line\n",
    " fig.add_trace(\n",
    " go.Scatter(\n",
    " x=[0, 1],\n",
    " y=[0, 1],\n",
    " mode='lines',\n",
    " name='Random Classifier',\n",
    " line=dict(color='gray', width=1, dash='dash')\n",
    " ),\n",
    " row=2, col=3\n",
    " )\n",
    " except:\n",
    " pass\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    " height=800,\n",
    " title=\"Comprehensive Anomaly Detection Dashboard\",\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Transaction Amount\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Temperature\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Method\", row=1, col=3)\n",
    "fig.update_xaxes(title_text=\"Anomaly Score\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Model\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"False Positive Rate\", row=2, col=3)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Account Age\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Pressure\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"F1 Score\", row=1, col=3)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"F1 Score\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"True Positive Rate\", row=2, col=3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74cd8db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2acf884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d812030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb3432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e39a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd1d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a9fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28dace7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03c3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c00b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d0e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3d17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7628cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb2f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128118f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738158a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5352f108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4f3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60b461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ad212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ce401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b167f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa082901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ecea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7512a562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e777e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17497489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7443ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0bc409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad2d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c81ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e5400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3471b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4287c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8cb65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a0134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad4311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d966b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca12f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca59dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f30e4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32649a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd4a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f0d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a65f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b33f32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061b4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce4f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c5704f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2a37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf70e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45b078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb06b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cfcbbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aece4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4a5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e762b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a20d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf519f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13455ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dab3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405fc36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee6964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99f8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1742c249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c7178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7b872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe9158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88939168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa761756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b7df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5dfd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14704507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a264d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ab03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a350c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2627f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b83d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b438992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4973cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e24da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39dd772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b838b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b6702c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab2e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724951d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5359c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c1cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4898e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b13527f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3df5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf215ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a9588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95819153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782d272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21585c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b2533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}