{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 6: One-Class SVM Anomaly Detection\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** 82de510c-aad9-4113-92dd-4f88b7ccac51\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 6: One-Class SVM Anomaly Detection,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** 82de510c-aad9-4113-92dd-4f88b7ccac51\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea9a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 6: One-Class SVM Anomaly Detection - Libraries Loaded!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"One-Class SVM Techniques:\")\n",
    "print(\"• Boundary-based anomaly detection with support vectors\")\n",
    "print(\"• Kernel trick for non-linear decision boundaries\")\n",
    "print(\"• Nu parameter tuning for outlier fraction control\")\n",
    "print(\"• High-dimensional anomaly detection capabilities\")\n",
    "print(\"• Novelty detection for previously unseen data patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f084c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive One-Class SVM datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Network intrusion detection dataset\n",
    "n_normal_connections = 4000\n",
    "n_intrusions = 200\n",
    "\n",
    "# Normal network traffic patterns\n",
    "normal_connections = []\n",
    "for i in range(n_normal_connections):\n",
    " # Simulate normal network behavior\n",
    " packet_size = np.random.normal(1500, 300) # Standard MTU size\n",
    " connection_duration = np.random.exponential(30) # Typical web session\n",
    " bytes_sent = np.random.lognormal(8, 1.5) # Normal data transfer\n",
    " bytes_received = bytes_sent * np.random.normal(1.2, 0.3) # Response ratio\n",
    " port_number = np.random.choice([80, 443, 22, 25, 53], p=[0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "\n",
    " # Protocol behavior features\n",
    " tcp_flags = np.random.randint(0, 8) # Normal TCP flag combinations\n",
    " packet_count = connection_duration * np.random.poisson(5) # Packets per second\n",
    "\n",
    " normal_connections.append({\n",
    " 'connection_id': f'NORM_{i:06d}',\n",
    " 'packet_size': packet_size,\n",
    " 'duration': connection_duration,\n",
    " 'bytes_sent': bytes_sent,\n",
    " 'bytes_received': bytes_received,\n",
    " 'port': port_number,\n",
    " 'tcp_flags': tcp_flags,\n",
    " 'packet_count': packet_count,\n",
    " 'protocol_anomaly_score': np.random.beta(2, 8), # Low anomaly score\n",
    " 'is_intrusion': 0\n",
    " })\n",
    "\n",
    "# Intrusion patterns (anomalies)\n",
    "intrusion_connections = []\n",
    "intrusion_types = ['DDoS', 'Port_Scan', 'Data_Exfiltration', 'Malware_C2']\n",
    "\n",
    "for i in range(n_intrusions):\n",
    " intrusion_type = np.random.choice(intrusion_types)\n",
    "\n",
    " if intrusion_type == 'DDoS':\n",
    " # High packet count, short duration, small packets\n",
    " packet_size = np.random.normal(64, 20) # Small packets\n",
    " connection_duration = np.random.exponential(2) # Very short\n",
    " bytes_sent = np.random.normal(500, 100)\n",
    " bytes_received = bytes_sent * 0.1 # Minimal response\n",
    " packet_count = np.random.poisson(1000) # Very high packet rate\n",
    "\n",
    " elif intrusion_type == 'Port_Scan':\n",
    " # Many different ports, minimal data\n",
    " packet_size = np.random.normal(100, 30)\n",
    " connection_duration = np.random.exponential(0.5) # Very quick\n",
    " bytes_sent = np.random.normal(50, 20)\n",
    " bytes_received = 0 # No response typically\n",
    " packet_count = np.random.poisson(2)\n",
    "\n",
    " elif intrusion_type == 'Data_Exfiltration':\n",
    " # Large data transfers, unusual patterns\n",
    " packet_size = np.random.normal(1500, 100)\n",
    " connection_duration = np.random.exponential(300) # Long sessions\n",
    " bytes_sent = np.random.lognormal(12, 2) # Very large transfers\n",
    " bytes_received = bytes_sent * 0.1 # Minimal response\n",
    " packet_count = connection_duration * np.random.poisson(20)\n",
    "\n",
    " else: # Malware_C2\n",
    " # Regular but suspicious patterns\n",
    " packet_size = np.random.normal(200, 50)\n",
    " connection_duration = np.random.exponential(60)\n",
    " bytes_sent = np.random.normal(1000, 300)\n",
    " bytes_received = bytes_sent * np.random.normal(0.5, 0.2)\n",
    " packet_count = np.random.poisson(30)\n",
    "\n",
    " port_number = np.random.randint(1024, 65535) if intrusion_type == 'Port_Scan' else np.random.choice([80, 443])\n",
    " tcp_flags = np.random.randint(8, 16) # Unusual flag combinations\n",
    "\n",
    " intrusion_connections.append({\n",
    " 'connection_id': f'{intrusion_type}_{i:06d}',\n",
    " 'packet_size': packet_size,\n",
    " 'duration': connection_duration,\n",
    " 'bytes_sent': bytes_sent,\n",
    " 'bytes_received': bytes_received,\n",
    " 'port': port_number,\n",
    " 'tcp_flags': tcp_flags,\n",
    " 'packet_count': packet_count,\n",
    " 'protocol_anomaly_score': np.random.beta(8, 2), # High anomaly score\n",
    " 'is_intrusion': 1,\n",
    " 'intrusion_type': intrusion_type\n",
    " })\n",
    "\n",
    "# Combine datasets\n",
    "all_connections = normal_connections + intrusion_connections\n",
    "network_df = pd.DataFrame(all_connections).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\" Network Intrusion Dataset Created:\")\n",
    "print(f\"Total connections: {len(network_df)}\")\n",
    "print(f\"Intrusion connections: {n_intrusions} ({n_intrusions/len(network_df)*100:.1f}%)\")\n",
    "print(f\"Intrusion types: {network_df[network_df['is_intrusion']==1]['intrusion_type'].value_counts().to_dict()}\")\n",
    "\n",
    "# 2. Industrial equipment sensor dataset\n",
    "n_normal_readings = 3500\n",
    "n_fault_readings = 175\n",
    "\n",
    "# Normal equipment operation\n",
    "normal_readings = []\n",
    "for i in range(n_normal_readings):\n",
    " # Normal operating parameters with correlations\n",
    " temperature = np.random.normal(75, 5) # °C\n",
    " pressure = np.random.normal(100, 8) # PSI\n",
    " vibration = np.random.normal(2, 0.5) # mm/s\n",
    " flow_rate = np.random.normal(50, 5) # L/min\n",
    "\n",
    " # Correlated parameters\n",
    " power_consumption = 100 + 0.8 * temperature + 0.3 * pressure + np.random.normal(0, 3)\n",
    " efficiency = 95 - 0.1 * temperature + 0.05 * flow_rate + np.random.normal(0, 2)\n",
    "\n",
    " normal_readings.append({\n",
    " 'reading_id': f'NORMAL_{i:06d}',\n",
    " 'temperature': temperature,\n",
    " 'pressure': pressure,\n",
    " 'vibration': vibration,\n",
    " 'flow_rate': flow_rate,\n",
    " 'power_consumption': power_consumption,\n",
    " 'efficiency': efficiency,\n",
    " 'is_fault': 0\n",
    " })\n",
    "\n",
    "# Equipment fault patterns\n",
    "fault_readings = []\n",
    "fault_types = ['Overheating', 'Pressure_Leak', 'Bearing_Wear', 'Flow_Blockage']\n",
    "\n",
    "for i in range(n_fault_readings):\n",
    " fault_type = np.random.choice(fault_types)\n",
    "\n",
    " if fault_type == 'Overheating':\n",
    " temperature = np.random.normal(95, 8) # High temperature\n",
    " pressure = np.random.normal(100, 8)\n",
    " vibration = np.random.normal(2.5, 0.8) # Slight increase\n",
    " flow_rate = np.random.normal(48, 5) # Slight decrease\n",
    "\n",
    " elif fault_type == 'Pressure_Leak':\n",
    " temperature = np.random.normal(75, 5)\n",
    " pressure = np.random.normal(80, 10) # Low pressure\n",
    " vibration = np.random.normal(3, 1) # Higher vibration\n",
    " flow_rate = np.random.normal(45, 8) # Reduced flow\n",
    "\n",
    " elif fault_type == 'Bearing_Wear':\n",
    " temperature = np.random.normal(80, 6) # Elevated temperature\n",
    " pressure = np.random.normal(100, 8)\n",
    " vibration = np.random.normal(8, 2) # Very high vibration\n",
    " flow_rate = np.random.normal(50, 5)\n",
    "\n",
    " else: # Flow_Blockage\n",
    " temperature = np.random.normal(78, 5)\n",
    " pressure = np.random.normal(120, 15) # High pressure\n",
    " vibration = np.random.normal(2, 0.5)\n",
    " flow_rate = np.random.normal(30, 10) # Very low flow\n",
    "\n",
    " power_consumption = 100 + 0.8 * temperature + 0.3 * pressure + np.random.normal(10, 5) # Higher variance\n",
    " efficiency = 85 - 0.2 * temperature + 0.02 * flow_rate + np.random.normal(0, 5) # Lower efficiency\n",
    "\n",
    " fault_readings.append({\n",
    " 'reading_id': f'{fault_type}_{i:06d}',\n",
    " 'temperature': temperature,\n",
    " 'pressure': pressure,\n",
    " 'vibration': vibration,\n",
    " 'flow_rate': flow_rate,\n",
    " 'power_consumption': power_consumption,\n",
    " 'efficiency': efficiency,\n",
    " 'is_fault': 1,\n",
    " 'fault_type': fault_type\n",
    " })\n",
    "\n",
    "# Combine equipment data\n",
    "all_readings = normal_readings + fault_readings\n",
    "equipment_df = pd.DataFrame(all_readings).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nIndustrial Equipment Dataset Created:\")\n",
    "print(f\"Total readings: {len(equipment_df)}\")\n",
    "print(f\"Fault readings: {n_fault_readings} ({n_fault_readings/len(equipment_df)*100:.1f}%)\")\n",
    "print(f\"Fault types: {equipment_df[equipment_df['is_fault']==1]['fault_type'].value_counts().to_dict()}\")\n",
    "\n",
    "# 3. Create 2D synthetic dataset for visualization\n",
    "X_synthetic, _ = make_blobs(n_samples=800, centers=1, cluster_std=1.0, random_state=42)\n",
    "# Add outliers\n",
    "n_outliers = 40\n",
    "outliers = np.random.uniform(low=-6, high=6, size=(n_outliers, 2))\n",
    "X_synthetic_with_outliers = np.vstack([X_synthetic, outliers])\n",
    "y_synthetic = np.hstack([np.ones(len(X_synthetic)), -np.ones(n_outliers)])\n",
    "\n",
    "print(f\"\\nSynthetic 2D Dataset Created:\")\n",
    "print(f\"Normal points: {len(X_synthetic)}\")\n",
    "print(f\"Outlier points: {n_outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ONE-CLASS SVM MODEL DEVELOPMENT AND TUNING\n",
    "print(\" 1. ONE-CLASS SVM MODEL DEVELOPMENT AND TUNING\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "# Prepare network intrusion data\n",
    "network_features = ['packet_size', 'duration', 'bytes_sent', 'bytes_received',\n",
    " 'packet_count', 'protocol_anomaly_score']\n",
    "X_network = network_df[network_features].values\n",
    "y_network = network_df['is_intrusion'].values\n",
    "\n",
    "# Split into normal training data and test data\n",
    "X_normal = X_network[y_network == 0] # Only normal data for training\n",
    "X_test = X_network # Full dataset for testing\n",
    "y_test = y_network\n",
    "\n",
    "# Standardize features\n",
    "scaler_network = StandardScaler()\n",
    "X_normal_scaled = scaler_network.fit_transform(X_normal)\n",
    "X_test_scaled = scaler_network.transform(X_test)\n",
    "\n",
    "print(f\"Network Data Preparation:\")\n",
    "print(f\"Training samples (normal only): {len(X_normal_scaled)}\")\n",
    "print(f\"Test samples (normal + intrusions): {len(X_test_scaled)}\")\n",
    "\n",
    "# One-Class SVM parameter tuning\n",
    "param_grid = {\n",
    " 'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
    " 'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    " 'nu': [0.01, 0.05, 0.1, 0.2] # Expected fraction of outliers\n",
    "}\n",
    "\n",
    "print(f\"\\nParameter Grid Search:\")\n",
    "best_score = -1\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Manual grid search (since One-Class SVM doesn't work well with standard CV)\n",
    "results = []\n",
    "\n",
    "for kernel in param_grid['kernel']:\n",
    " for gamma in param_grid['gamma']:\n",
    " for nu in param_grid['nu']:\n",
    " try:\n",
    " # Train One-Class SVM\n",
    " ocsvm = OneClassSVM(kernel=kernel, gamma=gamma, nu=nu, random_state=42)\n",
    " ocsvm.fit(X_normal_scaled)\n",
    "\n",
    " # Predict on test set\n",
    " predictions = ocsvm.predict(X_test_scaled)\n",
    " predictions_binary = (predictions == -1).astype(int) # Convert to binary (1 = anomaly)\n",
    "\n",
    " # Calculate performance metrics\n",
    " precision = precision_score(y_test, predictions_binary, zero_division=0)\n",
    " recall = recall_score(y_test, predictions_binary, zero_division=0)\n",
    " f1 = f1_score(y_test, predictions_binary, zero_division=0)\n",
    "\n",
    " results.append({\n",
    " 'kernel': kernel,\n",
    " 'gamma': gamma,\n",
    " 'nu': nu,\n",
    " 'precision': precision,\n",
    " 'recall': recall,\n",
    " 'f1': f1\n",
    " })\n",
    "\n",
    " if f1 > best_score:\n",
    " best_score = f1\n",
    " best_params = {'kernel': kernel, 'gamma': gamma, 'nu': nu}\n",
    " best_model = ocsvm\n",
    "\n",
    " print(f\"kernel={kernel}, gamma={gamma}, nu={nu}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}\")\n",
    "\n",
    " except Exception as e:\n",
    " print(f\"Error with kernel={kernel}, gamma={gamma}, nu={nu}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nBest One-Class SVM parameters: {best_params}\")\n",
    "print(f\"Best F1 score: {best_score:.3f}\")\n",
    "\n",
    "# Apply best model to network data\n",
    "best_predictions = best_model.predict(X_test_scaled)\n",
    "best_predictions_binary = (best_predictions == -1).astype(int)\n",
    "decision_scores = best_model.decision_function(X_test_scaled)\n",
    "\n",
    "network_df['ocsvm_prediction'] = best_predictions_binary\n",
    "network_df['decision_score'] = decision_scores\n",
    "\n",
    "# Equipment fault detection\n",
    "equipment_features = ['temperature', 'pressure', 'vibration', 'flow_rate',\n",
    " 'power_consumption', 'efficiency']\n",
    "X_equipment = equipment_df[equipment_features].values\n",
    "y_equipment = equipment_df['is_fault'].values\n",
    "\n",
    "X_equipment_normal = X_equipment[y_equipment == 0]\n",
    "scaler_equipment = StandardScaler()\n",
    "X_equipment_normal_scaled = scaler_equipment.fit_transform(X_equipment_normal)\n",
    "X_equipment_scaled = scaler_equipment.transform(X_equipment)\n",
    "\n",
    "# Train equipment One-Class SVM with best parameters from network analysis\n",
    "equipment_ocsvm = OneClassSVM(**best_params, random_state=42)\n",
    "equipment_ocsvm.fit(X_equipment_normal_scaled)\n",
    "\n",
    "equipment_predictions = equipment_ocsvm.predict(X_equipment_scaled)\n",
    "equipment_predictions_binary = (equipment_predictions == -1).astype(int)\n",
    "equipment_decision_scores = equipment_ocsvm.decision_function(X_equipment_scaled)\n",
    "\n",
    "equipment_df['ocsvm_prediction'] = equipment_predictions_binary\n",
    "equipment_df['decision_score'] = equipment_decision_scores\n",
    "\n",
    "# Equipment performance\n",
    "equipment_precision = precision_score(y_equipment, equipment_predictions_binary, zero_division=0)\n",
    "equipment_recall = recall_score(y_equipment, equipment_predictions_binary, zero_division=0)\n",
    "equipment_f1 = f1_score(y_equipment, equipment_predictions_binary, zero_division=0)\n",
    "\n",
    "print(f\"\\nEquipment Fault Detection Performance:\")\n",
    "print(f\"Precision: {equipment_precision:.3f}\")\n",
    "print(f\"Recall: {equipment_recall:.3f}\")\n",
    "print(f\"F1-Score: {equipment_f1:.3f}\")\n",
    "\n",
    "# Synthetic 2D data for visualization\n",
    "scaler_synthetic = StandardScaler()\n",
    "X_synthetic_scaled = scaler_synthetic.fit_transform(X_synthetic_with_outliers)\n",
    "X_normal_synthetic = X_synthetic_scaled[y_synthetic == 1]\n",
    "\n",
    "synthetic_ocsvm = OneClassSVM(kernel='rbf', gamma=0.1, nu=0.05, random_state=42)\n",
    "synthetic_ocsvm.fit(X_normal_synthetic)\n",
    "\n",
    "synthetic_predictions = synthetic_ocsvm.predict(X_synthetic_scaled)\n",
    "synthetic_decision_scores = synthetic_ocsvm.decision_function(X_synthetic_scaled)\n",
    "\n",
    "print(f\"\\nSynthetic Data Results:\")\n",
    "print(f\"Normal points detected as normal: {np.sum((y_synthetic == 1) & (synthetic_predictions == 1))}\")\n",
    "print(f\"Outliers detected as anomalies: {np.sum((y_synthetic == -1) & (synthetic_predictions == -1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56983784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. INTERACTIVE ONE-CLASS SVM VISUALIZATIONS\n",
    "print(\" 2. INTERACTIVE ONE-CLASS SVM VISUALIZATIONS\")\n",
    "print(\"=\" * 44)\n",
    "\n",
    "# Create comprehensive One-Class SVM dashboard\n",
    "fig = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=[\n",
    " 'One-Class SVM Decision Boundary (2D Synthetic)',\n",
    " 'Network Intrusion Detection Results',\n",
    " 'Parameter Tuning Performance Heatmap',\n",
    " 'Equipment Fault Detection by Sensor Type',\n",
    " 'Decision Score Distributions',\n",
    " 'ROC-Style Performance Analysis'\n",
    " ],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# 1. 2D Decision boundary visualization\n",
    "# Create decision boundary mesh\n",
    "h = 0.1\n",
    "x_min, x_max = X_synthetic_scaled[:, 0].min() - 1, X_synthetic_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_synthetic_scaled[:, 1].min() - 1, X_synthetic_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Get decision function values for the mesh\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = synthetic_ocsvm.decision_function(mesh_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary contour\n",
    "fig.add_trace(\n",
    " go.Contour(\n",
    " x=np.arange(x_min, x_max, h),\n",
    " y=np.arange(y_min, y_max, h),\n",
    " z=Z,\n",
    " levels=[0],\n",
    " line=dict(color='red', width=3),\n",
    " showscale=False,\n",
    " name='Decision Boundary'\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Plot normal points\n",
    "normal_mask = y_synthetic == 1\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=X_synthetic_scaled[normal_mask, 0],\n",
    " y=X_synthetic_scaled[normal_mask, 1],\n",
    " mode='markers',\n",
    " name='Normal Points',\n",
    " marker=dict(color='blue', size=6, opacity=0.7),\n",
    " hovertemplate='Normal<br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>'\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Plot outliers\n",
    "outlier_mask = y_synthetic == -1\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=X_synthetic_scaled[outlier_mask, 0],\n",
    " y=X_synthetic_scaled[outlier_mask, 1],\n",
    " mode='markers',\n",
    " name='True Outliers',\n",
    " marker=dict(color='red', size=8, opacity=0.8, symbol='x'),\n",
    " hovertemplate='Outlier<br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>'\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Network intrusion detection results\n",
    "normal_connections = network_df[network_df['is_intrusion'] == 0]\n",
    "intrusion_connections = network_df[network_df['is_intrusion'] == 1]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=normal_connections['bytes_sent'],\n",
    " y=normal_connections['duration'],\n",
    " mode='markers',\n",
    " name='Normal Connections',\n",
    " marker=dict(color='green', size=4, opacity=0.6),\n",
    " hovertemplate='Normal<br>Bytes Sent: %{x:,.0f}<br>Duration: %{y:.1f}s<extra></extra>'\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=intrusion_connections['bytes_sent'],\n",
    " y=intrusion_connections['duration'],\n",
    " mode='markers',\n",
    " name='Intrusions',\n",
    " marker=dict(color='red', size=6, opacity=0.8),\n",
    " text=intrusion_connections['intrusion_type'],\n",
    " hovertemplate='%{text}<br>Bytes Sent: %{x:,.0f}<br>Duration: %{y:.1f}s<extra></extra>'\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Parameter tuning heatmap\n",
    "results_df = pd.DataFrame(results)\n",
    "# Create pivot table for heatmap\n",
    "heatmap_data = results_df.pivot_table(values='f1', index='nu', columns='gamma', aggfunc='max')\n",
    "\n",
    "fig.add_trace(\n",
    " go.Heatmap(\n",
    " z=heatmap_data.values,\n",
    " x=heatmap_data.columns,\n",
    " y=heatmap_data.index,\n",
    " colorscale='Viridis',\n",
    " showscale=True,\n",
    " hovertemplate='Gamma: %{x}<br>Nu: %{y}<br>F1-Score: %{z:.3f}<extra></extra>'\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Equipment fault detection by sensor type\n",
    "fault_types = equipment_df[equipment_df['is_fault'] == 1]['fault_type'].unique()\n",
    "colors = ['red', 'orange', 'purple', 'brown']\n",
    "\n",
    "for i, fault_type in enumerate(fault_types):\n",
    " fault_data = equipment_df[equipment_df.get('fault_type') == fault_type]\n",
    "\n",
    " fig.add_trace(\n",
    " go.Scatter(\n",
    " x=fault_data['temperature'],\n",
    " y=fault_data['vibration'],\n",
    " mode='markers',\n",
    " name=f'{fault_type}',\n",
    " marker=dict(color=colors[i % len(colors)], size=8, opacity=0.8),\n",
    " hovertemplate=f'{fault_type}<br>Temperature: %{{x:.1f}}°C<br>Vibration: %{{y:.1f}} mm/s<extra></extra>'\n",
    " ),\n",
    " row=2, col=2\n",
    " )\n",
    "\n",
    "# Normal equipment data\n",
    "normal_equipment = equipment_df[equipment_df['is_fault'] == 0]\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=normal_equipment['temperature'],\n",
    " y=normal_equipment['vibration'],\n",
    " mode='markers',\n",
    " name='Normal Operation',\n",
    " marker=dict(color='green', size=4, opacity=0.5),\n",
    " hovertemplate='Normal<br>Temperature: %{x:.1f}°C<br>Vibration: %{y:.1f} mm/s<extra></extra>'\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "# 5. Decision score distributions\n",
    "fig.add_trace(\n",
    " go.Histogram(\n",
    " x=network_df[network_df['is_intrusion'] == 0]['decision_score'],\n",
    " name='Normal Scores',\n",
    " opacity=0.7,\n",
    " nbinsx=30,\n",
    " marker_color='blue'\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Histogram(\n",
    " x=network_df[network_df['is_intrusion'] == 1]['decision_score'],\n",
    " name='Intrusion Scores',\n",
    " opacity=0.7,\n",
    " nbinsx=30,\n",
    " marker_color='red'\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Performance analysis across different thresholds\n",
    "thresholds = np.linspace(-2, 1, 50)\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    " threshold_predictions = (network_df['decision_score'] < threshold).astype(int)\n",
    "\n",
    " precision = precision_score(network_df['is_intrusion'], threshold_predictions, zero_division=0)\n",
    " recall = recall_score(network_df['is_intrusion'], threshold_predictions, zero_division=0)\n",
    " f1 = f1_score(network_df['is_intrusion'], threshold_predictions, zero_division=0)\n",
    "\n",
    " precisions.append(precision)\n",
    " recalls.append(recall)\n",
    " f1_scores.append(f1)\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=thresholds,\n",
    " y=precisions,\n",
    " mode='lines',\n",
    " name='Precision',\n",
    " line=dict(color='blue', width=2)\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=thresholds,\n",
    " y=recalls,\n",
    " mode='lines',\n",
    " name='Recall',\n",
    " line=dict(color='red', width=2)\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=thresholds,\n",
    " y=f1_scores,\n",
    " mode='lines',\n",
    " name='F1-Score',\n",
    " line=dict(color='green', width=3)\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    " height=1200,\n",
    " title=\"One-Class SVM Anomaly Detection Dashboard\",\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Feature 1\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Bytes Sent\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Gamma Parameter\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Temperature (°C)\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"Decision Score\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"Decision Threshold\", row=3, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Feature 2\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Connection Duration (s)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Nu Parameter\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Vibration (mm/s)\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Performance Score\", row=3, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Business insights and applications\n",
    "print(f\"\\n ONE-CLASS SVM BUSINESS INSIGHTS:\")\n",
    "\n",
    "# Network security analysis\n",
    "total_connections = len(network_df)\n",
    "detected_intrusions = np.sum(network_df['ocsvm_prediction'])\n",
    "actual_intrusions = np.sum(network_df['is_intrusion'])\n",
    "detection_rate = np.sum((network_df['is_intrusion'] == 1) & (network_df['ocsvm_prediction'] == 1)) / actual_intrusions\n",
    "\n",
    "print(f\"\\nNetwork Security Analysis:\")\n",
    "print(f\"• Total connections analyzed: {total_connections:,}\")\n",
    "print(f\"• Actual intrusions: {actual_intrusions}\")\n",
    "print(f\"• Detected as anomalous: {detected_intrusions}\")\n",
    "print(f\"• Detection rate: {detection_rate*100:.1f}%\")\n",
    "print(f\"• False positive rate: {(detected_intrusions - actual_intrusions)/total_connections*100:.2f}%\")\n",
    "\n",
    "# Calculate security ROI\n",
    "avg_intrusion_cost = 50_000 # Average cost per successful intrusion\n",
    "prevented_intrusions = actual_intrusions * detection_rate\n",
    "security_savings = prevented_intrusions * avg_intrusion_cost\n",
    "\n",
    "print(f\"• Prevented intrusion cost: ${security_savings:,.0f}\")\n",
    "\n",
    "# Equipment maintenance analysis\n",
    "total_readings = len(equipment_df)\n",
    "detected_faults = np.sum(equipment_df['ocsvm_prediction'])\n",
    "actual_faults = np.sum(equipment_df['is_fault'])\n",
    "fault_detection_rate = np.sum((equipment_df['is_fault'] == 1) & (equipment_df['ocsvm_prediction'] == 1)) / actual_faults\n",
    "\n",
    "print(f\"\\nEquipment Maintenance Analysis:\")\n",
    "print(f\"• Total sensor readings: {total_readings:,}\")\n",
    "print(f\"• Actual faults: {actual_faults}\")\n",
    "print(f\"• Detected as anomalous: {detected_faults}\")\n",
    "print(f\"• Fault detection rate: {fault_detection_rate*100:.1f}%\")\n",
    "\n",
    "# Calculate maintenance ROI\n",
    "avg_unplanned_downtime_cost = 100_000 # Cost per unplanned downtime event\n",
    "avg_planned_maintenance_cost = 5_000 # Cost per planned maintenance\n",
    "prevented_downtime = actual_faults * fault_detection_rate\n",
    "maintenance_savings = prevented_downtime * (avg_unplanned_downtime_cost - avg_planned_maintenance_cost)\n",
    "\n",
    "print(f\"• Prevented downtime events: {prevented_downtime:.0f}\")\n",
    "print(f\"• Maintenance cost savings: ${maintenance_savings:,.0f}\")\n",
    "\n",
    "# System implementation costs\n",
    "ocsvm_implementation_cost = 200_000 # Initial setup\n",
    "annual_operational_cost = 75_000 # Annual monitoring and maintenance\n",
    "\n",
    "total_annual_benefits = security_savings + maintenance_savings\n",
    "net_annual_benefits = total_annual_benefits - annual_operational_cost\n",
    "roi = (net_annual_benefits - ocsvm_implementation_cost) / ocsvm_implementation_cost\n",
    "\n",
    "print(f\"\\n ONE-CLASS SVM IMPLEMENTATION ROI:\")\n",
    "print(f\"• Security savings: ${security_savings:,.0f}\")\n",
    "print(f\"• Maintenance savings: ${maintenance_savings:,.0f}\")\n",
    "print(f\"• Total annual benefits: ${total_annual_benefits:,.0f}\")\n",
    "print(f\"• Implementation cost: ${ocsvm_implementation_cost:,.0f}\")\n",
    "print(f\"• Annual operational cost: ${annual_operational_cost:,.0f}\")\n",
    "print(f\"• Net annual benefits: ${net_annual_benefits:,.0f}\")\n",
    "print(f\"• ROI: {roi*100:.0f}%\")\n",
    "print(f\"• Payback period: {ocsvm_implementation_cost/net_annual_benefits*12:.1f} months\")\n",
    "\n",
    "# Kernel comparison insights\n",
    "kernel_performance = results_df.groupby('kernel')['f1'].max().sort_values(ascending=False)\n",
    "print(f\"\\nKernel Performance Ranking:\")\n",
    "for kernel, f1_score in kernel_performance.items():\n",
    " print(f\"• {kernel}: {f1_score:.3f}\")\n",
    "\n",
    "print(f\"\\n Cross-Reference Learning Path:\")\n",
    "print(f\"• Foundation: Tier5_SVM.ipynb (support vector machine concepts)\")\n",
    "print(f\"• Comparison: Tier6_IsolationForest.ipynb (tree-based anomaly detection)\")\n",
    "print(f\"• Comparison: Tier6_StatAnomaly.ipynb (statistical methods)\")\n",
    "print(f\"• Advanced: Advanced_EnsembleAnomalyDetection.ipynb (combining methods)\")\n",
    "print(f\"• Application: Advanced_RealtimeAnomalyDetection.ipynb (streaming data)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}