{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 4: DBSCAN Clustering\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** 97cd4a02-d93d-40b1-b75b-fe523d810120\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 4: DBSCAN Clustering,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** 97cd4a02-d93d-40b1-b75b-fe523d810120\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 4: DBSCAN Clustering - Libraries Loaded!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"DBSCAN Techniques:\")\n",
    "print(\"• Density-based clustering with noise detection\")\n",
    "print(\"• Automatic cluster number determination\")\n",
    "print(\"• Outlier identification and removal\")\n",
    "print(\"• Parameter optimization (eps, min_samples)\")\n",
    "print(\"• Irregular cluster shape handling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate DBSCAN-optimized datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# Geospatial store locations with density clusters\n",
    "n_locations = 800\n",
    "city_centers = [(40.7589, -73.9851), (40.6892, -74.0445), (40.8176, -73.9782)] # NYC areas\n",
    "\n",
    "geo_data = []\n",
    "cluster_labels_true = []\n",
    "\n",
    "for i, (lat_center, lon_center) in enumerate(city_centers):\n",
    " n_cluster = np.random.randint(150, 200)\n",
    "\n",
    " # Generate clustered points with varying density\n",
    " lats = np.random.normal(lat_center, 0.02, n_cluster)\n",
    " lons = np.random.normal(lon_center, 0.02, n_cluster)\n",
    "\n",
    " for lat, lon in zip(lats, lons):\n",
    " geo_data.append({\n",
    " 'latitude': lat,\n",
    " 'longitude': lon,\n",
    " 'sales_volume': np.random.lognormal(10, 0.5),\n",
    " 'foot_traffic': np.random.poisson(100),\n",
    " 'competition_nearby': np.random.beta(2, 5)\n",
    " })\n",
    " cluster_labels_true.append(i)\n",
    "\n",
    "# Add noise points (outliers)\n",
    "n_noise = 200\n",
    "for _ in range(n_noise):\n",
    " geo_data.append({\n",
    " 'latitude': np.random.uniform(40.5, 41.0),\n",
    " 'longitude': np.random.uniform(-74.3, -73.7),\n",
    " 'sales_volume': np.random.lognormal(8, 1),\n",
    " 'foot_traffic': np.random.poisson(30),\n",
    " 'competition_nearby': np.random.beta(5, 2)\n",
    " })\n",
    " cluster_labels_true.append(-1) # Noise label\n",
    "\n",
    "geo_df = pd.DataFrame(geo_data)\n",
    "geo_df['true_cluster'] = cluster_labels_true\n",
    "\n",
    "print(\" DBSCAN Dataset Created:\")\n",
    "print(f\"Total locations: {len(geo_df)}\")\n",
    "print(f\"True clusters: {len(set(cluster_labels_true)) - (1 if -1 in cluster_labels_true else 0)}\")\n",
    "print(f\"Noise points: {sum(1 for x in cluster_labels_true if x == -1)}\")\n",
    "print(f\"Sales range: ${geo_df['sales_volume'].min():,.0f} - ${geo_df['sales_volume'].max():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff32efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DBSCAN PARAMETER OPTIMIZATION\n",
    "print(\" 1. DBSCAN PARAMETER OPTIMIZATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Prepare data for clustering\n",
    "features = ['latitude', 'longitude', 'sales_volume', 'foot_traffic', 'competition_nearby']\n",
    "scaler = StandardScaler()\n",
    "geo_scaled = scaler.fit_transform(geo_df[features])\n",
    "\n",
    "# Find optimal eps using k-distance graph\n",
    "def find_optimal_eps(data, k=4):\n",
    " \"\"\"Find optimal eps parameter using k-distance graph\"\"\"\n",
    " nbrs = NearestNeighbors(n_neighbors=k).fit(data)\n",
    " distances, indices = nbrs.kneighbors(data)\n",
    " distances = np.sort(distances[:, k-1], axis=0)\n",
    " return distances\n",
    "\n",
    "k_distances = find_optimal_eps(geo_scaled, k=4)\n",
    "# Optimal eps is typically at the \"elbow\" of the k-distance curve\n",
    "eps_optimal = np.percentile(k_distances, 95) # Conservative estimate\n",
    "\n",
    "print(f\"Optimal eps estimate: {eps_optimal:.3f}\")\n",
    "\n",
    "# Test different parameter combinations\n",
    "eps_values = [eps_optimal * 0.5, eps_optimal * 0.75, eps_optimal, eps_optimal * 1.25, eps_optimal * 1.5]\n",
    "min_samples_values = [3, 4, 5, 6]\n",
    "\n",
    "dbscan_results = {}\n",
    "best_score = -1\n",
    "best_params = None\n",
    "\n",
    "for eps in eps_values:\n",
    " for min_samples in min_samples_values:\n",
    " dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    " cluster_labels = dbscan.fit_predict(geo_scaled)\n",
    "\n",
    " # Calculate metrics (only if we have non-noise clusters)\n",
    " n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    " n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    " if n_clusters > 1 and n_clusters < len(geo_df) - n_noise:\n",
    " # Only calculate silhouette for non-noise points\n",
    " non_noise_mask = cluster_labels != -1\n",
    " if np.sum(non_noise_mask) > n_clusters:\n",
    " silhouette = silhouette_score(geo_scaled[non_noise_mask],\n",
    " cluster_labels[non_noise_mask])\n",
    " else:\n",
    " silhouette = -1\n",
    " else:\n",
    " silhouette = -1\n",
    "\n",
    " dbscan_results[(eps, min_samples)] = {\n",
    " 'labels': cluster_labels,\n",
    " 'n_clusters': n_clusters,\n",
    " 'n_noise': n_noise,\n",
    " 'silhouette': silhouette\n",
    " }\n",
    "\n",
    " if silhouette > best_score:\n",
    " best_score = silhouette\n",
    " best_params = (eps, min_samples)\n",
    "\n",
    " print(f\"eps={eps:.3f}, min_samples={min_samples}: \"\n",
    " f\"clusters={n_clusters}, noise={n_noise}, silhouette={silhouette:.3f}\")\n",
    "\n",
    "# Apply best DBSCAN model\n",
    "best_eps, best_min_samples = best_params\n",
    "final_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "final_labels = final_dbscan.fit_predict(geo_scaled)\n",
    "\n",
    "geo_df['dbscan_cluster'] = final_labels\n",
    "n_clusters_final = len(set(final_labels)) - (1 if -1 in final_labels else 0)\n",
    "n_noise_final = list(final_labels).count(-1)\n",
    "\n",
    "print(f\"\\nBest DBSCAN parameters: eps={best_eps:.3f}, min_samples={best_min_samples}\")\n",
    "print(f\"Final results: {n_clusters_final} clusters, {n_noise_final} noise points\")\n",
    "print(f\"Best silhouette score: {best_score:.3f}\")\n",
    "\n",
    "# Compare with true labels\n",
    "if len(set(cluster_labels_true)) > 1:\n",
    " ari_score = adjusted_rand_score(cluster_labels_true, final_labels)\n",
    " print(f\"Adjusted Rand Index vs true clusters: {ari_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13447e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. INTERACTIVE DBSCAN VISUALIZATIONS\n",
    "print(\" 2. INTERACTIVE DBSCAN VISUALIZATIONS\")\n",
    "print(\"=\" * 39)\n",
    "\n",
    "# Create comprehensive DBSCAN dashboard\n",
    "fig = make_subplots(\n",
    " rows=2, cols=2,\n",
    " subplot_titles=[\n",
    " 'K-Distance Graph (Eps Optimization)',\n",
    " 'Geographic Clusters (DBSCAN Results)',\n",
    " 'Parameter Grid Search Heatmap',\n",
    " 'Cluster Characteristics Comparison'\n",
    " ],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# K-distance graph\n",
    "sorted_indices = np.argsort(k_distances)\n",
    "fig.add_trace(\n",
    " go.Scatter(x=np.arange(len(k_distances)), y=k_distances[sorted_indices],\n",
    " mode='lines', name='4-NN Distance',\n",
    " line=dict(color='blue', width=2)),\n",
    " row=1, col=1\n",
    ")\n",
    "fig.add_hline(y=eps_optimal, line=dict(color='red', dash='dash'),\n",
    " annotation_text=f\"Optimal eps: {eps_optimal:.3f}\", row=1, col=1)\n",
    "\n",
    "# Geographic visualization with DBSCAN results\n",
    "colors = ['red', 'blue', 'green', 'purple', 'orange', 'yellow', 'pink']\n",
    "unique_clusters = sorted(set(final_labels))\n",
    "\n",
    "for cluster in unique_clusters:\n",
    " cluster_data = geo_df[geo_df['dbscan_cluster'] == cluster]\n",
    "\n",
    " if cluster == -1: # Noise points\n",
    " fig.add_trace(\n",
    " go.Scattermapbox(\n",
    " lat=cluster_data['latitude'],\n",
    " lon=cluster_data['longitude'],\n",
    " mode='markers',\n",
    " marker=dict(size=6, color='black', opacity=0.6),\n",
    " name='Noise',\n",
    " text=cluster_data['sales_volume'].round(0),\n",
    " hovertemplate='Noise Point<br>Sales: $%{text:,.0f}<extra></extra>'\n",
    " ),\n",
    " row=1, col=2\n",
    " )\n",
    " else:\n",
    " color = colors[cluster % len(colors)]\n",
    " fig.add_trace(\n",
    " go.Scattermapbox(\n",
    " lat=cluster_data['latitude'],\n",
    " lon=cluster_data['longitude'],\n",
    " mode='markers',\n",
    " marker=dict(size=8, color=color, opacity=0.8),\n",
    " name=f'Cluster {cluster}',\n",
    " text=cluster_data['sales_volume'].round(0),\n",
    " hovertemplate=f'Cluster {cluster}<br>Sales: $%{{text:,.0f}}<extra></extra>'\n",
    " ),\n",
    " row=1, col=2\n",
    " )\n",
    "\n",
    "# Parameter grid search heatmap\n",
    "eps_grid = []\n",
    "min_samples_grid = []\n",
    "silhouette_grid = []\n",
    "\n",
    "for (eps, min_samples), results in dbscan_results.items():\n",
    " eps_grid.append(eps)\n",
    " min_samples_grid.append(min_samples)\n",
    " silhouette_grid.append(results['silhouette'])\n",
    "\n",
    "# Create parameter performance matrix\n",
    "eps_unique = sorted(set(eps_grid))\n",
    "min_samples_unique = sorted(set(min_samples_grid))\n",
    "performance_matrix = np.full((len(min_samples_unique), len(eps_unique)), -1.0)\n",
    "\n",
    "for i, ms in enumerate(min_samples_unique):\n",
    " for j, eps in enumerate(eps_unique):\n",
    " for k, (eps_val, ms_val) in enumerate(zip(eps_grid, min_samples_grid)):\n",
    " if eps_val == eps and ms_val == ms:\n",
    " performance_matrix[i, j] = silhouette_grid[k]\n",
    " break\n",
    "\n",
    "fig.add_trace(\n",
    " go.Heatmap(\n",
    " z=performance_matrix,\n",
    " x=[f\"{eps:.3f}\" for eps in eps_unique],\n",
    " y=[f\"{ms}\" for ms in min_samples_unique],\n",
    " colorscale='Viridis',\n",
    " showscale=True,\n",
    " hovertemplate='eps: %{x}<br>min_samples: %{y}<br>Silhouette: %{z:.3f}<extra></extra>'\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# Cluster characteristics comparison\n",
    "cluster_stats = geo_df.groupby('dbscan_cluster').agg({\n",
    " 'sales_volume': 'mean',\n",
    " 'foot_traffic': 'mean',\n",
    " 'competition_nearby': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "cluster_stats = cluster_stats[cluster_stats['dbscan_cluster'] != -1] # Exclude noise\n",
    "\n",
    "for i, metric in enumerate(['sales_volume', 'foot_traffic', 'competition_nearby']):\n",
    " fig.add_trace(\n",
    " go.Bar(\n",
    " x=cluster_stats['dbscan_cluster'],\n",
    " y=cluster_stats[metric],\n",
    " name=metric.replace('_', ' ').title(),\n",
    " marker_color=colors[i % len(colors)],\n",
    " yaxis=f'y{4 if i == 0 else 4}',\n",
    " offsetgroup=i\n",
    " ),\n",
    " row=2, col=2\n",
    " )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    " height=800,\n",
    " title=\"DBSCAN Clustering Analysis Dashboard\",\n",
    " mapbox=dict(\n",
    " style=\"open-street-map\",\n",
    " center=dict(lat=40.7589, lon=-73.9851),\n",
    " zoom=10\n",
    " )\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Point Index\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Eps Parameter\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Cluster ID\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"4-NN Distance\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Min Samples\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Average Values\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Business insights and ROI\n",
    "print(f\"\\n DBSCAN BUSINESS INSIGHTS:\")\n",
    "\n",
    "total_revenue = 0\n",
    "for cluster in unique_clusters:\n",
    " if cluster != -1: # Skip noise\n",
    " cluster_data = geo_df[geo_df['dbscan_cluster'] == cluster]\n",
    " cluster_size = len(cluster_data)\n",
    " avg_sales = cluster_data['sales_volume'].mean()\n",
    " avg_traffic = cluster_data['foot_traffic'].mean()\n",
    " cluster_revenue = cluster_size * avg_sales\n",
    " total_revenue += cluster_revenue\n",
    "\n",
    " # Determine cluster type\n",
    " if avg_sales > geo_df['sales_volume'].median() and avg_traffic > geo_df['foot_traffic'].median():\n",
    " cluster_type = \"High-Performance Hub\"\n",
    " elif avg_sales > geo_df['sales_volume'].median():\n",
    " cluster_type = \"High-Revenue Zone\"\n",
    " elif avg_traffic > geo_df['foot_traffic'].median():\n",
    " cluster_type = \"High-Traffic Area\"\n",
    " else:\n",
    " cluster_type = \"Standard Zone\"\n",
    "\n",
    " print(f\"\\nCluster {cluster}: {cluster_type}\")\n",
    " print(f\"• Locations: {cluster_size}\")\n",
    " print(f\"• Avg sales: ${avg_sales:,.0f}\")\n",
    " print(f\"• Avg traffic: {avg_traffic:.0f} visitors\")\n",
    " print(f\"• Total revenue: ${cluster_revenue:,.0f}\")\n",
    "\n",
    "# ROI calculation\n",
    "location_optimization_improvement = 0.20 # 20% improvement from better location strategy\n",
    "marketing_efficiency_gain = 0.15 # 15% marketing efficiency from targeted clusters\n",
    "noise_point_investigation_cost = n_noise_final * 1000 # $1000 per noise point investigation\n",
    "\n",
    "roi_revenue_increase = total_revenue * location_optimization_improvement\n",
    "marketing_savings = total_revenue * 0.05 * marketing_efficiency_gain # 5% of revenue as marketing\n",
    "total_benefits = roi_revenue_increase + marketing_savings - noise_point_investigation_cost\n",
    "\n",
    "implementation_cost = 150_000\n",
    "\n",
    "print(f\"\\n DBSCAN CLUSTERING ROI:\")\n",
    "print(f\"• Total cluster revenue: ${total_revenue:,.0f}\")\n",
    "print(f\"• Location optimization gain: ${roi_revenue_increase:,.0f}\")\n",
    "print(f\"• Marketing efficiency savings: ${marketing_savings:,.0f}\")\n",
    "print(f\"• Investigation costs: ${noise_point_investigation_cost:,.0f}\")\n",
    "print(f\"• Net annual benefits: ${total_benefits:,.0f}\")\n",
    "print(f\"• Implementation cost: ${implementation_cost:,.0f}\")\n",
    "print(f\"• ROI: {(total_benefits - implementation_cost)/implementation_cost*100:.0f}%\")\n",
    "\n",
    "print(f\"\\n Cross-Reference Learning Path:\")\n",
    "print(f\"• Next: Tier4_PCA.ipynb (dimensionality reduction)\")\n",
    "print(f\"• Related: Tier6_AnomalyDetection.ipynb (noise detection)\")\n",
    "print(f\"• Advanced: Intermediate_Clustering.ipynb (ensemble methods)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}