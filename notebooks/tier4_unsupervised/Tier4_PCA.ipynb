{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 4: Principal Component Analysis (PCA)\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** 87895554-973d-42a5-ad90-e7c11fa3846a\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 4: Principal Component Analysis (PCA),\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** 87895554-973d-42a5-ad90-e7c11fa3846a\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8218cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 4: Principal Component Analysis - Libraries Loaded!\")\n",
    "print(\"=\" * 55)\n",
    "print(\"PCA Techniques:\")\n",
    "print(\"• Dimensionality reduction via eigendecomposition\")\n",
    "print(\"• Explained variance analysis and component selection\")\n",
    "print(\"• Feature transformation and data compression\")\n",
    "print(\"• Noise reduction and signal enhancement\")\n",
    "print(\"• High-dimensional data visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36122ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive high-dimensional datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Customer behavior dataset (high-dimensional)\n",
    "n_customers = 1500\n",
    "n_features = 50\n",
    "\n",
    "# Generate correlated customer features\n",
    "customer_segments = ['Premium', 'Standard', 'Budget']\n",
    "segment_data = []\n",
    "\n",
    "for i, segment in enumerate(customer_segments):\n",
    " n_segment = n_customers // 3\n",
    "\n",
    " # Base behavior patterns for each segment\n",
    " if segment == 'Premium':\n",
    " base_spending = np.random.normal(5000, 1000, n_segment)\n",
    " frequency_mult = np.random.normal(2.5, 0.5, n_segment)\n",
    " quality_pref = np.random.beta(8, 2, n_segment)\n",
    " elif segment == 'Standard':\n",
    " base_spending = np.random.normal(2000, 500, n_segment)\n",
    " frequency_mult = np.random.normal(1.5, 0.3, n_segment)\n",
    " quality_pref = np.random.beta(5, 5, n_segment)\n",
    " else: # Budget\n",
    " base_spending = np.random.normal(800, 300, n_segment)\n",
    " frequency_mult = np.random.normal(0.8, 0.2, n_segment)\n",
    " quality_pref = np.random.beta(2, 8, n_segment)\n",
    "\n",
    " # Create correlated features\n",
    " for j in range(n_segment):\n",
    " customer_features = []\n",
    "\n",
    " # Spending-related features (10 features)\n",
    " spending_features = base_spending[j] * np.random.normal(1, 0.2, 10) * frequency_mult[j]\n",
    " customer_features.extend(spending_features)\n",
    "\n",
    " # Engagement features (15 features)\n",
    " engagement_base = frequency_mult[j] * 100\n",
    " engagement_features = engagement_base * np.random.gamma(2, 0.5, 15)\n",
    " customer_features.extend(engagement_features)\n",
    "\n",
    " # Product preferences (15 features)\n",
    " pref_features = quality_pref[j] * np.random.normal(1, 0.3, 15)\n",
    " customer_features.extend(pref_features)\n",
    "\n",
    " # Demographic proxies (10 features)\n",
    " demo_features = np.random.normal(0, 1, 10)\n",
    " customer_features.extend(demo_features)\n",
    "\n",
    " segment_data.append({\n",
    " 'customer_id': f\"CUST_{i:03d}_{j:03d}\",\n",
    " 'segment': segment,\n",
    " 'segment_code': i,\n",
    " **{f'feature_{k:02d}': customer_features[k] for k in range(len(customer_features))}\n",
    " })\n",
    "\n",
    "customer_df = pd.DataFrame(segment_data)\n",
    "\n",
    "# Feature matrix for PCA\n",
    "feature_columns = [col for col in customer_df.columns if col.startswith('feature_')]\n",
    "X_customer = customer_df[feature_columns].values\n",
    "y_customer = customer_df['segment_code'].values\n",
    "\n",
    "print(\" PCA Dataset Created:\")\n",
    "print(f\"Customers: {len(customer_df)}\")\n",
    "print(f\"Features: {len(feature_columns)}\")\n",
    "print(f\"Segments: {customer_df['segment'].value_counts().to_dict()}\")\n",
    "print(f\"Feature range: {X_customer.min():.2f} to {X_customer.max():.2f}\")\n",
    "\n",
    "# 2. Load digits dataset for image PCA demonstration\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(f\"\\nDigits dataset: {X_digits.shape[0]} samples, {X_digits.shape[1]} features\")\n",
    "print(f\"Represents 8x8 pixel images of handwritten digits 0-9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f1eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PCA ANALYSIS AND COMPONENT SELECTION\n",
    "print(\" 1. PCA ANALYSIS AND COMPONENT SELECTION\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Standardize customer data\n",
    "scaler_customer = StandardScaler()\n",
    "X_customer_scaled = scaler_customer.fit_transform(X_customer)\n",
    "\n",
    "# Fit PCA with all components\n",
    "pca_full = PCA()\n",
    "X_customer_pca = pca_full.fit_transform(X_customer_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "explained_variance_ratio = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Find optimal number of components (95% variance explained)\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "n_components_80 = np.argmax(cumulative_variance >= 0.80) + 1\n",
    "\n",
    "print(f\"Original dimensions: {X_customer.shape[1]}\")\n",
    "print(f\"Components for 80% variance: {n_components_80}\")\n",
    "print(f\"Components for 90% variance: {n_components_90}\")\n",
    "print(f\"Components for 95% variance: {n_components_95}\")\n",
    "\n",
    "# Create optimal PCA models\n",
    "pca_optimal = PCA(n_components=n_components_95)\n",
    "X_customer_reduced = pca_optimal.fit_transform(X_customer_scaled)\n",
    "\n",
    "print(f\"Dimensionality reduction: {X_customer.shape[1]} → {X_customer_reduced.shape[1]}\")\n",
    "print(f\"Compression ratio: {X_customer.shape[1] / X_customer_reduced.shape[1]:.1f}x\")\n",
    "\n",
    "# Analyze component loadings\n",
    "components_df = pd.DataFrame(\n",
    " pca_optimal.components_[:5].T, # First 5 components\n",
    " columns=[f'PC{i+1}' for i in range(5)],\n",
    " index=[f'Feature_{i:02d}' for i in range(len(feature_columns))]\n",
    ")\n",
    "\n",
    "print(f\"\\nTop feature loadings for first 3 components:\")\n",
    "for i in range(3):\n",
    " pc_name = f'PC{i+1}'\n",
    " top_features = components_df[pc_name].abs().nlargest(5)\n",
    " print(f\"\\n{pc_name} (explains {explained_variance_ratio[i]*100:.1f}% variance):\")\n",
    " for feature, loading in top_features.items():\n",
    " print(f\" {feature}: {loading:.3f}\")\n",
    "\n",
    "# Digits PCA for comparison\n",
    "scaler_digits = StandardScaler()\n",
    "X_digits_scaled = scaler_digits.fit_transform(X_digits)\n",
    "\n",
    "pca_digits = PCA(n_components=0.95) # 95% variance\n",
    "X_digits_reduced = pca_digits.fit_transform(X_digits_scaled)\n",
    "\n",
    "print(f\"\\nDigits PCA: {X_digits.shape[1]} → {X_digits_reduced.shape[1]} dimensions\")\n",
    "print(f\"Digits compression ratio: {X_digits.shape[1] / X_digits_reduced.shape[1]:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e07030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. INTERACTIVE PCA VISUALIZATIONS\n",
    "print(\" 2. INTERACTIVE PCA VISUALIZATIONS\")\n",
    "print(\"=\" * 36)\n",
    "\n",
    "# Create comprehensive PCA dashboard\n",
    "fig = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=[\n",
    " 'Explained Variance by Component',\n",
    " 'Customer Segments in PC Space (2D)',\n",
    " 'Component Loadings Heatmap',\n",
    " 'Customer Segments in PC Space (3D)',\n",
    " 'Digits Reconstruction Comparison',\n",
    " 'PCA Performance Metrics'\n",
    " ],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"type\": \"scatter3d\"}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# 1. Explained variance plot\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=np.arange(1, len(explained_variance_ratio) + 1),\n",
    " y=explained_variance_ratio,\n",
    " mode='lines+markers',\n",
    " name='Individual Variance',\n",
    " line=dict(color='blue', width=2),\n",
    " marker=dict(size=6)\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=np.arange(1, len(cumulative_variance) + 1),\n",
    " y=cumulative_variance,\n",
    " mode='lines+markers',\n",
    " name='Cumulative Variance',\n",
    " line=dict(color='red', width=2),\n",
    " marker=dict(size=6)\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Add threshold lines\n",
    "fig.add_hline(y=0.95, line=dict(color='green', dash='dash'),\n",
    " annotation_text=\"95% threshold\", row=1, col=1)\n",
    "fig.add_vline(x=n_components_95, line=dict(color='green', dash='dash'),\n",
    " annotation_text=f\"n={n_components_95}\", row=1, col=1)\n",
    "\n",
    "# 2. Customer segments in 2D PC space\n",
    "colors = ['red', 'blue', 'green']\n",
    "segments = ['Premium', 'Standard', 'Budget']\n",
    "\n",
    "for i, (segment, color) in enumerate(zip(segments, colors)):\n",
    " mask = customer_df['segment'] == segment\n",
    " fig.add_trace(\n",
    " go.Scatter(\n",
    " x=X_customer_reduced[mask, 0],\n",
    " y=X_customer_reduced[mask, 1],\n",
    " mode='markers',\n",
    " name=f'{segment} Customers',\n",
    " marker=dict(color=color, size=8, opacity=0.7),\n",
    " text=customer_df[mask]['customer_id'],\n",
    " hovertemplate=f'{segment}<br>PC1: %{{x:.2f}}<br>PC2: %{{y:.2f}}<extra></extra>'\n",
    " ),\n",
    " row=1, col=2\n",
    " )\n",
    "\n",
    "# 3. Component loadings heatmap\n",
    "fig.add_trace(\n",
    " go.Heatmap(\n",
    " z=components_df.T.values,\n",
    " x=components_df.index,\n",
    " y=components_df.columns,\n",
    " colorscale='RdBu',\n",
    " zmid=0,\n",
    " showscale=True,\n",
    " hovertemplate='Feature: %{x}<br>Component: %{y}<br>Loading: %{z:.3f}<extra></extra>'\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Customer segments in 3D PC space\n",
    "for i, (segment, color) in enumerate(zip(segments, colors)):\n",
    " mask = customer_df['segment'] == segment\n",
    " fig.add_trace(\n",
    " go.Scatter3d(\n",
    " x=X_customer_reduced[mask, 0],\n",
    " y=X_customer_reduced[mask, 1],\n",
    " z=X_customer_reduced[mask, 2],\n",
    " mode='markers',\n",
    " name=f'{segment} 3D',\n",
    " marker=dict(color=color, size=5, opacity=0.7),\n",
    " text=customer_df[mask]['customer_id'],\n",
    " hovertemplate=f'{segment}<br>PC1: %{{x:.2f}}<br>PC2: %{{y:.2f}}<br>PC3: %{{z:.2f}}<extra></extra>'\n",
    " ),\n",
    " row=2, col=2\n",
    " )\n",
    "\n",
    "# 5. Digits reconstruction comparison\n",
    "# Select a few digit samples for reconstruction\n",
    "sample_indices = [0, 100, 200, 300, 400]\n",
    "original_images = X_digits[sample_indices].reshape(-1, 8, 8)\n",
    "\n",
    "# Reconstruct from PCA\n",
    "X_digits_reconstructed = pca_digits.inverse_transform(X_digits_reduced)\n",
    "reconstructed_images = X_digits_reconstructed[sample_indices].reshape(-1, 8, 8)\n",
    "\n",
    "# Show reconstruction quality\n",
    "reconstruction_errors = np.mean((X_digits[sample_indices] - X_digits_reconstructed[sample_indices])**2, axis=1)\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=[f'Digit {i}' for i in sample_indices],\n",
    " y=reconstruction_errors,\n",
    " name='Reconstruction Error',\n",
    " marker_color='orange'\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. PCA performance metrics comparison\n",
    "dimensions = [5, 10, 20, 30, n_components_95]\n",
    "performance_metrics = []\n",
    "\n",
    "for n_comp in dimensions:\n",
    " pca_temp = PCA(n_components=n_comp)\n",
    " X_temp = pca_temp.fit_transform(X_customer_scaled)\n",
    "\n",
    " # Classification performance with reduced dimensions\n",
    " X_train, X_test, y_train, y_test = train_test_split(X_temp, y_customer, test_size=0.3, random_state=42)\n",
    " clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    " clf.fit(X_train, y_train)\n",
    " accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    " variance_explained = np.sum(pca_temp.explained_variance_ratio_)\n",
    "\n",
    " performance_metrics.append({\n",
    " 'components': n_comp,\n",
    " 'accuracy': accuracy,\n",
    " 'variance_explained': variance_explained\n",
    " })\n",
    "\n",
    "perf_df = pd.DataFrame(performance_metrics)\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=perf_df['components'],\n",
    " y=perf_df['accuracy'],\n",
    " mode='lines+markers',\n",
    " name='Classification Accuracy',\n",
    " line=dict(color='purple', width=3),\n",
    " marker=dict(size=8),\n",
    " yaxis='y6'\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    " height=1200,\n",
    " title=\"Principal Component Analysis (PCA) Dashboard\",\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Component Number\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"First Principal Component\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Features\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Digit Samples\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"Number of Components\", row=3, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Explained Variance Ratio\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Second Principal Component\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Components\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Reconstruction Error\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Classification Accuracy\", row=3, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Business insights\n",
    "print(f\"\\n PCA BUSINESS INSIGHTS:\")\n",
    "\n",
    "# Customer segment analysis in PC space\n",
    "for i, segment in enumerate(segments):\n",
    " segment_mask = customer_df['segment'] == segment\n",
    " segment_pca = X_customer_reduced[segment_mask]\n",
    "\n",
    " pc1_mean = np.mean(segment_pca[:, 0])\n",
    " pc2_mean = np.mean(segment_pca[:, 1])\n",
    "\n",
    " print(f\"\\n{segment} Customers:\")\n",
    " print(f\"• PC1 (primary behavior): {pc1_mean:.2f}\")\n",
    " print(f\"• PC2 (secondary behavior): {pc2_mean:.2f}\")\n",
    " print(f\"• Cluster tightness: {np.std(segment_pca[:, 0]):.2f}\")\n",
    "\n",
    "# Storage and computational benefits\n",
    "original_storage = X_customer.shape[0] * X_customer.shape[1] * 8 # 8 bytes per float\n",
    "reduced_storage = X_customer_reduced.shape[0] * X_customer_reduced.shape[1] * 8\n",
    "storage_savings = (original_storage - reduced_storage) / original_storage\n",
    "\n",
    "print(f\"\\n DATA COMPRESSION BENEFITS:\")\n",
    "print(f\"• Original storage: {original_storage / 1024:.1f} KB\")\n",
    "print(f\"• Reduced storage: {reduced_storage / 1024:.1f} KB\")\n",
    "print(f\"• Storage savings: {storage_savings*100:.1f}%\")\n",
    "print(f\"• Information retained: {np.sum(pca_optimal.explained_variance_ratio_)*100:.1f}%\")\n",
    "\n",
    "# ROI calculation\n",
    "data_storage_cost_per_gb = 100 # $100 per GB per year\n",
    "processing_cost_reduction = 0.60 # 60% reduction in processing time\n",
    "model_accuracy_maintained = perf_df[perf_df['components'] == n_components_95]['accuracy'].iloc[0]\n",
    "\n",
    "annual_data_volume_gb = 1000 # 1TB of customer data\n",
    "storage_cost_savings = annual_data_volume_gb * data_storage_cost_per_gb * storage_savings\n",
    "processing_cost_savings = 200_000 * processing_cost_reduction # $200k annual processing costs\n",
    "\n",
    "total_benefits = storage_cost_savings + processing_cost_savings\n",
    "implementation_cost = 80_000\n",
    "\n",
    "print(f\"\\n PCA IMPLEMENTATION ROI:\")\n",
    "print(f\"• Storage cost savings: ${storage_cost_savings:,.0f}/year\")\n",
    "print(f\"• Processing cost savings: ${processing_cost_savings:,.0f}/year\")\n",
    "print(f\"• Model accuracy maintained: {model_accuracy_maintained*100:.1f}%\")\n",
    "print(f\"• Total annual benefits: ${total_benefits:,.0f}\")\n",
    "print(f\"• Implementation cost: ${implementation_cost:,.0f}\")\n",
    "print(f\"• ROI: {(total_benefits - implementation_cost)/implementation_cost*100:.0f}%\")\n",
    "print(f\"• Payback period: {implementation_cost/total_benefits*12:.1f} months\")\n",
    "\n",
    "print(f\"\\n Cross-Reference Learning Path:\")\n",
    "print(f\"• Foundation: Tier3_Statistics.ipynb (variance concepts)\")\n",
    "print(f\"• Application: Tier4_Clustering.ipynb (dimensionality preprocessing)\")\n",
    "print(f\"• Advanced: Tier5_NeuralNetworks.ipynb (autoencoder comparison)\")\n",
    "print(f\"• Next: Tier6_AdvancedML.ipynb (curse of dimensionality)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}