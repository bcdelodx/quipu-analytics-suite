{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 2: k-Nearest Neighbors (k-NN)\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** f14d64a5-c6b6-4cca-b462-101d78787dab\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 2: k-Nearest Neighbors (k-NN),\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** f14d64a5-c6b6-4cca-b462-101d78787dab\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed6a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Essential Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, mean_absolute_error\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "\n",
    "# Distance metrics\n",
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 2: k-Nearest Neighbors (k-NN) - Libraries Loaded Successfully!\")\n",
    "print(\"=\" * 75)\n",
    "print(\"Available k-NN Techniques:\")\n",
    "print(\"• k-NN Classification - Instance-based categorical prediction\")\n",
    "print(\"• k-NN Regression - Local averaging for continuous targets\")\n",
    "print(\"• Distance Metrics - Euclidean, Manhattan, Minkowski analysis\")\n",
    "print(\"• Optimal k Selection - Cross-validation and elbow method\")\n",
    "print(\"• Feature Scaling - Standardization and normalization impact\")\n",
    "print(\"• Neighborhood Analysis - Local pattern and density exploration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Comprehensive Datasets for k-NN Analysis\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_knn_datasets():\n",
    " \"\"\"Generate datasets optimized for k-NN analysis with local patterns\"\"\"\n",
    "\n",
    " # 1. REGRESSION DATASET - House Price Prediction with Spatial Components\n",
    " n_samples = 1000\n",
    "\n",
    " # Geographic coordinates (normalized to 0-100 range)\n",
    " latitude = np.random.uniform(40.0, 45.0, n_samples)\n",
    " longitude = np.random.uniform(-75.0, -70.0, n_samples)\n",
    "\n",
    " # Normalize coordinates\n",
    " lat_norm = (latitude - latitude.min()) / (latitude.max() - latitude.min()) * 100\n",
    " lon_norm = (longitude - longitude.min()) / (longitude.max() - longitude.min()) * 100\n",
    "\n",
    " # House characteristics\n",
    " house_size = np.random.gamma(shape=2, scale=1200, size=n_samples) + 800\n",
    " bedrooms = np.random.poisson(lam=3, size=n_samples) + 1\n",
    " bathrooms = np.random.poisson(lam=2, size=n_samples) + 1\n",
    " age_years = np.random.exponential(scale=15, size=n_samples) + 1\n",
    "\n",
    " # Neighborhood quality (spatially correlated)\n",
    " neighborhood_centers = [(20, 20), (50, 80), (80, 30), (30, 70)]\n",
    " neighborhood_quality = np.zeros(n_samples)\n",
    "\n",
    " for i in range(n_samples):\n",
    " # Distance to nearest high-quality neighborhood center\n",
    " distances = [np.sqrt((lat_norm[i] - center[0])**2 + (lon_norm[i] - center[1])**2)\n",
    " for center in neighborhood_centers]\n",
    " min_distance = min(distances)\n",
    "\n",
    " # Quality decreases with distance (local similarity)\n",
    " neighborhood_quality[i] = max(0, 10 - min_distance/5) + np.random.normal(0, 1)\n",
    " neighborhood_quality[i] = np.clip(neighborhood_quality[i], 1, 10)\n",
    "\n",
    " # School district ratings (spatially clustered)\n",
    " school_rating = np.zeros(n_samples)\n",
    " school_centers = [(40, 40), (70, 70)]\n",
    "\n",
    " for i in range(n_samples):\n",
    " distances = [np.sqrt((lat_norm[i] - center[0])**2 + (lon_norm[i] - center[1])**2)\n",
    " for center in school_centers]\n",
    " min_distance = min(distances)\n",
    "\n",
    " school_rating[i] = max(1, 10 - min_distance/8) + np.random.normal(0, 0.5)\n",
    " school_rating[i] = np.clip(school_rating[i], 1, 10)\n",
    "\n",
    " # Generate prices with local effects (perfect for k-NN)\n",
    " price = (house_size * 150 +\n",
    " bedrooms * 15000 +\n",
    " bathrooms * 10000 -\n",
    " age_years * 2000 +\n",
    " neighborhood_quality * 25000 +\n",
    " school_rating * 20000 +\n",
    " np.random.normal(0, 30000, n_samples))\n",
    "\n",
    " price = np.maximum(price, 100000) # Minimum price\n",
    "\n",
    " # Create regression DataFrame\n",
    " regression_df = pd.DataFrame({\n",
    " 'latitude': lat_norm,\n",
    " 'longitude': lon_norm,\n",
    " 'house_size': house_size,\n",
    " 'bedrooms': bedrooms,\n",
    " 'bathrooms': bathrooms,\n",
    " 'age_years': age_years,\n",
    " 'neighborhood_quality': neighborhood_quality,\n",
    " 'school_rating': school_rating,\n",
    " 'price': price\n",
    " })\n",
    "\n",
    " # 2. CLASSIFICATION DATASET - Customer Segment Prediction\n",
    " # Generate customer features with local clustering patterns\n",
    " customer_age = np.random.normal(45, 15, n_samples)\n",
    " customer_age = np.clip(customer_age, 18, 80)\n",
    "\n",
    " annual_income = np.random.lognormal(mean=10.8, sigma=0.6, size=n_samples)\n",
    " annual_income = np.clip(annual_income, 30000, 200000)\n",
    "\n",
    " spending_score = np.random.beta(a=2, b=2, size=n_samples) * 100\n",
    "\n",
    " # Create clustered segments based on age and income\n",
    " segments = np.zeros(n_samples, dtype=int)\n",
    "\n",
    " for i in range(n_samples):\n",
    " # Young, high income, high spending\n",
    " if customer_age[i] < 35 and annual_income[i] > 70000 and spending_score[i] > 60:\n",
    " segments[i] = 0 # \"Premium Young\"\n",
    " # Middle age, moderate income, family-oriented\n",
    " elif 35 <= customer_age[i] <= 55 and 50000 <= annual_income[i] <= 90000:\n",
    " segments[i] = 1 # \"Family Focused\"\n",
    " # Older, high income, conservative spending\n",
    " elif customer_age[i] > 55 and annual_income[i] > 60000 and spending_score[i] < 50:\n",
    " segments[i] = 2 # \"Conservative Savers\"\n",
    " # Budget conscious across all ages\n",
    " else:\n",
    " segments[i] = 3 # \"Budget Conscious\"\n",
    "\n",
    " # Add some noise to make it more realistic\n",
    " flip_probability = 0.1\n",
    " noise_mask = np.random.random(n_samples) < flip_probability\n",
    " segments[noise_mask] = np.random.randint(0, 4, size=noise_mask.sum())\n",
    "\n",
    " # Additional features\n",
    " years_customer = np.random.exponential(scale=3, size=n_samples) + 0.5\n",
    " monthly_purchases = np.random.poisson(lam=8, size=n_samples) + 1\n",
    "\n",
    " # Purchase categories (related to segments)\n",
    " electronics_purchases = np.zeros(n_samples)\n",
    " clothing_purchases = np.zeros(n_samples)\n",
    " grocery_purchases = np.zeros(n_samples)\n",
    "\n",
    " for i in range(n_samples):\n",
    " if segments[i] == 0: # Premium Young\n",
    " electronics_purchases[i] = np.random.poisson(5) + 2\n",
    " clothing_purchases[i] = np.random.poisson(4) + 1\n",
    " grocery_purchases[i] = np.random.poisson(3) + 1\n",
    " elif segments[i] == 1: # Family Focused\n",
    " electronics_purchases[i] = np.random.poisson(2) + 1\n",
    " clothing_purchases[i] = np.random.poisson(6) + 2\n",
    " grocery_purchases[i] = np.random.poisson(8) + 3\n",
    " elif segments[i] == 2: # Conservative Savers\n",
    " electronics_purchases[i] = np.random.poisson(1) + 1\n",
    " clothing_purchases[i] = np.random.poisson(2) + 1\n",
    " grocery_purchases[i] = np.random.poisson(4) + 2\n",
    " else: # Budget Conscious\n",
    " electronics_purchases[i] = np.random.poisson(1) + 1\n",
    " clothing_purchases[i] = np.random.poisson(3) + 1\n",
    " grocery_purchases[i] = np.random.poisson(6) + 2\n",
    "\n",
    " # Create classification DataFrame\n",
    " classification_df = pd.DataFrame({\n",
    " 'customer_age': customer_age,\n",
    " 'annual_income': annual_income,\n",
    " 'spending_score': spending_score,\n",
    " 'years_customer': years_customer,\n",
    " 'monthly_purchases': monthly_purchases,\n",
    " 'electronics_purchases': electronics_purchases,\n",
    " 'clothing_purchases': clothing_purchases,\n",
    " 'grocery_purchases': grocery_purchases,\n",
    " 'segment': segments\n",
    " })\n",
    "\n",
    " return regression_df, classification_df\n",
    "\n",
    "# Generate datasets\n",
    "print(\" Generating k-NN optimized datasets...\")\n",
    "regression_df, classification_df = generate_knn_datasets()\n",
    "\n",
    "print(f\"Regression Dataset Shape: {regression_df.shape}\")\n",
    "print(f\"Classification Dataset Shape: {classification_df.shape}\")\n",
    "\n",
    "print(\"\\nRegression Dataset (House Price Prediction):\")\n",
    "print(regression_df.head())\n",
    "print(\"\\nRegression Target Statistics:\")\n",
    "print(regression_df['price'].describe())\n",
    "\n",
    "print(\"\\nClassification Dataset (Customer Segmentation):\")\n",
    "print(classification_df.head())\n",
    "print(\"\\nSegment Distribution:\")\n",
    "segment_names = ['Premium Young', 'Family Focused', 'Conservative Savers', 'Budget Conscious']\n",
    "segment_counts = classification_df['segment'].value_counts().sort_index()\n",
    "for i, count in enumerate(segment_counts):\n",
    " print(f\"• {segment_names[i]}: {count} ({count/len(classification_df):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. k-NN REGRESSION ANALYSIS\n",
    "print(\" 1. k-NN REGRESSION ANALYSIS\")\n",
    "print(\"=\" * 33)\n",
    "\n",
    "# Prepare regression data\n",
    "reg_features = ['latitude', 'longitude', 'house_size', 'bedrooms', 'bathrooms',\n",
    " 'age_years', 'neighborhood_quality', 'school_rating']\n",
    "X_reg = regression_df[reg_features]\n",
    "y_reg = regression_df['price']\n",
    "\n",
    "# Split data\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    " X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_reg_train.shape}\")\n",
    "print(f\"Test set: {X_reg_test.shape}\")\n",
    "\n",
    "# Feature scaling comparison\n",
    "scalers = {\n",
    " 'StandardScaler': StandardScaler(),\n",
    " 'MinMaxScaler': MinMaxScaler(),\n",
    " 'No Scaling': None\n",
    "}\n",
    "\n",
    "scaling_results = {}\n",
    "\n",
    "for scaler_name, scaler in scalers.items():\n",
    " if scaler is not None:\n",
    " X_reg_train_scaled = scaler.fit_transform(X_reg_train)\n",
    " X_reg_test_scaled = scaler.transform(X_reg_test)\n",
    " else:\n",
    " X_reg_train_scaled = X_reg_train.values\n",
    " X_reg_test_scaled = X_reg_test.values\n",
    "\n",
    " # Fit k-NN with default k=5\n",
    " knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
    " knn_reg.fit(X_reg_train_scaled, y_reg_train)\n",
    "\n",
    " # Predictions\n",
    " y_reg_pred = knn_reg.predict(X_reg_test_scaled)\n",
    "\n",
    " # Metrics\n",
    " mse = mean_squared_error(y_reg_test, y_reg_pred)\n",
    " r2 = r2_score(y_reg_test, y_reg_pred)\n",
    " mae = mean_absolute_error(y_reg_test, y_reg_pred)\n",
    "\n",
    " scaling_results[scaler_name] = {\n",
    " 'MSE': mse,\n",
    " 'R2': r2,\n",
    " 'MAE': mae,\n",
    " 'RMSE': np.sqrt(mse)\n",
    " }\n",
    "\n",
    "print(\" Feature Scaling Impact on k-NN Regression:\")\n",
    "scaling_df = pd.DataFrame(scaling_results).T\n",
    "print(scaling_df.round(4))\n",
    "\n",
    "# Choose best scaling method\n",
    "best_scaler_name = scaling_df['R2'].idxmax()\n",
    "best_scaler = scalers[best_scaler_name]\n",
    "\n",
    "print(f\"\\n Best scaling method: {best_scaler_name}\")\n",
    "\n",
    "# Scale data with best scaler\n",
    "if best_scaler is not None:\n",
    " X_reg_train_final = best_scaler.fit_transform(X_reg_train)\n",
    " X_reg_test_final = best_scaler.transform(X_reg_test)\n",
    "else:\n",
    " X_reg_train_final = X_reg_train.values\n",
    " X_reg_test_final = X_reg_test.values\n",
    "\n",
    "# Visualize scaling impact\n",
    "fig_scaling = go.Figure()\n",
    "\n",
    "metrics = ['R2', 'MSE', 'MAE']\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    " # Normalize metrics for comparison (R2 is already 0-1, others need normalization)\n",
    " if metric == 'R2':\n",
    " values = scaling_df[metric]\n",
    " else:\n",
    " values = 1 / (1 + scaling_df[metric] / scaling_df[metric].max()) # Inverse normalization\n",
    "\n",
    " fig_scaling.add_trace(\n",
    " go.Bar(\n",
    " x=scaling_df.index,\n",
    " y=values,\n",
    " name=metric,\n",
    " marker_color=colors[i],\n",
    " opacity=0.7,\n",
    " yaxis=f'y{i+1}' if i > 0 else 'y',\n",
    " offsetgroup=i\n",
    " )\n",
    " )\n",
    "\n",
    "fig_scaling.update_layout(\n",
    " title=\"Feature Scaling Impact on k-NN Performance\",\n",
    " xaxis_title=\"Scaling Method\",\n",
    " barmode='group',\n",
    " height=500\n",
    ")\n",
    "fig_scaling.show()\n",
    "\n",
    "# k-value optimization\n",
    "print(f\"\\n Optimal k Selection:\")\n",
    "\n",
    "k_values = range(1, 31)\n",
    "k_scores_cv = []\n",
    "k_scores_train = []\n",
    "k_scores_test = []\n",
    "\n",
    "for k in k_values:\n",
    " knn_reg = KNeighborsRegressor(n_neighbors=k)\n",
    "\n",
    " # Cross-validation score\n",
    " cv_scores = cross_val_score(knn_reg, X_reg_train_final, y_reg_train, cv=5, scoring='r2')\n",
    " k_scores_cv.append(cv_scores.mean())\n",
    "\n",
    " # Training and test scores\n",
    " knn_reg.fit(X_reg_train_final, y_reg_train)\n",
    " k_scores_train.append(knn_reg.score(X_reg_train_final, y_reg_train))\n",
    " k_scores_test.append(knn_reg.score(X_reg_test_final, y_reg_test))\n",
    "\n",
    "# Find optimal k\n",
    "optimal_k_cv = k_values[np.argmax(k_scores_cv)]\n",
    "optimal_k_test = k_values[np.argmax(k_scores_test)]\n",
    "\n",
    "print(f\"• Optimal k (CV): {optimal_k_cv} (R² = {max(k_scores_cv):.4f})\")\n",
    "print(f\"• Optimal k (Test): {optimal_k_test} (R² = {max(k_scores_test):.4f})\")\n",
    "\n",
    "# Plot k optimization\n",
    "fig_k_opt = go.Figure()\n",
    "\n",
    "fig_k_opt.add_trace(\n",
    " go.Scatter(\n",
    " x=list(k_values),\n",
    " y=k_scores_train,\n",
    " mode='lines+markers',\n",
    " name='Training R²',\n",
    " line=dict(color='blue'),\n",
    " hovertemplate=\"k: %{x}<br>Training R²: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_k_opt.add_trace(\n",
    " go.Scatter(\n",
    " x=list(k_values),\n",
    " y=k_scores_cv,\n",
    " mode='lines+markers',\n",
    " name='CV R²',\n",
    " line=dict(color='green'),\n",
    " hovertemplate=\"k: %{x}<br>CV R²: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_k_opt.add_trace(\n",
    " go.Scatter(\n",
    " x=list(k_values),\n",
    " y=k_scores_test,\n",
    " mode='lines+markers',\n",
    " name='Test R²',\n",
    " line=dict(color='red'),\n",
    " hovertemplate=\"k: %{x}<br>Test R²: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "# Mark optimal k\n",
    "fig_k_opt.add_vline(\n",
    " x=optimal_k_cv,\n",
    " line_dash=\"dash\",\n",
    " line_color=\"green\",\n",
    " annotation_text=f\"Optimal k (CV) = {optimal_k_cv}\"\n",
    ")\n",
    "\n",
    "fig_k_opt.update_layout(\n",
    " title=\"k-NN Regression: Optimal k Selection\",\n",
    " xaxis_title=\"Number of Neighbors (k)\",\n",
    " yaxis_title=\"R² Score\",\n",
    " height=500\n",
    ")\n",
    "fig_k_opt.show()\n",
    "\n",
    "# Fit final model with optimal k\n",
    "knn_reg_optimal = KNeighborsRegressor(n_neighbors=optimal_k_cv)\n",
    "knn_reg_optimal.fit(X_reg_train_final, y_reg_train)\n",
    "\n",
    "# Final predictions\n",
    "y_reg_pred_optimal = knn_reg_optimal.predict(X_reg_test_final)\n",
    "\n",
    "# Final metrics\n",
    "final_mse = mean_squared_error(y_reg_test, y_reg_pred_optimal)\n",
    "final_r2 = r2_score(y_reg_test, y_reg_pred_optimal)\n",
    "final_mae = mean_absolute_error(y_reg_test, y_reg_pred_optimal)\n",
    "\n",
    "print(f\"\\n Final k-NN Regression Performance:\")\n",
    "print(f\"• k = {optimal_k_cv}\")\n",
    "print(f\"• Test R²: {final_r2:.4f}\")\n",
    "print(f\"• Test RMSE: ${np.sqrt(final_mse):,.0f}\")\n",
    "print(f\"• Test MAE: ${final_mae:,.0f}\")\n",
    "\n",
    "# Actual vs Predicted plot\n",
    "fig_pred_reg = go.Figure()\n",
    "\n",
    "fig_pred_reg.add_trace(\n",
    " go.Scatter(\n",
    " x=y_reg_test,\n",
    " y=y_reg_pred_optimal,\n",
    " mode='markers',\n",
    " marker=dict(color='blue', opacity=0.6),\n",
    " name='Predictions',\n",
    " hovertemplate=\"Actual: $%{x:,.0f}<br>Predicted: $%{y:,.0f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "# Perfect prediction line\n",
    "min_price = min(y_reg_test.min(), y_reg_pred_optimal.min())\n",
    "max_price = max(y_reg_test.max(), y_reg_pred_optimal.max())\n",
    "\n",
    "fig_pred_reg.add_trace(\n",
    " go.Scatter(\n",
    " x=[min_price, max_price],\n",
    " y=[min_price, max_price],\n",
    " mode='lines',\n",
    " line=dict(color='red', dash='dash'),\n",
    " name='Perfect Prediction',\n",
    " hovertemplate=\"Perfect Line<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_pred_reg.update_layout(\n",
    " title=f\"k-NN Regression: Actual vs Predicted (k={optimal_k_cv})\",\n",
    " xaxis_title=\"Actual Price ($)\",\n",
    " yaxis_title=\"Predicted Price ($)\",\n",
    " height=500\n",
    ")\n",
    "fig_pred_reg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae98de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. k-NN CLASSIFICATION ANALYSIS\n",
    "print(\" 2. k-NN CLASSIFICATION ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Prepare classification data\n",
    "class_features = ['customer_age', 'annual_income', 'spending_score', 'years_customer',\n",
    " 'monthly_purchases', 'electronics_purchases', 'clothing_purchases', 'grocery_purchases']\n",
    "X_class = classification_df[class_features]\n",
    "y_class = classification_df['segment']\n",
    "\n",
    "# Split data\n",
    "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(\n",
    " X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_class_train.shape}\")\n",
    "print(f\"Test set: {X_class_test.shape}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution (training):\")\n",
    "train_distribution = y_class_train.value_counts().sort_index()\n",
    "segment_names = ['Premium Young', 'Family Focused', 'Conservative Savers', 'Budget Conscious']\n",
    "for i, count in enumerate(train_distribution):\n",
    " print(f\"• {segment_names[i]}: {count} ({count/len(y_class_train):.1%})\")\n",
    "\n",
    "# Scale features for classification\n",
    "scaler_class = StandardScaler()\n",
    "X_class_train_scaled = scaler_class.fit_transform(X_class_train)\n",
    "X_class_test_scaled = scaler_class.transform(X_class_test)\n",
    "\n",
    "# k-value optimization for classification\n",
    "print(f\"\\n Optimal k Selection for Classification:\")\n",
    "\n",
    "k_values = range(1, 31)\n",
    "k_accuracy_cv = []\n",
    "k_accuracy_train = []\n",
    "k_accuracy_test = []\n",
    "\n",
    "for k in k_values:\n",
    " knn_class = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    " # Cross-validation score\n",
    " cv_scores = cross_val_score(knn_class, X_class_train_scaled, y_class_train, cv=5, scoring='accuracy')\n",
    " k_accuracy_cv.append(cv_scores.mean())\n",
    "\n",
    " # Training and test scores\n",
    " knn_class.fit(X_class_train_scaled, y_class_train)\n",
    " k_accuracy_train.append(knn_class.score(X_class_train_scaled, y_class_train))\n",
    " k_accuracy_test.append(knn_class.score(X_class_test_scaled, y_class_test))\n",
    "\n",
    "# Find optimal k\n",
    "optimal_k_class_cv = k_values[np.argmax(k_accuracy_cv)]\n",
    "optimal_k_class_test = k_values[np.argmax(k_accuracy_test)]\n",
    "\n",
    "print(f\"• Optimal k (CV): {optimal_k_class_cv} (Accuracy = {max(k_accuracy_cv):.4f})\")\n",
    "print(f\"• Optimal k (Test): {optimal_k_class_test} (Accuracy = {max(k_accuracy_test):.4f})\")\n",
    "\n",
    "# Plot k optimization for classification\n",
    "fig_k_class = go.Figure()\n",
    "\n",
    "fig_k_class.add_trace(\n",
    " go.Scatter(\n",
    " x=list(k_values),\n",
    " y=k_accuracy_train,\n",
    " mode='lines+markers',\n",
    " name='Training Accuracy',\n",
    " line=dict(color='blue'),\n",
    " hovertemplate=\"k: %{x}<br>Training Accuracy: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_k_class.add_trace(\n",
    " go.Scatter(\n",
    " x=list(k_values),\n",
    " y=k_accuracy_cv,\n",
    " mode='lines+markers',\n",
    " name='CV Accuracy',\n",
    " line=dict(color='green'),\n",
    " hovertemplate=\"k: %{x}<br>CV Accuracy: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_k_class.add_trace(\n",
    " go.Scatter(\n",
    " x=list(k_values),\n",
    " y=k_accuracy_test,\n",
    " mode='lines+markers',\n",
    " name='Test Accuracy',\n",
    " line=dict(color='red'),\n",
    " hovertemplate=\"k: %{x}<br>Test Accuracy: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "# Mark optimal k\n",
    "fig_k_class.add_vline(\n",
    " x=optimal_k_class_cv,\n",
    " line_dash=\"dash\",\n",
    " line_color=\"green\",\n",
    " annotation_text=f\"Optimal k (CV) = {optimal_k_class_cv}\"\n",
    ")\n",
    "\n",
    "fig_k_class.update_layout(\n",
    " title=\"k-NN Classification: Optimal k Selection\",\n",
    " xaxis_title=\"Number of Neighbors (k)\",\n",
    " yaxis_title=\"Accuracy Score\",\n",
    " height=500\n",
    ")\n",
    "fig_k_class.show()\n",
    "\n",
    "# Fit final classification model\n",
    "knn_class_optimal = KNeighborsClassifier(n_neighbors=optimal_k_class_cv)\n",
    "knn_class_optimal.fit(X_class_train_scaled, y_class_train)\n",
    "\n",
    "# Predictions\n",
    "y_class_pred = knn_class_optimal.predict(X_class_test_scaled)\n",
    "y_class_proba = knn_class_optimal.predict_proba(X_class_test_scaled)\n",
    "\n",
    "# Classification metrics\n",
    "class_accuracy = accuracy_score(y_class_test, y_class_pred)\n",
    "print(f\"\\n Final k-NN Classification Performance:\")\n",
    "print(f\"• k = {optimal_k_class_cv}\")\n",
    "print(f\"• Test Accuracy: {class_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_class_test, y_class_pred, target_names=segment_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_class_test, y_class_pred)\n",
    "\n",
    "# Create interactive confusion matrix\n",
    "fig_cm = ff.create_annotated_heatmap(\n",
    " z=cm,\n",
    " x=segment_names,\n",
    " y=segment_names,\n",
    " annotation_text=cm,\n",
    " colorscale='Blues',\n",
    " showscale=True\n",
    ")\n",
    "\n",
    "fig_cm.update_layout(\n",
    " title=f\"k-NN Classification Confusion Matrix (k={optimal_k_class_cv})\",\n",
    " xaxis_title=\"Predicted Segment\",\n",
    " yaxis_title=\"Actual Segment\",\n",
    " height=500\n",
    ")\n",
    "fig_cm.show()\n",
    "\n",
    "# Feature importance analysis (using distance-based importance)\n",
    "print(f\"\\n Feature Impact Analysis:\")\n",
    "\n",
    "# Calculate feature ranges for importance estimation\n",
    "feature_ranges = X_class_train_scaled.std(axis=0)\n",
    "feature_importance_proxy = feature_ranges / feature_ranges.sum()\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    " 'Feature': class_features,\n",
    " 'Importance_Proxy': feature_importance_proxy\n",
    "}).sort_values('Importance_Proxy', ascending=False)\n",
    "\n",
    "print(\"Feature Impact (based on standard deviation):\")\n",
    "for _, row in feature_importance_df.iterrows():\n",
    " print(f\"• {row['Feature']}: {row['Importance_Proxy']:.3f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "fig_feat_imp = go.Figure()\n",
    "\n",
    "fig_feat_imp.add_trace(\n",
    " go.Bar(\n",
    " x=feature_importance_df['Feature'],\n",
    " y=feature_importance_df['Importance_Proxy'],\n",
    " marker_color='skyblue',\n",
    " hovertemplate=\"Feature: %{x}<br>Importance: %{y:.3f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_feat_imp.update_layout(\n",
    " title=\"k-NN Feature Impact Analysis\",\n",
    " xaxis_title=\"Features\",\n",
    " yaxis_title=\"Importance Proxy (Std. Deviation)\",\n",
    " xaxis_tickangle=-45,\n",
    " height=500\n",
    ")\n",
    "fig_feat_imp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DISTANCE METRICS COMPARISON\n",
    "print(\" 3. DISTANCE METRICS COMPARISON\")\n",
    "print(\"=\" * 33)\n",
    "\n",
    "# Test different distance metrics\n",
    "distance_metrics = ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
    "metric_results = {}\n",
    "\n",
    "print(\"Testing different distance metrics on classification task:\")\n",
    "\n",
    "for metric in distance_metrics:\n",
    " if metric == 'minkowski':\n",
    " # Test different p values for Minkowski distance\n",
    " for p in [1, 2, 3]:\n",
    " knn_metric = KNeighborsClassifier(\n",
    " n_neighbors=optimal_k_class_cv,\n",
    " metric='minkowski',\n",
    " p=p\n",
    " )\n",
    "\n",
    " knn_metric.fit(X_class_train_scaled, y_class_train)\n",
    " accuracy = knn_metric.score(X_class_test_scaled, y_class_test)\n",
    "\n",
    " metric_name = f'minkowski_p{p}'\n",
    " metric_results[metric_name] = accuracy\n",
    " print(f\"• {metric_name}: {accuracy:.4f}\")\n",
    " else:\n",
    " knn_metric = KNeighborsClassifier(\n",
    " n_neighbors=optimal_k_class_cv,\n",
    " metric=metric\n",
    " )\n",
    "\n",
    " knn_metric.fit(X_class_train_scaled, y_class_train)\n",
    " accuracy = knn_metric.score(X_class_test_scaled, y_class_test)\n",
    "\n",
    " metric_results[metric] = accuracy\n",
    " print(f\"• {metric}: {accuracy:.4f}\")\n",
    "\n",
    "# Find best metric\n",
    "best_metric = max(metric_results, key=metric_results.get)\n",
    "print(f\"\\n Best distance metric: {best_metric} (Accuracy: {metric_results[best_metric]:.4f})\")\n",
    "\n",
    "# Visualize distance metrics comparison\n",
    "fig_metrics = go.Figure()\n",
    "\n",
    "fig_metrics.add_trace(\n",
    " go.Bar(\n",
    " x=list(metric_results.keys()),\n",
    " y=list(metric_results.values()),\n",
    " marker_color='lightcoral',\n",
    " hovertemplate=\"Metric: %{x}<br>Accuracy: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_metrics.update_layout(\n",
    " title=\"Distance Metrics Comparison (k-NN Classification)\",\n",
    " xaxis_title=\"Distance Metric\",\n",
    " yaxis_title=\"Test Accuracy\",\n",
    " xaxis_tickangle=-45,\n",
    " height=500\n",
    ")\n",
    "fig_metrics.show()\n",
    "\n",
    "# Distance visualization for a sample of points\n",
    "print(f\"\\n Distance Calculation Examples:\")\n",
    "\n",
    "# Take first 5 test samples\n",
    "sample_X = X_class_test_scaled[:5]\n",
    "sample_y = y_class_test.iloc[:5]\n",
    "\n",
    "# Calculate distances to all training points for each metric\n",
    "distance_examples = {}\n",
    "\n",
    "for i, (idx, y_true) in enumerate(zip(sample_X, sample_y)):\n",
    " print(f\"\\nSample {i+1} (True segment: {segment_names[y_true]}):\")\n",
    "\n",
    " # Euclidean distances\n",
    " euclidean_dist = euclidean_distances([idx], X_class_train_scaled)[0]\n",
    " nearest_euclidean = np.argsort(euclidean_dist)[:3]\n",
    "\n",
    " # Manhattan distances\n",
    " manhattan_dist = manhattan_distances([idx], X_class_train_scaled)[0]\n",
    " nearest_manhattan = np.argsort(manhattan_dist)[:3]\n",
    "\n",
    " print(f\" Euclidean - 3 nearest neighbors: {y_class_train.iloc[nearest_euclidean].values}\")\n",
    " print(f\" Manhattan - 3 nearest neighbors: {y_class_train.iloc[nearest_manhattan].values}\")\n",
    "\n",
    " # Prediction with each metric\n",
    " knn_euclidean = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    " knn_manhattan = KNeighborsClassifier(n_neighbors=3, metric='manhattan')\n",
    "\n",
    " knn_euclidean.fit(X_class_train_scaled, y_class_train)\n",
    " knn_manhattan.fit(X_class_train_scaled, y_class_train)\n",
    "\n",
    " pred_euclidean = knn_euclidean.predict([idx])[0]\n",
    " pred_manhattan = knn_manhattan.predict([idx])[0]\n",
    "\n",
    " print(f\" Euclidean prediction: {segment_names[pred_euclidean]}\")\n",
    " print(f\" Manhattan prediction: {segment_names[pred_manhattan]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f48822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DIMENSIONALITY AND CURSE OF DIMENSIONALITY\n",
    "print(\" 4. DIMENSIONALITY ANALYSIS\")\n",
    "print(\"=\" * 29)\n",
    "\n",
    "# Test performance with different numbers of features\n",
    "feature_subsets = {\n",
    " '2D': ['annual_income', 'spending_score'],\n",
    " '3D': ['annual_income', 'spending_score', 'customer_age'],\n",
    " '4D': ['annual_income', 'spending_score', 'customer_age', 'years_customer'],\n",
    " '6D': ['annual_income', 'spending_score', 'customer_age', 'years_customer',\n",
    " 'monthly_purchases', 'electronics_purchases'],\n",
    " 'Full (8D)': class_features\n",
    "}\n",
    "\n",
    "dimensionality_results = {}\n",
    "\n",
    "print(\"Testing k-NN performance across different dimensionalities:\")\n",
    "\n",
    "for dim_name, features in feature_subsets.items():\n",
    " # Prepare data\n",
    " X_dim = X_class_train[features]\n",
    " X_dim_test = X_class_test[features]\n",
    "\n",
    " # Scale\n",
    " scaler_dim = StandardScaler()\n",
    " X_dim_scaled = scaler_dim.fit_transform(X_dim)\n",
    " X_dim_test_scaled = scaler_dim.transform(X_dim_test)\n",
    "\n",
    " # Fit k-NN\n",
    " knn_dim = KNeighborsClassifier(n_neighbors=optimal_k_class_cv)\n",
    " knn_dim.fit(X_dim_scaled, y_class_train)\n",
    "\n",
    " # Evaluate\n",
    " accuracy = knn_dim.score(X_dim_test_scaled, y_class_test)\n",
    "\n",
    " # Cross-validation for robustness\n",
    " cv_scores = cross_val_score(knn_dim, X_dim_scaled, y_class_train, cv=5)\n",
    "\n",
    " dimensionality_results[dim_name] = {\n",
    " 'test_accuracy': accuracy,\n",
    " 'cv_mean': cv_scores.mean(),\n",
    " 'cv_std': cv_scores.std(),\n",
    " 'n_features': len(features)\n",
    " }\n",
    "\n",
    " print(f\"• {dim_name}: Test Acc = {accuracy:.4f}, CV = {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# Visualize dimensionality impact\n",
    "dim_df = pd.DataFrame(dimensionality_results).T\n",
    "dim_df['dimension_label'] = dim_df.index\n",
    "\n",
    "fig_dim = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=['Test Accuracy vs Dimensions', 'Cross-Validation Performance'],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# Test accuracy plot\n",
    "fig_dim.add_trace(\n",
    " go.Scatter(\n",
    " x=dim_df['n_features'],\n",
    " y=dim_df['test_accuracy'],\n",
    " mode='lines+markers',\n",
    " name='Test Accuracy',\n",
    " line=dict(color='blue'),\n",
    " hovertemplate=\"Dimensions: %{x}<br>Accuracy: %{y:.4f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# CV performance with error bars\n",
    "fig_dim.add_trace(\n",
    " go.Scatter(\n",
    " x=dim_df['n_features'],\n",
    " y=dim_df['cv_mean'],\n",
    " error_y=dict(type='data', array=dim_df['cv_std'], visible=True),\n",
    " mode='lines+markers',\n",
    " name='CV Mean ± Std',\n",
    " line=dict(color='red'),\n",
    " hovertemplate=\"Dimensions: %{x}<br>CV Mean: %{y:.4f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_dim.update_layout(\n",
    " title=\"k-NN Performance vs Dimensionality\",\n",
    " height=500\n",
    ")\n",
    "\n",
    "fig_dim.update_xaxes(title_text=\"Number of Features\", row=1, col=1)\n",
    "fig_dim.update_xaxes(title_text=\"Number of Features\", row=1, col=2)\n",
    "fig_dim.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "fig_dim.update_yaxes(title_text=\"CV Accuracy\", row=1, col=2)\n",
    "\n",
    "fig_dim.show()\n",
    "\n",
    "# Analyze curse of dimensionality\n",
    "print(f\"\\n Curse of Dimensionality Analysis:\")\n",
    "print(\"As dimensions increase:\")\n",
    "\n",
    "best_2d_acc = dimensionality_results['2D']['test_accuracy']\n",
    "best_full_acc = dimensionality_results['Full (8D)']['test_accuracy']\n",
    "performance_change = ((best_full_acc - best_2d_acc) / best_2d_acc) * 100\n",
    "\n",
    "print(f\"• 2D performance: {best_2d_acc:.4f}\")\n",
    "print(f\"• Full dimensional performance: {best_full_acc:.4f}\")\n",
    "print(f\"• Performance change: {performance_change:+.1f}%\")\n",
    "\n",
    "if performance_change > 5:\n",
    " print(\"• Result: Adding dimensions IMPROVED performance\")\n",
    " print(\"• Interpretation: Additional features contain valuable signal\")\n",
    "elif performance_change < -5:\n",
    " print(\"• Result: Adding dimensions HURT performance\")\n",
    " print(\"• Interpretation: Curse of dimensionality effect observed\")\n",
    "else:\n",
    " print(\"• Result: Minimal impact from additional dimensions\")\n",
    " print(\"• Interpretation: Marginal information in extra features\")\n",
    "\n",
    "# Distance concentration analysis\n",
    "print(f\"\\n Distance Concentration in High Dimensions:\")\n",
    "\n",
    "# Calculate average distances in different dimensions\n",
    "for dim_name, features in feature_subsets.items():\n",
    " if len(features) >= 2: # Skip if too few features\n",
    " X_sample = X_class_train_scaled[:100, :len(features)] # First 100 samples\n",
    "\n",
    " # Calculate pairwise distances\n",
    " distances = euclidean_distances(X_sample, X_sample)\n",
    " # Remove diagonal (zero distances)\n",
    " distances = distances[np.triu_indices_from(distances, k=1)]\n",
    "\n",
    " mean_dist = distances.mean()\n",
    " std_dist = distances.std()\n",
    " cv_dist = std_dist / mean_dist # Coefficient of variation\n",
    "\n",
    " print(f\"• {dim_name}: Mean distance = {mean_dist:.3f}, CV = {cv_dist:.3f}\")\n",
    "\n",
    "print(\"\\nNote: Lower CV indicates distance concentration (curse of dimensionality)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. NEIGHBORHOOD ANALYSIS AND VISUALIZATION\n",
    "print(\" 5. NEIGHBORHOOD ANALYSIS AND VISUALIZATION\")\n",
    "print(\"=\" * 44)\n",
    "\n",
    "# Analyze neighborhood composition for different values of k\n",
    "print(\"Analyzing neighborhood composition for customer segmentation:\")\n",
    "\n",
    "# Select a few interesting test samples for analysis\n",
    "interesting_samples = []\n",
    "for segment in range(4):\n",
    " # Find samples of each segment\n",
    " segment_indices = np.where(y_class_test == segment)[0]\n",
    " if len(segment_indices) > 0:\n",
    " interesting_samples.append(segment_indices[0])\n",
    "\n",
    "print(f\"Analyzing {len(interesting_samples)} representative samples...\")\n",
    "\n",
    "# Fit nearest neighbors model for analysis\n",
    "nn_analyzer = NearestNeighbors(n_neighbors=15, metric='euclidean')\n",
    "nn_analyzer.fit(X_class_train_scaled)\n",
    "\n",
    "neighborhood_analysis = {}\n",
    "\n",
    "for i, sample_idx in enumerate(interesting_samples):\n",
    " sample = X_class_test_scaled[sample_idx:sample_idx+1]\n",
    " true_segment = y_class_test.iloc[sample_idx]\n",
    "\n",
    " # Find nearest neighbors\n",
    " distances, indices = nn_analyzer.kneighbors(sample)\n",
    " neighbor_segments = y_class_train.iloc[indices[0]]\n",
    "\n",
    " print(f\"\\nSample {i+1} - True Segment: {segment_names[true_segment]}\")\n",
    "\n",
    " # Analyze neighborhood composition for different k values\n",
    " for k in [1, 3, 5, 10, 15]:\n",
    " k_neighbors = neighbor_segments[:k]\n",
    " segment_counts = k_neighbors.value_counts().sort_index()\n",
    "\n",
    " print(f\" k={k}: \", end=\"\")\n",
    " composition = []\n",
    " for seg in range(4):\n",
    " count = segment_counts.get(seg, 0)\n",
    " if count > 0:\n",
    " composition.append(f\"{segment_names[seg][:3]}({count})\")\n",
    " print(\" | \".join(composition))\n",
    "\n",
    " # Make prediction for this k\n",
    " majority_vote = k_neighbors.mode()[0] if len(k_neighbors.mode()) > 0 else k_neighbors.iloc[0]\n",
    " correct = \"\" if majority_vote == true_segment else \"\"\n",
    " print(f\" Prediction: {segment_names[majority_vote]} {correct}\")\n",
    "\n",
    " neighborhood_analysis[f\"Sample_{i+1}\"] = {\n",
    " 'true_segment': true_segment,\n",
    " 'neighbors': neighbor_segments,\n",
    " 'distances': distances[0]\n",
    " }\n",
    "\n",
    "# Visualize neighborhood diversity\n",
    "print(f\"\\n Neighborhood Diversity Analysis:\")\n",
    "\n",
    "# Calculate neighborhood purity for different k values\n",
    "k_values_analysis = [1, 3, 5, 7, 10, 15, 20]\n",
    "purity_scores = []\n",
    "\n",
    "for k in k_values_analysis:\n",
    " total_purity = 0\n",
    " n_samples = min(100, len(X_class_test_scaled)) # Limit for performance\n",
    "\n",
    " for i in range(n_samples):\n",
    " sample = X_class_test_scaled[i:i+1]\n",
    " true_segment = y_class_test.iloc[i]\n",
    "\n",
    " # Find k nearest neighbors\n",
    " distances, indices = nn_analyzer.kneighbors(sample, n_neighbors=k)\n",
    " neighbor_segments = y_class_train.iloc[indices[0]]\n",
    "\n",
    " # Calculate purity (fraction of neighbors with same segment as majority)\n",
    " majority_segment = neighbor_segments.mode()[0]\n",
    " purity = (neighbor_segments == majority_segment).sum() / k\n",
    " total_purity += purity\n",
    "\n",
    " avg_purity = total_purity / n_samples\n",
    " purity_scores.append(avg_purity)\n",
    " print(f\"• k={k}: Average neighborhood purity = {avg_purity:.3f}\")\n",
    "\n",
    "# Plot neighborhood purity\n",
    "fig_purity = go.Figure()\n",
    "\n",
    "fig_purity.add_trace(\n",
    " go.Scatter(\n",
    " x=k_values_analysis,\n",
    " y=purity_scores,\n",
    " mode='lines+markers',\n",
    " name='Neighborhood Purity',\n",
    " line=dict(color='purple'),\n",
    " hovertemplate=\"k: %{x}<br>Purity: %{y:.3f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_purity.update_layout(\n",
    " title=\"Neighborhood Purity vs k Value\",\n",
    " xaxis_title=\"Number of Neighbors (k)\",\n",
    " yaxis_title=\"Average Neighborhood Purity\",\n",
    " height=400\n",
    ")\n",
    "fig_purity.show()\n",
    "\n",
    "# Distance distribution analysis\n",
    "print(f\"\\n Distance Distribution Analysis:\")\n",
    "\n",
    "# Analyze distance distributions for each segment\n",
    "segment_distances = {seg: [] for seg in range(4)}\n",
    "\n",
    "# Sample 50 points from each segment for analysis\n",
    "for segment in range(4):\n",
    " segment_mask = y_class_train == segment\n",
    " segment_data = X_class_train_scaled[segment_mask]\n",
    "\n",
    " if len(segment_data) > 1:\n",
    " # Sample points\n",
    " sample_size = min(50, len(segment_data))\n",
    " sampled_indices = np.random.choice(len(segment_data), sample_size, replace=False)\n",
    " sampled_data = segment_data[sampled_indices]\n",
    "\n",
    " # Calculate pairwise distances within segment\n",
    " distances = euclidean_distances(sampled_data, sampled_data)\n",
    " # Get upper triangle (avoid duplicates and zeros)\n",
    " distances = distances[np.triu_indices_from(distances, k=1)]\n",
    " segment_distances[segment] = distances\n",
    "\n",
    "# Plot distance distributions\n",
    "fig_dist = go.Figure()\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "for segment in range(4):\n",
    " if len(segment_distances[segment]) > 0:\n",
    " fig_dist.add_trace(\n",
    " go.Histogram(\n",
    " x=segment_distances[segment],\n",
    " name=segment_names[segment],\n",
    " opacity=0.7,\n",
    " marker_color=colors[segment],\n",
    " nbinsx=30\n",
    " )\n",
    " )\n",
    "\n",
    "fig_dist.update_layout(\n",
    " title=\"Intra-Segment Distance Distributions\",\n",
    " xaxis_title=\"Euclidean Distance\",\n",
    " yaxis_title=\"Frequency\",\n",
    " barmode='overlay',\n",
    " height=500\n",
    ")\n",
    "fig_dist.show()\n",
    "\n",
    "# Calculate and display statistics\n",
    "print(f\"\\nIntra-segment distance statistics:\")\n",
    "for segment in range(4):\n",
    " if len(segment_distances[segment]) > 0:\n",
    " distances = segment_distances[segment]\n",
    " mean_dist = distances.mean()\n",
    " std_dist = distances.std()\n",
    " print(f\"• {segment_names[segment]}: Mean = {mean_dist:.3f}, Std = {std_dist:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\n",
    "print(\" 6. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\")\n",
    "print(\"=\" * 54)\n",
    "\n",
    "# Model interpretability analysis\n",
    "print(\" k-NN Model Interpretability Analysis:\")\n",
    "\n",
    "# Feature importance based on prediction sensitivity\n",
    "def calculate_feature_sensitivity(model, X_baseline, feature_names, n_samples=100):\n",
    " \"\"\"Calculate feature sensitivity by perturbing each feature\"\"\"\n",
    "\n",
    " # Sample baseline predictions\n",
    " sample_indices = np.random.choice(len(X_baseline), n_samples, replace=False)\n",
    " X_sample = X_baseline[sample_indices]\n",
    " baseline_predictions = model.predict(X_sample)\n",
    "\n",
    " sensitivities = {}\n",
    "\n",
    " for i, feature in enumerate(feature_names):\n",
    " # Perturb this feature by adding noise\n",
    " X_perturbed = X_sample.copy()\n",
    " noise_std = X_sample[:, i].std() * 0.1 # 10% of feature std\n",
    " X_perturbed[:, i] += np.random.normal(0, noise_std, len(X_sample))\n",
    "\n",
    " # Get new predictions\n",
    " perturbed_predictions = model.predict(X_perturbed)\n",
    "\n",
    " # Calculate sensitivity (prediction change rate)\n",
    " change_rate = (baseline_predictions != perturbed_predictions).mean()\n",
    " sensitivities[feature] = change_rate\n",
    "\n",
    " return sensitivities\n",
    "\n",
    "# Calculate feature sensitivities\n",
    "sensitivities = calculate_feature_sensitivity(\n",
    " knn_class_optimal, X_class_train_scaled, class_features\n",
    ")\n",
    "\n",
    "print(\"Feature Sensitivity Analysis (prediction change rate with 10% noise):\")\n",
    "sensitivity_df = pd.DataFrame(list(sensitivities.items()),\n",
    " columns=['Feature', 'Sensitivity']).sort_values('Sensitivity', ascending=False)\n",
    "\n",
    "for _, row in sensitivity_df.iterrows():\n",
    " print(f\"• {row['Feature']}: {row['Sensitivity']:.3f}\")\n",
    "\n",
    "# Visualize feature sensitivity\n",
    "fig_sensitivity = go.Figure()\n",
    "\n",
    "fig_sensitivity.add_trace(\n",
    " go.Bar(\n",
    " x=sensitivity_df['Feature'],\n",
    " y=sensitivity_df['Sensitivity'],\n",
    " marker_color='lightgreen',\n",
    " hovertemplate=\"Feature: %{x}<br>Sensitivity: %{y:.3f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_sensitivity.update_layout(\n",
    " title=\"k-NN Feature Sensitivity Analysis\",\n",
    " xaxis_title=\"Features\",\n",
    " yaxis_title=\"Prediction Change Rate\",\n",
    " xaxis_tickangle=-45,\n",
    " height=500\n",
    ")\n",
    "fig_sensitivity.show()\n",
    "\n",
    "# Segment-specific insights\n",
    "print(f\"\\n Segment-Specific Business Insights:\")\n",
    "\n",
    "# Analyze prediction confidence by segment\n",
    "segment_confidence = {}\n",
    "segment_sample_analysis = {}\n",
    "\n",
    "for segment in range(4):\n",
    " # Get test samples for this segment\n",
    " segment_mask = y_class_test == segment\n",
    " if segment_mask.sum() > 0:\n",
    " segment_X = X_class_test_scaled[segment_mask]\n",
    " segment_predictions = knn_class_optimal.predict(segment_X)\n",
    " segment_probabilities = knn_class_optimal.predict_proba(segment_X)\n",
    "\n",
    " # Calculate confidence (max probability)\n",
    " confidences = segment_probabilities.max(axis=1)\n",
    " accuracy = (segment_predictions == segment).mean()\n",
    "\n",
    " segment_confidence[segment] = {\n",
    " 'accuracy': accuracy,\n",
    " 'avg_confidence': confidences.mean(),\n",
    " 'min_confidence': confidences.min(),\n",
    " 'max_confidence': confidences.max()\n",
    " }\n",
    "\n",
    " # Sample characteristics analysis\n",
    " segment_features = X_class_test[segment_mask].mean()\n",
    " segment_sample_analysis[segment] = segment_features\n",
    "\n",
    "print(\"Segment Prediction Performance:\")\n",
    "for segment in range(4):\n",
    " if segment in segment_confidence:\n",
    " conf = segment_confidence[segment]\n",
    " print(f\"\\n• {segment_names[segment]}:\")\n",
    " print(f\" - Accuracy: {conf['accuracy']:.3f}\")\n",
    " print(f\" - Avg Confidence: {conf['avg_confidence']:.3f}\")\n",
    " print(f\" - Confidence Range: {conf['min_confidence']:.3f} - {conf['max_confidence']:.3f}\")\n",
    "\n",
    "# Strategic recommendations based on analysis\n",
    "print(f\"\\n STRATEGIC RECOMMENDATIONS:\")\n",
    "\n",
    "print(f\"\\n1. OPTIMAL MODEL CONFIGURATION:\")\n",
    "print(f\" • Use k = {optimal_k_class_cv} neighbors for customer segmentation\")\n",
    "print(f\" • Apply StandardScaler for feature preprocessing\")\n",
    "print(f\" • Expected classification accuracy: {class_accuracy:.1%}\")\n",
    "print(f\" • Use {best_metric} distance metric for optimal performance\")\n",
    "\n",
    "print(f\"\\n2. FEATURE ENGINEERING INSIGHTS:\")\n",
    "most_sensitive = sensitivity_df.iloc[0]['Feature']\n",
    "least_sensitive = sensitivity_df.iloc[-1]['Feature']\n",
    "print(f\" • Most impactful feature: {most_sensitive}\")\n",
    "print(f\" • Least impactful feature: {least_sensitive}\")\n",
    "print(f\" • Consider feature selection to reduce dimensionality\")\n",
    "print(f\" • Focus data collection efforts on high-sensitivity features\")\n",
    "\n",
    "print(f\"\\n3. SEGMENT-SPECIFIC STRATEGIES:\")\n",
    "\n",
    "# Find most/least predictable segments\n",
    "if segment_confidence:\n",
    " segment_accuracies = {seg: conf['accuracy'] for seg, conf in segment_confidence.items()}\n",
    " most_predictable = max(segment_accuracies, key=segment_accuracies.get)\n",
    " least_predictable = min(segment_accuracies, key=segment_accuracies.get)\n",
    "\n",
    " print(f\" • Most predictable segment: {segment_names[most_predictable]} ({segment_accuracies[most_predictable]:.1%} accuracy)\")\n",
    " print(f\" - Implement automated targeting for this segment\")\n",
    " print(f\" - High confidence in model recommendations\")\n",
    "\n",
    " print(f\" • Least predictable segment: {segment_names[least_predictable]} ({segment_accuracies[least_predictable]:.1%} accuracy)\")\n",
    " print(f\" - Requires additional data collection\")\n",
    " print(f\" - Consider manual review for predictions\")\n",
    "\n",
    "print(f\"\\n4. IMPLEMENTATION RECOMMENDATIONS:\")\n",
    "print(f\" • Real-time prediction latency: Very fast (simple distance calculation)\")\n",
    "print(f\" • Memory requirements: Store all training data ({len(X_class_train)} samples)\")\n",
    "print(f\" • Model updates: Retrain when significant data drift detected\")\n",
    "print(f\" • Scalability: Consider approximate nearest neighbors for large datasets\")\n",
    "\n",
    "print(f\"\\n5. BUSINESS VALUE DRIVERS:\")\n",
    "\n",
    "# Calculate potential business impact\n",
    "baseline_accuracy = max(y_class_train.value_counts()) / len(y_class_train) # Majority class baseline\n",
    "improvement = (class_accuracy - baseline_accuracy) * 100\n",
    "\n",
    "print(f\" • Model improves over random assignment by {improvement:.1f} percentage points\")\n",
    "print(f\" • Enables personalized marketing campaigns\")\n",
    "print(f\" • Supports customer lifetime value prediction\")\n",
    "print(f\" • Facilitates inventory planning by segment\")\n",
    "\n",
    "# ROI estimation\n",
    "print(f\"\\n6. ROI ESTIMATION:\")\n",
    "print(f\" • If applied to customer base of 10,000:\")\n",
    "print(f\" - Correctly classified customers: ~{int(class_accuracy * 10000)}\")\n",
    "print(f\" - Misclassified customers: ~{int((1-class_accuracy) * 10000)}\")\n",
    "print(f\" • Assuming $50 value per correct classification:\")\n",
    "print(f\" - Annual value: ${int(class_accuracy * 10000 * 50):,}\")\n",
    "print(f\" • Model development cost amortized over high-volume predictions\")\n",
    "\n",
    "print(f\"\\n7. NEXT STEPS:\")\n",
    "print(f\" • Deploy model for A/B testing on customer subset\")\n",
    "print(f\" • Monitor prediction confidence and flag low-confidence cases\")\n",
    "print(f\" • Collect feedback to validate segment assignments\")\n",
    "print(f\" • Consider ensemble methods combining k-NN with other algorithms\")\n",
    "print(f\" • Implement automated model retraining pipeline\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*75)\n",
    "print(f\" k-NN LEARNING SUMMARY:\")\n",
    "print(f\" Mastered instance-based learning principles\")\n",
    "print(f\" Optimized k-value selection through cross-validation\")\n",
    "print(f\" Analyzed impact of distance metrics and scaling\")\n",
    "print(f\" Understood curse of dimensionality effects\")\n",
    "print(f\" Performed neighborhood analysis and interpretability\")\n",
    "print(f\" Generated actionable business insights and ROI estimates\")\n",
    "print(f\"=\"*75)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}