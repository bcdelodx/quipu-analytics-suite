{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 2: Naive Bayes Classification\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** dc5e0d95-8fab-4735-bcc0-fbc7b6c5ed53\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 2: Naive Bayes Classification,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** dc5e0d95-8fab-4735-bcc0-fbc7b6c5ed53\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c779c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Essential Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Additional utilities\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 2: Naive Bayes Classification - Libraries Loaded Successfully!\")\n",
    "print(\"=\" * 75)\n",
    "print(\"Available Naive Bayes Techniques:\")\n",
    "print(\"• Gaussian Naive Bayes - Continuous features with normal distribution\")\n",
    "print(\"• Multinomial Naive Bayes - Discrete count features (text classification)\")\n",
    "print(\"• Bernoulli Naive Bayes - Binary features and document classification\")\n",
    "print(\"• Probability Analysis - Conditional probabilities and feature independence\")\n",
    "print(\"• Text Classification - Email spam detection and sentiment analysis\")\n",
    "print(\"• Medical Diagnosis - Risk assessment with categorical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ba147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Comprehensive Datasets for Naive Bayes Analysis\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_naive_bayes_datasets():\n",
    " \"\"\"Generate datasets optimized for different Naive Bayes variants\"\"\"\n",
    "\n",
    " # 1. GAUSSIAN NAIVE BAYES DATASET - Medical Diagnosis\n",
    " n_patients = 1000\n",
    "\n",
    " # Generate patient features with realistic distributions\n",
    " age = np.random.normal(45, 15, n_patients)\n",
    " age = np.clip(age, 18, 90)\n",
    "\n",
    " # BMI with different distributions by health status\n",
    " healthy_mask = np.random.random(n_patients) < 0.7 # 70% healthy\n",
    "\n",
    " bmi = np.zeros(n_patients)\n",
    " bmi[healthy_mask] = np.random.normal(24, 3, healthy_mask.sum()) # Healthy BMI\n",
    " bmi[~healthy_mask] = np.random.normal(29, 4, (~healthy_mask).sum()) # Higher BMI for at-risk\n",
    " bmi = np.clip(bmi, 16, 45)\n",
    "\n",
    " # Blood pressure (systolic)\n",
    " blood_pressure = np.zeros(n_patients)\n",
    " blood_pressure[healthy_mask] = np.random.normal(120, 10, healthy_mask.sum())\n",
    " blood_pressure[~healthy_mask] = np.random.normal(140, 15, (~healthy_mask).sum())\n",
    " blood_pressure = np.clip(blood_pressure, 90, 200)\n",
    "\n",
    " # Cholesterol levels\n",
    " cholesterol = np.zeros(n_patients)\n",
    " cholesterol[healthy_mask] = np.random.normal(180, 20, healthy_mask.sum())\n",
    " cholesterol[~healthy_mask] = np.random.normal(220, 25, (~healthy_mask).sum())\n",
    " cholesterol = np.clip(cholesterol, 120, 350)\n",
    "\n",
    " # Exercise hours per week\n",
    " exercise_hours = np.zeros(n_patients)\n",
    " exercise_hours[healthy_mask] = np.random.gamma(2, 2, healthy_mask.sum()) + 2\n",
    " exercise_hours[~healthy_mask] = np.random.gamma(1, 1, (~healthy_mask).sum()) + 0.5\n",
    " exercise_hours = np.clip(exercise_hours, 0, 20)\n",
    "\n",
    " # Smoking history (binary, but affects continuous features)\n",
    " smoking = np.random.binomial(1, 0.25, n_patients) # 25% smokers\n",
    "\n",
    " # Adjust features for smokers\n",
    " blood_pressure[smoking == 1] += np.random.normal(10, 5, (smoking == 1).sum())\n",
    " cholesterol[smoking == 1] += np.random.normal(15, 8, (smoking == 1).sum())\n",
    " exercise_hours[smoking == 1] *= 0.8 # Smokers exercise less\n",
    "\n",
    " # Create health risk target (influenced by all factors)\n",
    " risk_score = (\n",
    " (age - 45) / 15 * 0.3 +\n",
    " (bmi - 25) / 5 * 0.25 +\n",
    " (blood_pressure - 120) / 20 * 0.2 +\n",
    " (cholesterol - 200) / 30 * 0.15 +\n",
    " (5 - exercise_hours) / 5 * 0.1 +\n",
    " smoking * 0.3 +\n",
    " np.random.normal(0, 0.1, n_patients) # Add noise\n",
    " )\n",
    "\n",
    " health_risk = (risk_score > 0.5).astype(int) # Binary: 0=Low Risk, 1=High Risk\n",
    "\n",
    " medical_df = pd.DataFrame({\n",
    " 'age': age,\n",
    " 'bmi': bmi,\n",
    " 'blood_pressure': blood_pressure,\n",
    " 'cholesterol': cholesterol,\n",
    " 'exercise_hours': exercise_hours,\n",
    " 'smoking': smoking,\n",
    " 'health_risk': health_risk\n",
    " })\n",
    "\n",
    " # 2. MULTINOMIAL NAIVE BAYES DATASET - Document Classification\n",
    " # Generate synthetic text documents\n",
    " topics = ['technology', 'sports', 'politics', 'entertainment']\n",
    "\n",
    " # Word vocabularies for each topic\n",
    " vocab = {\n",
    " 'technology': ['computer', 'software', 'data', 'algorithm', 'artificial', 'intelligence',\n",
    " 'programming', 'code', 'system', 'digital', 'innovation', 'tech', 'app',\n",
    " 'database', 'machine', 'learning', 'cloud', 'server', 'network', 'security'],\n",
    " 'sports': ['game', 'team', 'player', 'score', 'win', 'match', 'championship', 'league',\n",
    " 'football', 'basketball', 'soccer', 'baseball', 'tennis', 'golf', 'olympic',\n",
    " 'coach', 'training', 'competition', 'tournament', 'athletic'],\n",
    " 'politics': ['government', 'election', 'vote', 'president', 'congress', 'policy', 'law',\n",
    " 'democracy', 'republican', 'democrat', 'campaign', 'senator', 'governor',\n",
    " 'bill', 'constitution', 'court', 'justice', 'reform', 'debate', 'citizen'],\n",
    " 'entertainment': ['movie', 'film', 'actor', 'actress', 'director', 'music', 'song',\n",
    " 'concert', 'album', 'artist', 'show', 'television', 'celebrity',\n",
    " 'theater', 'performance', 'award', 'oscar', 'grammy', 'festival', 'star']\n",
    " }\n",
    "\n",
    " documents = []\n",
    " labels = []\n",
    "\n",
    " n_docs_per_topic = 200\n",
    "\n",
    " for topic in topics:\n",
    " for _ in range(n_docs_per_topic):\n",
    " # Generate document with 10-30 words\n",
    " doc_length = np.random.randint(10, 31)\n",
    "\n",
    " # 70% words from topic vocabulary, 30% from other topics (noise)\n",
    " topic_words = np.random.choice(vocab[topic],\n",
    " size=int(doc_length * 0.7),\n",
    " replace=True)\n",
    "\n",
    " # Add some noise from other topics\n",
    " other_topics = [t for t in topics if t != topic]\n",
    " noise_topic = np.random.choice(other_topics)\n",
    " noise_words = np.random.choice(vocab[noise_topic],\n",
    " size=int(doc_length * 0.3),\n",
    " replace=True)\n",
    "\n",
    " # Combine and shuffle\n",
    " all_words = np.concatenate([topic_words, noise_words])\n",
    " np.random.shuffle(all_words)\n",
    "\n",
    " documents.append(' '.join(all_words))\n",
    " labels.append(topic)\n",
    "\n",
    " text_df = pd.DataFrame({\n",
    " 'document': documents,\n",
    " 'topic': labels\n",
    " })\n",
    "\n",
    " # 3. BERNOULLI NAIVE BAYES DATASET - Email Spam Detection\n",
    " n_emails = 1000\n",
    "\n",
    " # Define spam and ham indicators\n",
    " spam_indicators = [\n",
    " 'free', 'money', 'offer', 'click', 'urgent', 'limited', 'win', 'prize',\n",
    " 'discount', 'deal', 'sale', 'buy', 'cheap', 'save', 'cash', 'bonus'\n",
    " ]\n",
    "\n",
    " ham_indicators = [\n",
    " 'meeting', 'schedule', 'report', 'project', 'team', 'work', 'office',\n",
    " 'client', 'business', 'professional', 'conference', 'presentation'\n",
    " ]\n",
    "\n",
    " emails = []\n",
    " spam_labels = []\n",
    "\n",
    " # Generate spam emails (40% of dataset)\n",
    " n_spam = int(n_emails * 0.4)\n",
    " for _ in range(n_spam):\n",
    " # Spam emails have higher probability of spam indicators\n",
    " features = {}\n",
    " for indicator in spam_indicators:\n",
    " features[f'has_{indicator}'] = np.random.binomial(1, 0.6) # 60% chance\n",
    " for indicator in ham_indicators:\n",
    " features[f'has_{indicator}'] = np.random.binomial(1, 0.1) # 10% chance\n",
    "\n",
    " # Additional spam features\n",
    " features['has_exclamation'] = np.random.binomial(1, 0.8)\n",
    " features['has_all_caps'] = np.random.binomial(1, 0.7)\n",
    " features['has_numbers'] = np.random.binomial(1, 0.9)\n",
    " features['length_short'] = np.random.binomial(1, 0.6) # Spam tends to be shorter\n",
    "\n",
    " emails.append(features)\n",
    " spam_labels.append(1) # Spam\n",
    "\n",
    " # Generate ham emails (60% of dataset)\n",
    " n_ham = n_emails - n_spam\n",
    " for _ in range(n_ham):\n",
    " features = {}\n",
    " for indicator in spam_indicators:\n",
    " features[f'has_{indicator}'] = np.random.binomial(1, 0.05) # 5% chance\n",
    " for indicator in ham_indicators:\n",
    " features[f'has_{indicator}'] = np.random.binomial(1, 0.4) # 40% chance\n",
    "\n",
    " # Additional ham features\n",
    " features['has_exclamation'] = np.random.binomial(1, 0.2)\n",
    " features['has_all_caps'] = np.random.binomial(1, 0.1)\n",
    " features['has_numbers'] = np.random.binomial(1, 0.3)\n",
    " features['length_short'] = np.random.binomial(1, 0.2) # Ham tends to be longer\n",
    "\n",
    " emails.append(features)\n",
    " spam_labels.append(0) # Ham\n",
    "\n",
    " # Convert to DataFrame\n",
    " email_features = list(emails[0].keys())\n",
    " email_data = {feature: [email[feature] for email in emails] for feature in email_features}\n",
    " email_data['is_spam'] = spam_labels\n",
    "\n",
    " email_df = pd.DataFrame(email_data)\n",
    "\n",
    " return medical_df, text_df, email_df\n",
    "\n",
    "# Generate datasets\n",
    "print(\" Generating Naive Bayes optimized datasets...\")\n",
    "medical_df, text_df, email_df = generate_naive_bayes_datasets()\n",
    "\n",
    "print(f\"Medical Dataset (Gaussian NB): {medical_df.shape}\")\n",
    "print(f\"Text Dataset (Multinomial NB): {text_df.shape}\")\n",
    "print(f\"Email Dataset (Bernoulli NB): {email_df.shape}\")\n",
    "\n",
    "print(\"\\nMedical Dataset (Health Risk Assessment):\")\n",
    "print(medical_df.head())\n",
    "print(f\"High Risk Rate: {medical_df['health_risk'].mean():.1%}\")\n",
    "\n",
    "print(\"\\nText Dataset (Document Classification):\")\n",
    "print(text_df.head())\n",
    "print(\"Topic Distribution:\")\n",
    "print(text_df['topic'].value_counts())\n",
    "\n",
    "print(\"\\nEmail Dataset (Spam Detection):\")\n",
    "print(email_df.head())\n",
    "print(f\"Spam Rate: {email_df['is_spam'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GAUSSIAN NAIVE BAYES ANALYSIS\n",
    "print(\"🩺 1. GAUSSIAN NAIVE BAYES ANALYSIS\")\n",
    "print(\"=\" * 34)\n",
    "\n",
    "# Prepare medical data\n",
    "medical_features = ['age', 'bmi', 'blood_pressure', 'cholesterol', 'exercise_hours', 'smoking']\n",
    "X_medical = medical_df[medical_features]\n",
    "y_medical = medical_df['health_risk']\n",
    "\n",
    "# Split data\n",
    "X_med_train, X_med_test, y_med_train, y_med_test = train_test_split(\n",
    " X_medical, y_medical, test_size=0.2, random_state=42, stratify=y_medical\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_med_train.shape}\")\n",
    "print(f\"Test set: {X_med_test.shape}\")\n",
    "print(f\"Class distribution: {y_med_train.value_counts().to_dict()}\")\n",
    "\n",
    "# Train Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_med_train, y_med_train)\n",
    "\n",
    "# Predictions and probabilities\n",
    "y_med_pred = gnb.predict(X_med_test)\n",
    "y_med_proba = gnb.predict_proba(X_med_test)\n",
    "\n",
    "# Performance metrics\n",
    "med_accuracy = accuracy_score(y_med_test, y_med_pred)\n",
    "print(f\"\\n Gaussian Naive Bayes Performance:\")\n",
    "print(f\"• Test Accuracy: {med_accuracy:.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(gnb, X_med_train, y_med_train, cv=5)\n",
    "print(f\"• Cross-validation: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_med_test, y_med_pred, target_names=['Low Risk', 'High Risk']))\n",
    "\n",
    "# Feature distribution analysis by class\n",
    "print(f\"\\n Feature Distribution Analysis:\")\n",
    "\n",
    "feature_stats = {}\n",
    "for feature in medical_features:\n",
    " low_risk_values = X_med_train[y_med_train == 0][feature]\n",
    " high_risk_values = X_med_train[y_med_train == 1][feature]\n",
    "\n",
    " feature_stats[feature] = {\n",
    " 'low_risk_mean': low_risk_values.mean(),\n",
    " 'low_risk_std': low_risk_values.std(),\n",
    " 'high_risk_mean': high_risk_values.mean(),\n",
    " 'high_risk_std': high_risk_values.std()\n",
    " }\n",
    "\n",
    " print(f\"• {feature}:\")\n",
    " print(f\" Low Risk: μ={low_risk_values.mean():.2f}, σ={low_risk_values.std():.2f}\")\n",
    " print(f\" High Risk: μ={high_risk_values.mean():.2f}, σ={high_risk_values.std():.2f}\")\n",
    "\n",
    "# Visualize feature distributions\n",
    "fig_distributions = make_subplots(\n",
    " rows=2, cols=3,\n",
    " subplot_titles=medical_features,\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "colors = ['blue', 'red']\n",
    "risk_labels = ['Low Risk', 'High Risk']\n",
    "\n",
    "for i, feature in enumerate(medical_features):\n",
    " row = i // 3 + 1\n",
    " col = i % 3 + 1\n",
    "\n",
    " for risk_level in [0, 1]:\n",
    " feature_data = X_med_train[y_med_train == risk_level][feature]\n",
    "\n",
    " fig_distributions.add_trace(\n",
    " go.Histogram(\n",
    " x=feature_data,\n",
    " name=f'{risk_labels[risk_level]}',\n",
    " opacity=0.7,\n",
    " marker_color=colors[risk_level],\n",
    " nbinsx=20,\n",
    " showlegend=(i == 0) # Only show legend for first subplot\n",
    " ),\n",
    " row=row, col=col\n",
    " )\n",
    "\n",
    "fig_distributions.update_layout(\n",
    " title=\"Feature Distributions by Health Risk Class\",\n",
    " height=600,\n",
    " barmode='overlay'\n",
    ")\n",
    "fig_distributions.show()\n",
    "\n",
    "# Probability analysis\n",
    "print(f\"\\n Probability Analysis (Sample Predictions):\")\n",
    "\n",
    "# Show probability predictions for first 10 test samples\n",
    "sample_indices = range(min(10, len(X_med_test)))\n",
    "for i in sample_indices:\n",
    " actual = y_med_test.iloc[i]\n",
    " predicted = y_med_pred[i]\n",
    " prob_low = y_med_proba[i][0]\n",
    " prob_high = y_med_proba[i][1]\n",
    "\n",
    " print(f\"Patient {i+1}: Actual={risk_labels[actual]}, \"\n",
    " f\"Predicted={risk_labels[predicted]} \"\n",
    " f\"(P(Low)={prob_low:.3f}, P(High)={prob_high:.3f})\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_medical = confusion_matrix(y_med_test, y_med_pred)\n",
    "\n",
    "fig_cm_medical = ff.create_annotated_heatmap(\n",
    " z=cm_medical,\n",
    " x=['Low Risk', 'High Risk'],\n",
    " y=['Low Risk', 'High Risk'],\n",
    " annotation_text=cm_medical,\n",
    " colorscale='Blues',\n",
    " showscale=True\n",
    ")\n",
    "\n",
    "fig_cm_medical.update_layout(\n",
    " title=\"Gaussian Naive Bayes Confusion Matrix (Medical Diagnosis)\",\n",
    " xaxis_title=\"Predicted\",\n",
    " yaxis_title=\"Actual\",\n",
    " height=400\n",
    ")\n",
    "fig_cm_medical.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_med_test, y_med_proba[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig_roc = go.Figure()\n",
    "\n",
    "fig_roc.add_trace(\n",
    " go.Scatter(\n",
    " x=fpr,\n",
    " y=tpr,\n",
    " mode='lines',\n",
    " name=f'ROC Curve (AUC = {roc_auc:.3f})',\n",
    " line=dict(color='blue', width=2)\n",
    " )\n",
    ")\n",
    "\n",
    "fig_roc.add_trace(\n",
    " go.Scatter(\n",
    " x=[0, 1],\n",
    " y=[0, 1],\n",
    " mode='lines',\n",
    " name='Random Classifier',\n",
    " line=dict(color='red', dash='dash')\n",
    " )\n",
    ")\n",
    "\n",
    "fig_roc.update_layout(\n",
    " title=\"ROC Curve - Gaussian Naive Bayes (Medical Diagnosis)\",\n",
    " xaxis_title=\"False Positive Rate\",\n",
    " yaxis_title=\"True Positive Rate\",\n",
    " height=500\n",
    ")\n",
    "fig_roc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1def0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MULTINOMIAL NAIVE BAYES ANALYSIS (TEXT CLASSIFICATION)\n",
    "print(\" 2. MULTINOMIAL NAIVE BAYES ANALYSIS\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "# Prepare text data using TF-IDF vectorization\n",
    "print(\"Processing text documents...\")\n",
    "\n",
    "# Create TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    " max_features=1000, # Limit vocabulary size\n",
    " stop_words='english',\n",
    " ngram_range=(1, 2), # Include unigrams and bigrams\n",
    " min_df=2, # Ignore terms that appear in less than 2 documents\n",
    " max_df=0.95 # Ignore terms that appear in more than 95% of documents\n",
    ")\n",
    "\n",
    "# Transform documents to TF-IDF features\n",
    "X_text_tfidf = tfidf_vectorizer.fit_transform(text_df['document'])\n",
    "y_text = text_df['topic']\n",
    "\n",
    "print(f\"TF-IDF Matrix shape: {X_text_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Split data\n",
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(\n",
    " X_text_tfidf, y_text, test_size=0.2, random_state=42, stratify=y_text\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_text_train.shape}\")\n",
    "print(f\"Test set: {X_text_test.shape}\")\n",
    "\n",
    "# Also create count-based features for Multinomial NB\n",
    "count_vectorizer = CountVectorizer(\n",
    " max_features=1000,\n",
    " stop_words='english',\n",
    " ngram_range=(1, 2),\n",
    " min_df=2,\n",
    " max_df=0.95\n",
    ")\n",
    "\n",
    "X_text_counts = count_vectorizer.fit_transform(text_df['document'])\n",
    "X_counts_train, X_counts_test, _, _ = train_test_split(\n",
    " X_text_counts, y_text, test_size=0.2, random_state=42, stratify=y_text\n",
    ")\n",
    "\n",
    "# Train Multinomial Naive Bayes with different alpha values\n",
    "alpha_values = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "mnb_results = {}\n",
    "\n",
    "print(f\"\\n Alpha Parameter Optimization:\")\n",
    "\n",
    "for alpha in alpha_values:\n",
    " mnb = MultinomialNB(alpha=alpha)\n",
    " mnb.fit(X_counts_train, y_text_train)\n",
    "\n",
    " # Cross-validation\n",
    " cv_scores = cross_val_score(mnb, X_counts_train, y_text_train, cv=5)\n",
    " test_score = mnb.score(X_counts_test, y_text_test)\n",
    "\n",
    " mnb_results[alpha] = {\n",
    " 'cv_mean': cv_scores.mean(),\n",
    " 'cv_std': cv_scores.std(),\n",
    " 'test_accuracy': test_score\n",
    " }\n",
    "\n",
    " print(f\"• α={alpha}: Test Acc={test_score:.4f}, CV={cv_scores.mean():.4f}±{cv_scores.std():.4f}\")\n",
    "\n",
    "# Find optimal alpha\n",
    "mnb_df = pd.DataFrame(mnb_results).T\n",
    "optimal_alpha = mnb_df['cv_mean'].idxmax()\n",
    "print(f\"\\n Optimal α: {optimal_alpha}\")\n",
    "\n",
    "# Train final model with optimal alpha\n",
    "mnb_final = MultinomialNB(alpha=optimal_alpha)\n",
    "mnb_final.fit(X_counts_train, y_text_train)\n",
    "\n",
    "# Predictions\n",
    "y_text_pred = mnb_final.predict(X_counts_test)\n",
    "y_text_proba = mnb_final.predict_proba(X_counts_test)\n",
    "\n",
    "text_accuracy = accuracy_score(y_text_test, y_text_pred)\n",
    "print(f\"\\n Final Multinomial Naive Bayes Performance:\")\n",
    "print(f\"• α = {optimal_alpha}\")\n",
    "print(f\"• Test Accuracy: {text_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_text_test, y_text_pred))\n",
    "\n",
    "# Visualize alpha parameter effect\n",
    "fig_alpha = go.Figure()\n",
    "\n",
    "fig_alpha.add_trace(\n",
    " go.Scatter(\n",
    " x=list(alpha_values),\n",
    " y=mnb_df['cv_mean'],\n",
    " mode='lines+markers',\n",
    " name='CV Mean',\n",
    " line=dict(color='blue'),\n",
    " error_y=dict(type='data', array=mnb_df['cv_std'])\n",
    " )\n",
    ")\n",
    "\n",
    "fig_alpha.add_trace(\n",
    " go.Scatter(\n",
    " x=list(alpha_values),\n",
    " y=mnb_df['test_accuracy'],\n",
    " mode='lines+markers',\n",
    " name='Test Accuracy',\n",
    " line=dict(color='red')\n",
    " )\n",
    ")\n",
    "\n",
    "fig_alpha.update_layout(\n",
    " title=\"Multinomial Naive Bayes: Alpha Parameter Effect\",\n",
    " xaxis_title=\"Alpha (Smoothing Parameter)\",\n",
    " yaxis_title=\"Accuracy\",\n",
    " height=500\n",
    ")\n",
    "fig_alpha.show()\n",
    "\n",
    "# Feature importance analysis (top words per topic)\n",
    "print(f\"\\n Top Words per Topic:\")\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "topics = mnb_final.classes_\n",
    "\n",
    "for i, topic in enumerate(topics):\n",
    " # Get log probabilities for this topic\n",
    " log_probs = mnb_final.feature_log_prob_[i]\n",
    "\n",
    " # Get top 10 features\n",
    " top_indices = log_probs.argsort()[-10:][::-1]\n",
    " top_words = [feature_names[idx] for idx in top_indices]\n",
    " top_probs = [np.exp(log_probs[idx]) for idx in top_indices]\n",
    "\n",
    " print(f\"\\n{topic.upper()}:\")\n",
    " for word, prob in zip(top_words, top_probs):\n",
    " print(f\" • {word}: {prob:.4f}\")\n",
    "\n",
    "# Visualize top words for each topic\n",
    "fig_words = make_subplots(\n",
    " rows=2, cols=2,\n",
    " subplot_titles=[topic.title() for topic in topics]\n",
    ")\n",
    "\n",
    "positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "\n",
    "for i, topic in enumerate(topics):\n",
    " row, col = positions[i]\n",
    "\n",
    " log_probs = mnb_final.feature_log_prob_[i]\n",
    " top_indices = log_probs.argsort()[-10:][::-1]\n",
    " top_words = [feature_names[idx] for idx in top_indices]\n",
    " top_probs = [np.exp(log_probs[idx]) for idx in top_indices]\n",
    "\n",
    " fig_words.add_trace(\n",
    " go.Bar(\n",
    " x=top_probs,\n",
    " y=top_words,\n",
    " orientation='h',\n",
    " name=topic,\n",
    " showlegend=False,\n",
    " marker_color=px.colors.qualitative.Set1[i]\n",
    " ),\n",
    " row=row, col=col\n",
    " )\n",
    "\n",
    "fig_words.update_layout(\n",
    " title=\"Top Words by Topic (Multinomial Naive Bayes)\",\n",
    " height=600\n",
    ")\n",
    "fig_words.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_text = confusion_matrix(y_text_test, y_text_pred)\n",
    "\n",
    "fig_cm_text = ff.create_annotated_heatmap(\n",
    " z=cm_text,\n",
    " x=topics,\n",
    " y=topics,\n",
    " annotation_text=cm_text,\n",
    " colorscale='Blues',\n",
    " showscale=True\n",
    ")\n",
    "\n",
    "fig_cm_text.update_layout(\n",
    " title=f\"Multinomial Naive Bayes Confusion Matrix (α={optimal_alpha})\",\n",
    " xaxis_title=\"Predicted Topic\",\n",
    " yaxis_title=\"Actual Topic\",\n",
    " height=500\n",
    ")\n",
    "fig_cm_text.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6199e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. BERNOULLI NAIVE BAYES ANALYSIS (SPAM DETECTION)\n",
    "print(\" 3. BERNOULLI NAIVE BAYES ANALYSIS\")\n",
    "print(\"=\" * 34)\n",
    "\n",
    "# Prepare email data\n",
    "email_features = [col for col in email_df.columns if col != 'is_spam']\n",
    "X_email = email_df[email_features]\n",
    "y_email = email_df['is_spam']\n",
    "\n",
    "print(f\"Email features: {len(email_features)}\")\n",
    "print(f\"Feature sample: {email_features[:5]}\")\n",
    "\n",
    "# Split data\n",
    "X_email_train, X_email_test, y_email_train, y_email_test = train_test_split(\n",
    " X_email, y_email, test_size=0.2, random_state=42, stratify=y_email\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_email_train.shape}\")\n",
    "print(f\"Test set: {X_email_test.shape}\")\n",
    "print(f\"Class distribution: {y_email_train.value_counts().to_dict()}\")\n",
    "\n",
    "# Train Bernoulli Naive Bayes with different alpha values\n",
    "bnb_results = {}\n",
    "\n",
    "print(f\"\\n Alpha Parameter Optimization (Bernoulli NB):\")\n",
    "\n",
    "for alpha in alpha_values:\n",
    " bnb = BernoulliNB(alpha=alpha)\n",
    " bnb.fit(X_email_train, y_email_train)\n",
    "\n",
    " # Cross-validation\n",
    " cv_scores = cross_val_score(bnb, X_email_train, y_email_train, cv=5)\n",
    " test_score = bnb.score(X_email_test, y_email_test)\n",
    "\n",
    " bnb_results[alpha] = {\n",
    " 'cv_mean': cv_scores.mean(),\n",
    " 'cv_std': cv_scores.std(),\n",
    " 'test_accuracy': test_score\n",
    " }\n",
    "\n",
    " print(f\"• α={alpha}: Test Acc={test_score:.4f}, CV={cv_scores.mean():.4f}±{cv_scores.std():.4f}\")\n",
    "\n",
    "# Find optimal alpha\n",
    "bnb_df = pd.DataFrame(bnb_results).T\n",
    "optimal_alpha_bnb = bnb_df['cv_mean'].idxmax()\n",
    "print(f\"\\n Optimal α: {optimal_alpha_bnb}\")\n",
    "\n",
    "# Train final model\n",
    "bnb_final = BernoulliNB(alpha=optimal_alpha_bnb)\n",
    "bnb_final.fit(X_email_train, y_email_train)\n",
    "\n",
    "# Predictions\n",
    "y_email_pred = bnb_final.predict(X_email_test)\n",
    "y_email_proba = bnb_final.predict_proba(X_email_test)\n",
    "\n",
    "email_accuracy = accuracy_score(y_email_test, y_email_pred)\n",
    "print(f\"\\n Final Bernoulli Naive Bayes Performance:\")\n",
    "print(f\"• α = {optimal_alpha_bnb}\")\n",
    "print(f\"• Test Accuracy: {email_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_email_test, y_email_pred, target_names=['Ham', 'Spam']))\n",
    "\n",
    "# Feature importance analysis for spam detection\n",
    "print(f\"\\n Feature Analysis for Spam Detection:\")\n",
    "\n",
    "# Calculate feature importance based on log probability ratios\n",
    "spam_log_probs = bnb_final.feature_log_prob_[1] # Spam class\n",
    "ham_log_probs = bnb_final.feature_log_prob_[0] # Ham class\n",
    "\n",
    "# Log odds ratio\n",
    "log_odds_ratio = spam_log_probs - ham_log_probs\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance = pd.DataFrame({\n",
    " 'feature': email_features,\n",
    " 'log_odds_ratio': log_odds_ratio,\n",
    " 'spam_indicator': log_odds_ratio > 0\n",
    "}).sort_values('log_odds_ratio', key=abs, ascending=False)\n",
    "\n",
    "print(f\"Top 10 Spam Indicators:\")\n",
    "spam_indicators = feature_importance[feature_importance['spam_indicator'] == True].head(10)\n",
    "for _, row in spam_indicators.iterrows():\n",
    " print(f\"• {row['feature']}: {row['log_odds_ratio']:.3f}\")\n",
    "\n",
    "print(f\"\\nTop 10 Ham Indicators:\")\n",
    "ham_indicators = feature_importance[feature_importance['spam_indicator'] == False].head(10)\n",
    "for _, row in ham_indicators.iterrows():\n",
    " print(f\"• {row['feature']}: {row['log_odds_ratio']:.3f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "fig_feat_importance = go.Figure()\n",
    "\n",
    "# Top spam features\n",
    "top_spam = feature_importance[feature_importance['spam_indicator'] == True].head(10)\n",
    "fig_feat_importance.add_trace(\n",
    " go.Bar(\n",
    " x=top_spam['log_odds_ratio'],\n",
    " y=top_spam['feature'],\n",
    " orientation='h',\n",
    " name='Spam Indicators',\n",
    " marker_color='red',\n",
    " opacity=0.7\n",
    " )\n",
    ")\n",
    "\n",
    "# Top ham features\n",
    "top_ham = feature_importance[feature_importance['spam_indicator'] == False].head(10)\n",
    "fig_feat_importance.add_trace(\n",
    " go.Bar(\n",
    " x=top_ham['log_odds_ratio'],\n",
    " y=top_ham['feature'],\n",
    " orientation='h',\n",
    " name='Ham Indicators',\n",
    " marker_color='blue',\n",
    " opacity=0.7\n",
    " )\n",
    ")\n",
    "\n",
    "fig_feat_importance.update_layout(\n",
    " title=\"Feature Importance for Spam Detection (Bernoulli Naive Bayes)\",\n",
    " xaxis_title=\"Log Odds Ratio (Spam vs Ham)\",\n",
    " yaxis_title=\"Features\",\n",
    " height=600\n",
    ")\n",
    "fig_feat_importance.show()\n",
    "\n",
    "# ROC and Precision-Recall curves\n",
    "fpr_email, tpr_email, _ = roc_curve(y_email_test, y_email_proba[:, 1])\n",
    "roc_auc_email = auc(fpr_email, tpr_email)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_email_test, y_email_proba[:, 1])\n",
    "\n",
    "fig_curves = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=['ROC Curve', 'Precision-Recall Curve']\n",
    ")\n",
    "\n",
    "# ROC Curve\n",
    "fig_curves.add_trace(\n",
    " go.Scatter(\n",
    " x=fpr_email,\n",
    " y=tpr_email,\n",
    " mode='lines',\n",
    " name=f'ROC (AUC = {roc_auc_email:.3f})',\n",
    " line=dict(color='blue')\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig_curves.add_trace(\n",
    " go.Scatter(\n",
    " x=[0, 1],\n",
    " y=[0, 1],\n",
    " mode='lines',\n",
    " name='Random',\n",
    " line=dict(color='red', dash='dash'),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Precision-Recall Curve\n",
    "fig_curves.add_trace(\n",
    " go.Scatter(\n",
    " x=recall,\n",
    " y=precision,\n",
    " mode='lines',\n",
    " name='Precision-Recall',\n",
    " line=dict(color='green'),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_curves.update_xaxes(title_text=\"False Positive Rate\", row=1, col=1)\n",
    "fig_curves.update_yaxes(title_text=\"True Positive Rate\", row=1, col=1)\n",
    "fig_curves.update_xaxes(title_text=\"Recall\", row=1, col=2)\n",
    "fig_curves.update_yaxes(title_text=\"Precision\", row=1, col=2)\n",
    "\n",
    "fig_curves.update_layout(\n",
    " title=\"Bernoulli Naive Bayes Performance Curves (Spam Detection)\",\n",
    " height=500\n",
    ")\n",
    "fig_curves.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_email = confusion_matrix(y_email_test, y_email_pred)\n",
    "\n",
    "fig_cm_email = ff.create_annotated_heatmap(\n",
    " z=cm_email,\n",
    " x=['Ham', 'Spam'],\n",
    " y=['Ham', 'Spam'],\n",
    " annotation_text=cm_email,\n",
    " colorscale='Blues',\n",
    " showscale=True\n",
    ")\n",
    "\n",
    "fig_cm_email.update_layout(\n",
    " title=f\"Bernoulli Naive Bayes Confusion Matrix (α={optimal_alpha_bnb})\",\n",
    " xaxis_title=\"Predicted\",\n",
    " yaxis_title=\"Actual\",\n",
    " height=400\n",
    ")\n",
    "fig_cm_email.show()\n",
    "\n",
    "# Prediction confidence analysis\n",
    "print(f\"\\n Prediction Confidence Analysis:\")\n",
    "\n",
    "# Analyze prediction confidence for both classes\n",
    "spam_confidences = y_email_proba[y_email_test == 1][:, 1] # Spam predictions\n",
    "ham_confidences = y_email_proba[y_email_test == 0][:, 0] # Ham predictions\n",
    "\n",
    "print(f\"Spam detection confidence: {spam_confidences.mean():.3f} ± {spam_confidences.std():.3f}\")\n",
    "print(f\"Ham detection confidence: {ham_confidences.mean():.3f} ± {ham_confidences.std():.3f}\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(f\"\\nSample Email Predictions:\")\n",
    "sample_size = min(5, len(X_email_test))\n",
    "for i in range(sample_size):\n",
    " actual = y_email_test.iloc[i]\n",
    " predicted = y_email_pred[i]\n",
    " prob_ham = y_email_proba[i][0]\n",
    " prob_spam = y_email_proba[i][1]\n",
    "\n",
    " actual_label = 'Spam' if actual == 1 else 'Ham'\n",
    " pred_label = 'Spam' if predicted == 1 else 'Ham'\n",
    "\n",
    " print(f\"Email {i+1}: Actual={actual_label}, Predicted={pred_label} \"\n",
    " f\"(P(Ham)={prob_ham:.3f}, P(Spam)={prob_spam:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. NAIVE BAYES VARIANTS COMPARISON\n",
    "print(\" 4. NAIVE BAYES VARIANTS COMPARISON\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "# Compare all three Naive Bayes variants on appropriate datasets\n",
    "print(\"Comparing Naive Bayes variants across different problem types:\")\n",
    "\n",
    "# Create comparison results\n",
    "comparison_results = {\n",
    " 'Gaussian NB (Medical)': {\n",
    " 'dataset': 'Medical Diagnosis',\n",
    " 'accuracy': med_accuracy,\n",
    " 'best_for': 'Continuous features with normal distributions',\n",
    " 'features': 'Age, BMI, Blood Pressure, etc.',\n",
    " 'sample_size': len(X_med_test)\n",
    " },\n",
    " 'Multinomial NB (Text)': {\n",
    " 'dataset': 'Document Classification',\n",
    " 'accuracy': text_accuracy,\n",
    " 'best_for': 'Discrete count features (text, documents)',\n",
    " 'features': 'Word counts, TF-IDF scores',\n",
    " 'sample_size': len(X_text_test)\n",
    " },\n",
    " 'Bernoulli NB (Spam)': {\n",
    " 'dataset': 'Email Spam Detection',\n",
    " 'accuracy': email_accuracy,\n",
    " 'best_for': 'Binary/boolean features',\n",
    " 'features': 'Presence/absence of keywords',\n",
    " 'sample_size': len(X_email_test)\n",
    " }\n",
    "}\n",
    "\n",
    "print(f\"\\n Performance Comparison:\")\n",
    "for variant, results in comparison_results.items():\n",
    " print(f\"\\n{variant}:\")\n",
    " print(f\" • Dataset: {results['dataset']}\")\n",
    " print(f\" • Test Accuracy: {results['accuracy']:.4f}\")\n",
    " print(f\" • Best for: {results['best_for']}\")\n",
    " print(f\" • Feature types: {results['features']}\")\n",
    " print(f\" • Test samples: {results['sample_size']}\")\n",
    "\n",
    "# Visualize comparison\n",
    "variants = list(comparison_results.keys())\n",
    "accuracies = [results['accuracy'] for results in comparison_results.values()]\n",
    "datasets = [results['dataset'] for results in comparison_results.values()]\n",
    "\n",
    "fig_comparison = go.Figure()\n",
    "\n",
    "fig_comparison.add_trace(\n",
    " go.Bar(\n",
    " x=variants,\n",
    " y=accuracies,\n",
    " text=[f\"{acc:.3f}\" for acc in accuracies],\n",
    " textposition='outside',\n",
    " marker_color=['lightblue', 'lightgreen', 'lightcoral'],\n",
    " hovertemplate=\"Variant: %{x}<br>Accuracy: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_comparison.update_layout(\n",
    " title=\"Naive Bayes Variants Performance Comparison\",\n",
    " xaxis_title=\"Naive Bayes Variant\",\n",
    " yaxis_title=\"Test Accuracy\",\n",
    " height=500\n",
    ")\n",
    "fig_comparison.show()\n",
    "\n",
    "# Feature independence analysis\n",
    "print(f\"\\n Feature Independence Analysis:\")\n",
    "\n",
    "# Analyze feature correlations for each dataset\n",
    "print(f\"\\n1. Medical Dataset Feature Correlations:\")\n",
    "medical_corr = X_medical.corr()\n",
    "print(\"Strong correlations (|r| > 0.3):\")\n",
    "for i in range(len(medical_corr.columns)):\n",
    " for j in range(i+1, len(medical_corr.columns)):\n",
    " corr_val = medical_corr.iloc[i, j]\n",
    " if abs(corr_val) > 0.3:\n",
    " print(f\" • {medical_corr.columns[i]} ↔ {medical_corr.columns[j]}: {corr_val:.3f}\")\n",
    "\n",
    "print(f\"\\n2. Email Dataset Feature Correlations:\")\n",
    "email_corr = X_email.corr()\n",
    "print(\"Strong correlations (|r| > 0.3):\")\n",
    "strong_corr_count = 0\n",
    "for i in range(len(email_corr.columns)):\n",
    " for j in range(i+1, len(email_corr.columns)):\n",
    " corr_val = email_corr.iloc[i, j]\n",
    " if abs(corr_val) > 0.3:\n",
    " print(f\" • {email_corr.columns[i]} ↔ {email_corr.columns[j]}: {corr_val:.3f}\")\n",
    " strong_corr_count += 1\n",
    " if strong_corr_count >= 5: # Limit output\n",
    " break\n",
    " if strong_corr_count >= 5:\n",
    " break\n",
    "\n",
    "if strong_corr_count >= 5:\n",
    " print(\" • ... (showing first 5 correlations)\")\n",
    "\n",
    "# Independence assumption violation impact\n",
    "print(f\"\\n3. Independence Assumption Impact:\")\n",
    "print(f\" • Medical data: Some correlations exist (e.g., BMI vs blood pressure)\")\n",
    "print(f\" • Impact: Moderate - correlations are expected in medical data\")\n",
    "print(f\" • Recommendation: Monitor performance; consider feature selection\")\n",
    "print(f\" \")\n",
    "print(f\" • Email data: Binary features may have logical correlations\")\n",
    "print(f\" • Impact: Low - Naive Bayes often robust to moderate violations\")\n",
    "print(f\" • Recommendation: Current performance suggests assumption is reasonable\")\n",
    "\n",
    "# Training speed comparison\n",
    "import time\n",
    "\n",
    "print(f\"\\n Training Speed Comparison:\")\n",
    "\n",
    "training_times = {}\n",
    "\n",
    "# Gaussian NB timing\n",
    "start_time = time.time()\n",
    "gnb_timing = GaussianNB()\n",
    "gnb_timing.fit(X_med_train, y_med_train)\n",
    "training_times['Gaussian NB'] = time.time() - start_time\n",
    "\n",
    "# Multinomial NB timing\n",
    "start_time = time.time()\n",
    "mnb_timing = MultinomialNB(alpha=optimal_alpha)\n",
    "mnb_timing.fit(X_counts_train, y_text_train)\n",
    "training_times['Multinomial NB'] = time.time() - start_time\n",
    "\n",
    "# Bernoulli NB timing\n",
    "start_time = time.time()\n",
    "bnb_timing = BernoulliNB(alpha=optimal_alpha_bnb)\n",
    "bnb_timing.fit(X_email_train, y_email_train)\n",
    "training_times['Bernoulli NB'] = time.time() - start_time\n",
    "\n",
    "for variant, time_taken in training_times.items():\n",
    " print(f\" • {variant}: {time_taken:.4f} seconds\")\n",
    "\n",
    "print(f\"\\nAll Naive Bayes variants demonstrate excellent scalability!\")\n",
    "\n",
    "# Memory usage estimation\n",
    "print(f\"\\n Memory Usage Characteristics:\")\n",
    "print(f\" • Gaussian NB: Stores mean and variance for each feature-class pair\")\n",
    "print(f\" Memory: O(features × classes) = O({len(medical_features)} × 2) parameters\")\n",
    "print(f\" \")\n",
    "print(f\" • Multinomial NB: Stores probability for each feature-class pair\")\n",
    "print(f\" Memory: O(vocabulary × classes) = O({X_counts_train.shape[1]} × {len(mnb_final.classes_)}) parameters\")\n",
    "print(f\" \")\n",
    "print(f\" • Bernoulli NB: Stores probability for each binary feature-class pair\")\n",
    "print(f\" Memory: O(features × classes) = O({len(email_features)} × 2) parameters\")\n",
    "print(f\" \")\n",
    "print(f\"All variants have minimal memory requirements compared to other algorithms!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fdce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\n",
    "print(\" 5. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\")\n",
    "print(\"=\" * 54)\n",
    "\n",
    "# Probabilistic insights and business applications\n",
    "print(\" Probabilistic Decision Making Insights:\")\n",
    "\n",
    "print(f\"\\n1. MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\" • Medical Diagnosis (Gaussian NB): {med_accuracy:.1%} accuracy\")\n",
    "print(f\" • Document Classification (Multinomial NB): {text_accuracy:.1%} accuracy\")\n",
    "print(f\" • Spam Detection (Bernoulli NB): {email_accuracy:.1%} accuracy\")\n",
    "\n",
    "# ROI Analysis for each application\n",
    "print(f\"\\n2. ROI ANALYSIS BY APPLICATION:\")\n",
    "\n",
    "# Medical diagnosis ROI\n",
    "medical_volume = 1000 # Daily patients\n",
    "misdiagnosis_cost = 5000 # Cost of misdiagnosis\n",
    "screening_cost = 50 # Cost per automated screening\n",
    "manual_diagnosis_cost = 200 # Cost per manual diagnosis\n",
    "\n",
    "automated_daily_cost = medical_volume * screening_cost\n",
    "manual_daily_cost = medical_volume * manual_diagnosis_cost\n",
    "daily_savings = manual_daily_cost - automated_daily_cost\n",
    "\n",
    "# Assume 5% of high-risk patients would have serious conditions if missed\n",
    "high_risk_patients = medical_volume * (medical_df['health_risk'].mean())\n",
    "false_negative_rate = 1 - med_accuracy # Simplified\n",
    "potential_misdiagnosis_cost = high_risk_patients * false_negative_rate * misdiagnosis_cost * 0.05\n",
    "\n",
    "print(f\"\\n Medical Diagnosis System:\")\n",
    "print(f\" • Daily patient volume: {medical_volume:,}\")\n",
    "print(f\" • Automated screening cost: ${automated_daily_cost:,}/day\")\n",
    "print(f\" • Manual diagnosis cost: ${manual_daily_cost:,}/day\")\n",
    "print(f\" • Daily cost savings: ${daily_savings:,}\")\n",
    "print(f\" • Annual savings: ${daily_savings * 365:,}\")\n",
    "print(f\" • Risk mitigation: Early detection of {high_risk_patients:.0f} high-risk patients/day\")\n",
    "\n",
    "# Document classification ROI\n",
    "doc_volume = 10000 # Daily documents\n",
    "manual_classification_cost = 2 # Cost per manual classification\n",
    "automated_classification_cost = 0.1 # Cost per automated classification\n",
    "\n",
    "doc_daily_savings = doc_volume * (manual_classification_cost - automated_classification_cost)\n",
    "\n",
    "print(f\"\\n Document Classification System:\")\n",
    "print(f\" • Daily document volume: {doc_volume:,}\")\n",
    "print(f\" • Cost savings per document: ${manual_classification_cost - automated_classification_cost:.2f}\")\n",
    "print(f\" • Daily cost savings: ${doc_daily_savings:,}\")\n",
    "print(f\" • Annual savings: ${doc_daily_savings * 365:,}\")\n",
    "print(f\" • Accuracy: {text_accuracy:.1%} automated classification\")\n",
    "\n",
    "# Spam detection ROI\n",
    "email_volume = 100000 # Daily emails\n",
    "spam_rate = 0.4 # 40% spam\n",
    "false_positive_cost = 10 # Cost of blocking legitimate email\n",
    "false_negative_cost = 1 # Cost of letting spam through\n",
    "manual_review_cost = 0.5 # Cost per manual review\n",
    "\n",
    "spam_emails = email_volume * spam_rate\n",
    "ham_emails = email_volume * (1 - spam_rate)\n",
    "\n",
    "# Calculate costs with current system\n",
    "fp_rate = cm_email[0][1] / (cm_email[0][0] + cm_email[0][1]) # Ham classified as spam\n",
    "fn_rate = cm_email[1][0] / (cm_email[1][0] + cm_email[1][1]) # Spam classified as ham\n",
    "\n",
    "daily_fp_cost = ham_emails * fp_rate * false_positive_cost\n",
    "daily_fn_cost = spam_emails * fn_rate * false_negative_cost\n",
    "total_daily_cost = daily_fp_cost + daily_fn_cost\n",
    "\n",
    "print(f\"\\n Spam Detection System:\")\n",
    "print(f\" • Daily email volume: {email_volume:,}\")\n",
    "print(f\" • Spam rate: {spam_rate:.1%}\")\n",
    "print(f\" • False positive cost: ${daily_fp_cost:,.0f}/day\")\n",
    "print(f\" • False negative cost: ${daily_fn_cost:,.0f}/day\")\n",
    "print(f\" • Total error cost: ${total_daily_cost:,.0f}/day\")\n",
    "print(f\" • Annual error cost: ${total_daily_cost * 365:,.0f}\")\n",
    "\n",
    "print(f\"\\n3. STRATEGIC IMPLEMENTATION RECOMMENDATIONS:\")\n",
    "\n",
    "print(f\"\\n Gaussian Naive Bayes (Medical Diagnosis):\")\n",
    "print(f\" • Deploy as preliminary screening tool\")\n",
    "print(f\" • Use probability scores to prioritize urgent cases\")\n",
    "print(f\" • Integrate with electronic health records\")\n",
    "print(f\" • Maintain human oversight for final diagnosis\")\n",
    "print(f\" • Regular model updates with new patient data\")\n",
    "\n",
    "print(f\"\\n Multinomial Naive Bayes (Document Classification):\")\n",
    "print(f\" • Implement for content management systems\")\n",
    "print(f\" • Use for automated news categorization\")\n",
    "print(f\" • Apply to customer support ticket routing\")\n",
    "print(f\" • Enable real-time document processing\")\n",
    "print(f\" • Continuously update vocabulary and categories\")\n",
    "\n",
    "print(f\"\\n Bernoulli Naive Bayes (Spam Detection):\")\n",
    "print(f\" • Deploy as first-line email defense\")\n",
    "print(f\" • Combine with other security measures\")\n",
    "print(f\" • Implement user feedback loop\")\n",
    "print(f\" • Regular feature engineering for new spam patterns\")\n",
    "print(f\" • Monitor false positive rates closely\")\n",
    "\n",
    "print(f\"\\n4. NAIVE BAYES ADVANTAGES:\")\n",
    "print(f\" • Fast training and prediction (real-time capable)\")\n",
    "print(f\" • Minimal memory requirements\")\n",
    "print(f\" • Excellent baseline performance\")\n",
    "print(f\" • Handles multiple classes naturally\")\n",
    "print(f\" • Probabilistic output enables confidence scoring\")\n",
    "print(f\" • Robust to irrelevant features\")\n",
    "print(f\" • Works well with small datasets\")\n",
    "\n",
    "print(f\"\\n5. LIMITATIONS AND MITIGATION:\")\n",
    "print(f\" • Independence assumption rarely holds perfectly\")\n",
    "print(f\" → Monitor feature correlations and performance\")\n",
    "print(f\" • Can be outperformed by more complex models\")\n",
    "print(f\" → Use as baseline; ensemble with other methods\")\n",
    "print(f\" • Sensitive to skewed features\")\n",
    "print(f\" → Apply appropriate preprocessing and smoothing\")\n",
    "print(f\" • Zero probability problem\")\n",
    "print(f\" → Use Laplace smoothing (alpha parameter)\")\n",
    "\n",
    "print(f\"\\n6. MONITORING AND MAINTENANCE:\")\n",
    "print(f\" • Track prediction confidence distributions\")\n",
    "print(f\" • Monitor feature importance changes over time\")\n",
    "print(f\" • Set up automated retraining pipelines\")\n",
    "print(f\" • Implement A/B testing for model updates\")\n",
    "print(f\" • Regular validation against ground truth\")\n",
    "\n",
    "print(f\"\\n7. ADVANCED TECHNIQUES:\")\n",
    "print(f\" • Complement Naive Bayes for dependent features\")\n",
    "print(f\" • Ensemble methods combining multiple NB variants\")\n",
    "print(f\" • Online learning for streaming data\")\n",
    "print(f\" • Feature selection to improve independence\")\n",
    "print(f\" • Calibration for better probability estimates\")\n",
    "\n",
    "print(f\"\\n8. NEXT STEPS:\")\n",
    "print(f\" • Pilot deployment in production environment\")\n",
    "print(f\" • Collect user feedback and performance metrics\")\n",
    "print(f\" • Experiment with feature engineering\")\n",
    "print(f\" • Compare against ensemble methods\")\n",
    "print(f\" • Develop domain-specific variants\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\" NAIVE BAYES LEARNING SUMMARY:\")\n",
    "print(f\" Mastered Bayes' theorem and conditional probability\")\n",
    "print(f\" Applied three NB variants to appropriate problem types\")\n",
    "print(f\" Optimized hyperparameters and analyzed feature importance\")\n",
    "print(f\" Understood independence assumptions and their violations\")\n",
    "print(f\" Analyzed probabilistic outputs and prediction confidence\")\n",
    "print(f\" Generated comprehensive business applications and ROI analysis\")\n",
    "print(f\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}