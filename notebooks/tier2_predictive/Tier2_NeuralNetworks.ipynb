{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 2: Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** fa5b5b11-ba48-4179-a4b0-c9b80d8d3452\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 2: Neural Networks,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** fa5b5b11-ba48-4179-a4b0-c9b80d8d3452\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c2356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries for Neural Networks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Scikit-learn neural networks\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import train_test_split, validation_curve, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "\n",
    "# TensorFlow/Keras (if available)\n",
    "try:\n",
    " import tensorflow as tf\n",
    " from tensorflow.keras.models import Sequential\n",
    " from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    " from tensorflow.keras.optimizers import Adam\n",
    " from tensorflow.keras.callbacks import EarlyStopping\n",
    " TENSORFLOW_AVAILABLE = True\n",
    " print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    " TENSORFLOW_AVAILABLE = False\n",
    " print(\"TensorFlow not available - using scikit-learn MLPClassifier/MLPRegressor\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 2: Neural Networks - Libraries Loaded!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Available Neural Network Techniques:\")\n",
    "print(\"• Multi-Layer Perceptrons (MLP) - Feedforward networks\")\n",
    "print(\"• Activation functions - ReLU, Sigmoid, Tanh comparison\")\n",
    "print(\"• Regularization - Dropout and weight decay\")\n",
    "print(\"• Optimization - Adam, SGD, learning rate scheduling\")\n",
    "if TENSORFLOW_AVAILABLE:\n",
    " print(\"• Deep Learning - TensorFlow/Keras implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd14c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Neural Network Optimized Datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "def create_neural_network_datasets():\n",
    " \"\"\"Create datasets optimized for neural network demonstration\"\"\"\n",
    "\n",
    " # 1. CLASSIFICATION: Customer Churn Prediction (Non-linear patterns)\n",
    " n_customers = 1000\n",
    "\n",
    " # Customer demographics\n",
    " age = np.random.normal(40, 15, n_customers)\n",
    " age = np.clip(age, 18, 80)\n",
    "\n",
    " monthly_charges = np.random.gamma(2, 25, n_customers) + 20\n",
    " monthly_charges = np.clip(monthly_charges, 20, 150)\n",
    "\n",
    " tenure_months = np.random.exponential(12, n_customers) + 1\n",
    " tenure_months = np.clip(tenure_months, 1, 72)\n",
    "\n",
    " # Service usage (creates non-linear relationships)\n",
    " total_charges = monthly_charges * tenure_months + np.random.normal(0, 100, n_customers)\n",
    "\n",
    " # Support interactions\n",
    " support_calls = np.random.poisson(2, n_customers)\n",
    " satisfaction_score = np.random.beta(3, 2, n_customers) * 10\n",
    "\n",
    " # Contract features\n",
    " contract_length = np.random.choice([1, 12, 24], n_customers, p=[0.5, 0.3, 0.2])\n",
    " has_premium = np.random.binomial(1, 0.3, n_customers)\n",
    "\n",
    " # Create complex non-linear churn probability\n",
    " # Neural networks excel at capturing these complex relationships\n",
    " churn_logit = (\n",
    " -0.05 * age +\n",
    " 0.02 * monthly_charges +\n",
    " -0.1 * tenure_months +\n",
    " 0.3 * support_calls +\n",
    " -0.2 * satisfaction_score +\n",
    " -0.1 * contract_length +\n",
    " -0.5 * has_premium +\n",
    " # Non-linear interactions (perfect for neural networks)\n",
    " 0.001 * monthly_charges * support_calls +\n",
    " -0.01 * age * satisfaction_score +\n",
    " 0.005 * monthly_charges**2 / 100 + # Quadratic term\n",
    " np.random.normal(0, 0.5, n_customers)\n",
    " )\n",
    "\n",
    " churn_prob = 1 / (1 + np.exp(-churn_logit))\n",
    " churn = np.random.binomial(1, churn_prob)\n",
    "\n",
    " churn_df = pd.DataFrame({\n",
    " 'age': age,\n",
    " 'monthly_charges': monthly_charges,\n",
    " 'tenure_months': tenure_months,\n",
    " 'total_charges': total_charges,\n",
    " 'support_calls': support_calls,\n",
    " 'satisfaction_score': satisfaction_score,\n",
    " 'contract_length': contract_length,\n",
    " 'has_premium': has_premium,\n",
    " 'churn': churn\n",
    " })\n",
    "\n",
    " # 2. REGRESSION: Energy Consumption Prediction (Complex patterns)\n",
    " n_buildings = 800\n",
    "\n",
    " # Building characteristics\n",
    " square_footage = np.random.gamma(3, 800, n_buildings) + 500\n",
    " num_floors = np.random.poisson(3, n_buildings) + 1\n",
    " num_floors = np.clip(num_floors, 1, 10)\n",
    "\n",
    " building_age = np.random.exponential(15, n_buildings) + 1\n",
    " building_age = np.clip(building_age, 1, 100)\n",
    "\n",
    " # Environmental factors\n",
    " avg_temperature = np.random.normal(70, 10, n_buildings)\n",
    " humidity = np.random.beta(2, 2, n_buildings) * 100\n",
    "\n",
    " # Usage patterns\n",
    " occupancy_rate = np.random.beta(5, 2, n_buildings)\n",
    " hvac_efficiency = np.random.gamma(2, 2, n_buildings) + 1\n",
    "\n",
    " # Generate energy consumption with complex non-linear relationships\n",
    " base_consumption = (\n",
    " 0.05 * square_footage +\n",
    " 200 * num_floors +\n",
    " 10 * building_age +\n",
    " 50 * abs(avg_temperature - 70) + # U-shaped relationship\n",
    " 5 * humidity +\n",
    " 1000 * occupancy_rate +\n",
    " -100 * hvac_efficiency\n",
    " )\n",
    "\n",
    " # Add complex interactions\n",
    " interactions = (\n",
    " 0.001 * square_footage * num_floors + # Size-complexity interaction\n",
    " 2 * building_age * (avg_temperature - 70)**2 + # Age-temperature interaction\n",
    " 500 * occupancy_rate * (1 - 1/hvac_efficiency) + # Usage-efficiency interaction\n",
    " np.sin(humidity / 20) * 100 # Seasonal humidity effect\n",
    " )\n",
    "\n",
    " energy_consumption = base_consumption + interactions + np.random.normal(0, 200, n_buildings)\n",
    " energy_consumption = np.maximum(energy_consumption, 100) # Minimum consumption\n",
    "\n",
    " energy_df = pd.DataFrame({\n",
    " 'square_footage': square_footage,\n",
    " 'num_floors': num_floors,\n",
    " 'building_age': building_age,\n",
    " 'avg_temperature': avg_temperature,\n",
    " 'humidity': humidity,\n",
    " 'occupancy_rate': occupancy_rate,\n",
    " 'hvac_efficiency': hvac_efficiency,\n",
    " 'energy_consumption': energy_consumption\n",
    " })\n",
    "\n",
    " # 3. MULTI-CLASS CLASSIFICATION: Product Category Prediction\n",
    " X_multi, y_multi = make_classification(\n",
    " n_samples=800,\n",
    " n_features=10,\n",
    " n_informative=8,\n",
    " n_redundant=2,\n",
    " n_classes=4,\n",
    " n_clusters_per_class=1,\n",
    " class_sep=0.8,\n",
    " random_state=42\n",
    " )\n",
    "\n",
    " feature_names = [f'feature_{i+1}' for i in range(10)]\n",
    " multiclass_df = pd.DataFrame(X_multi, columns=feature_names)\n",
    " multiclass_df['category'] = y_multi\n",
    "\n",
    " return churn_df, energy_df, multiclass_df\n",
    "\n",
    "churn_df, energy_df, multiclass_df = create_neural_network_datasets()\n",
    "\n",
    "print(\" Neural Network Datasets Created:\")\n",
    "print(f\"Customer Churn: {churn_df.shape} - {churn_df['churn'].mean():.1%} churn rate\")\n",
    "print(f\"Energy Consumption: {energy_df.shape}\")\n",
    "print(f\"Multi-class Classification: {multiclass_df.shape} - {len(multiclass_df['category'].unique())} classes\")\n",
    "print(f\"Energy range: {energy_df['energy_consumption'].min():.0f} - {energy_df['energy_consumption'].max():.0f} kWh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MLP CLASSIFICATION - Customer Churn Prediction\n",
    "print(\" 1. MLP CLASSIFICATION - CUSTOMER CHURN\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Prepare churn data\n",
    "churn_features = ['age', 'monthly_charges', 'tenure_months', 'total_charges',\n",
    " 'support_calls', 'satisfaction_score', 'contract_length', 'has_premium']\n",
    "X_churn = churn_df[churn_features]\n",
    "y_churn = churn_df['churn']\n",
    "\n",
    "# Split and scale data (important for neural networks)\n",
    "X_churn_train, X_churn_test, y_churn_train, y_churn_test = train_test_split(\n",
    " X_churn, y_churn, test_size=0.2, random_state=42, stratify=y_churn\n",
    ")\n",
    "\n",
    "# Scale features for neural networks\n",
    "scaler_churn = StandardScaler()\n",
    "X_churn_train_scaled = scaler_churn.fit_transform(X_churn_train)\n",
    "X_churn_test_scaled = scaler_churn.transform(X_churn_test)\n",
    "\n",
    "print(f\"Training set: {X_churn_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_churn_test_scaled.shape}\")\n",
    "print(f\"Churn distribution: {y_churn_train.value_counts().to_dict()}\")\n",
    "\n",
    "# Train basic MLP\n",
    "mlp_basic = MLPClassifier(\n",
    " hidden_layer_sizes=(100, 50),\n",
    " max_iter=1000,\n",
    " random_state=42,\n",
    " early_stopping=True,\n",
    " validation_fraction=0.1\n",
    ")\n",
    "\n",
    "mlp_basic.fit(X_churn_train_scaled, y_churn_train)\n",
    "churn_accuracy = mlp_basic.score(X_churn_test_scaled, y_churn_test)\n",
    "\n",
    "print(f\"\\n Basic MLP Performance:\")\n",
    "print(f\"• Test Accuracy: {churn_accuracy:.3f}\")\n",
    "print(f\"• Training iterations: {mlp_basic.n_iter_}\")\n",
    "print(f\"• Training loss: {mlp_basic.loss_:.4f}\")\n",
    "\n",
    "# Compare different architectures\n",
    "architectures = [\n",
    " (50,), # Single layer\n",
    " (100, 50), # Two layers\n",
    " (100, 50, 25), # Three layers\n",
    " (200, 100, 50) # Deeper network\n",
    "]\n",
    "\n",
    "arch_results = []\n",
    "for arch in architectures:\n",
    " mlp_temp = MLPClassifier(\n",
    " hidden_layer_sizes=arch,\n",
    " max_iter=1000,\n",
    " random_state=42,\n",
    " early_stopping=True\n",
    " )\n",
    " mlp_temp.fit(X_churn_train_scaled, y_churn_train)\n",
    " test_acc = mlp_temp.score(X_churn_test_scaled, y_churn_test)\n",
    "\n",
    " arch_results.append({\n",
    " 'architecture': str(arch),\n",
    " 'accuracy': test_acc,\n",
    " 'n_iter': mlp_temp.n_iter_\n",
    " })\n",
    "\n",
    "print(f\"\\n Architecture Comparison:\")\n",
    "for result in arch_results:\n",
    " print(f\"• {result['architecture']:15}: {result['accuracy']:.3f} ({result['n_iter']} iterations)\")\n",
    "\n",
    "# Find best architecture\n",
    "best_arch = max(arch_results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\n Best Architecture: {best_arch['architecture']} with {best_arch['accuracy']:.3f} accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada7f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ACTIVATION FUNCTIONS COMPARISON\n",
    "print(\" 2. ACTIVATION FUNCTIONS COMPARISON\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test different activation functions\n",
    "activations = ['relu', 'tanh', 'logistic']\n",
    "activation_results = []\n",
    "\n",
    "for activation in activations:\n",
    " mlp_temp = MLPClassifier(\n",
    " hidden_layer_sizes=(100, 50),\n",
    " activation=activation,\n",
    " max_iter=1000,\n",
    " random_state=42,\n",
    " early_stopping=True\n",
    " )\n",
    " mlp_temp.fit(X_churn_train_scaled, y_churn_train)\n",
    " test_acc = mlp_temp.score(X_churn_test_scaled, y_churn_test)\n",
    "\n",
    " activation_results.append({\n",
    " 'activation': activation,\n",
    " 'accuracy': test_acc,\n",
    " 'loss': mlp_temp.loss_,\n",
    " 'n_iter': mlp_temp.n_iter_\n",
    " })\n",
    "\n",
    "print(\"Activation Function Performance:\")\n",
    "for result in activation_results:\n",
    " print(f\"• {result['activation']:8}: {result['accuracy']:.3f} accuracy, {result['loss']:.4f} loss, {result['n_iter']} iterations\")\n",
    "\n",
    "# Learning rate optimization\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.2]\n",
    "lr_results = []\n",
    "\n",
    "print(f\"\\n Learning Rate Optimization:\")\n",
    "for lr in learning_rates:\n",
    " mlp_temp = MLPClassifier(\n",
    " hidden_layer_sizes=(100, 50),\n",
    " learning_rate_init=lr,\n",
    " max_iter=1000,\n",
    " random_state=42,\n",
    " early_stopping=True\n",
    " )\n",
    " mlp_temp.fit(X_churn_train_scaled, y_churn_train)\n",
    " test_acc = mlp_temp.score(X_churn_test_scaled, y_churn_test)\n",
    "\n",
    " print(f\"• lr={lr:5.3f}: {test_acc:.3f} accuracy ({mlp_temp.n_iter_} iterations)\")\n",
    " lr_results.append({'lr': lr, 'accuracy': test_acc})\n",
    "\n",
    "optimal_lr = max(lr_results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\n Optimal Learning Rate: {optimal_lr['lr']} with {optimal_lr['accuracy']:.3f} accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10564068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MLP REGRESSION - Energy Consumption Prediction\n",
    "print(\" 3. MLP REGRESSION - ENERGY CONSUMPTION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Prepare energy data\n",
    "energy_features = ['square_footage', 'num_floors', 'building_age', 'avg_temperature',\n",
    " 'humidity', 'occupancy_rate', 'hvac_efficiency']\n",
    "X_energy = energy_df[energy_features]\n",
    "y_energy = energy_df['energy_consumption']\n",
    "\n",
    "# Split and scale\n",
    "X_energy_train, X_energy_test, y_energy_train, y_energy_test = train_test_split(\n",
    " X_energy, y_energy, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_energy = StandardScaler()\n",
    "X_energy_train_scaled = scaler_energy.fit_transform(X_energy_train)\n",
    "X_energy_test_scaled = scaler_energy.transform(X_energy_test)\n",
    "\n",
    "# Train MLP Regressor\n",
    "mlp_reg = MLPRegressor(\n",
    " hidden_layer_sizes=(100, 50, 25),\n",
    " max_iter=1000,\n",
    " random_state=42,\n",
    " early_stopping=True,\n",
    " validation_fraction=0.1\n",
    ")\n",
    "\n",
    "mlp_reg.fit(X_energy_train_scaled, y_energy_train)\n",
    "\n",
    "# Predictions and metrics\n",
    "y_energy_pred = mlp_reg.predict(X_energy_test_scaled)\n",
    "energy_r2 = r2_score(y_energy_test, y_energy_pred)\n",
    "energy_rmse = np.sqrt(mean_squared_error(y_energy_test, y_energy_pred))\n",
    "\n",
    "print(f\"Energy Consumption Prediction Performance:\")\n",
    "print(f\"• R²: {energy_r2:.3f}\")\n",
    "print(f\"• RMSE: {energy_rmse:.1f} kWh\")\n",
    "print(f\"• Training iterations: {mlp_reg.n_iter_}\")\n",
    "print(f\"• Final loss: {mlp_reg.loss_:.2f}\")\n",
    "\n",
    "# Regularization comparison\n",
    "alpha_values = [0.0001, 0.001, 0.01, 0.1]\n",
    "reg_results = []\n",
    "\n",
    "print(f\"\\n Regularization (Alpha) Analysis:\")\n",
    "for alpha in alpha_values:\n",
    " mlp_temp = MLPRegressor(\n",
    " hidden_layer_sizes=(100, 50),\n",
    " alpha=alpha,\n",
    " max_iter=1000,\n",
    " random_state=42,\n",
    " early_stopping=True\n",
    " )\n",
    " mlp_temp.fit(X_energy_train_scaled, y_energy_train)\n",
    " pred_temp = mlp_temp.predict(X_energy_test_scaled)\n",
    " r2_temp = r2_score(y_energy_test, pred_temp)\n",
    "\n",
    " print(f\"• alpha={alpha:6.4f}: R² = {r2_temp:.3f} ({mlp_temp.n_iter_} iterations)\")\n",
    " reg_results.append({'alpha': alpha, 'r2': r2_temp})\n",
    "\n",
    "optimal_alpha = max(reg_results, key=lambda x: x['r2'])\n",
    "print(f\"\\n Optimal Alpha: {optimal_alpha['alpha']} with R² = {optimal_alpha['r2']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfeebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DEEP LEARNING WITH TENSORFLOW (if available)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    " print(\" 4. DEEP LEARNING WITH TENSORFLOW\")\n",
    " print(\"=\" * 32)\n",
    "\n",
    " # Build a more complex model for churn prediction\n",
    " model = Sequential([\n",
    " Dense(128, activation='relu', input_shape=(X_churn_train_scaled.shape[1],)),\n",
    " Dropout(0.3),\n",
    " BatchNormalization(),\n",
    " Dense(64, activation='relu'),\n",
    " Dropout(0.2),\n",
    " Dense(32, activation='relu'),\n",
    " Dense(1, activation='sigmoid')\n",
    " ])\n",
    "\n",
    " model.compile(\n",
    " optimizer=Adam(learning_rate=0.001),\n",
    " loss='binary_crossentropy',\n",
    " metrics=['accuracy']\n",
    " )\n",
    "\n",
    " print(\"Deep Neural Network Architecture:\")\n",
    " model.summary()\n",
    "\n",
    " # Train with early stopping\n",
    " early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    " history = model.fit(\n",
    " X_churn_train_scaled, y_churn_train,\n",
    " epochs=100,\n",
    " batch_size=32,\n",
    " validation_split=0.2,\n",
    " callbacks=[early_stop],\n",
    " verbose=0\n",
    " )\n",
    "\n",
    " # Evaluate\n",
    " test_loss, test_accuracy = model.evaluate(X_churn_test_scaled, y_churn_test, verbose=0)\n",
    "\n",
    " print(f\"\\n TensorFlow Model Performance:\")\n",
    " print(f\"• Test Accuracy: {test_accuracy:.3f}\")\n",
    " print(f\"• Test Loss: {test_loss:.4f}\")\n",
    " print(f\"• Training epochs: {len(history.history['loss'])}\")\n",
    "\n",
    " # Compare with scikit-learn\n",
    " print(f\"\\n Model Comparison:\")\n",
    " print(f\"• Scikit-learn MLP: {churn_accuracy:.3f}\")\n",
    " print(f\"• TensorFlow Deep NN: {test_accuracy:.3f}\")\n",
    " improvement = (test_accuracy - churn_accuracy) * 100\n",
    " print(f\"• Improvement: {improvement:+.1f} percentage points\")\n",
    "\n",
    "else:\n",
    " print(\" 4. TENSORFLOW NOT AVAILABLE\")\n",
    " print(\"=\" * 28)\n",
    " print(\"Install TensorFlow for advanced deep learning capabilities:\")\n",
    " print(\"pip install tensorflow\")\n",
    " print(\"\\nUsing scikit-learn MLPClassifier for neural network analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\n",
    "print(\" 5. BUSINESS INSIGHTS & ROI ANALYSIS\")\n",
    "print(\"=\" * 36)\n",
    "\n",
    "print(\" NEURAL NETWORKS BUSINESS APPLICATIONS:\")\n",
    "\n",
    "# Customer churn analysis\n",
    "churn_reduction = 0.20 # 20% improvement in churn prediction\n",
    "customer_base = 50000\n",
    "current_churn_rate = churn_df['churn'].mean()\n",
    "avg_customer_value = 1200 # Annual value\n",
    "\n",
    "customers_at_risk = customer_base * current_churn_rate\n",
    "improved_retention = customers_at_risk * churn_reduction\n",
    "retention_value = improved_retention * avg_customer_value\n",
    "\n",
    "print(f\"\\n Customer Churn Prevention ROI:\")\n",
    "print(f\"• Customer base: {customer_base:,}\")\n",
    "print(f\"• Current churn rate: {current_churn_rate:.1%}\")\n",
    "print(f\"• Customers at risk: {customers_at_risk:,.0f}\")\n",
    "print(f\"• Additional retention (20% improvement): {improved_retention:,.0f}\")\n",
    "print(f\"• Annual retention value: ${retention_value:,.0f}\")\n",
    "\n",
    "# Energy optimization value\n",
    "energy_improvement = 0.15 # 15% better prediction accuracy\n",
    "buildings_managed = 1000\n",
    "avg_annual_consumption = energy_df['energy_consumption'].mean() * 12 # Monthly to annual\n",
    "cost_per_kwh = 0.12\n",
    "optimization_savings = 0.10 # 10% energy savings through optimization\n",
    "\n",
    "total_energy_cost = buildings_managed * avg_annual_consumption * cost_per_kwh\n",
    "prediction_improvement_savings = total_energy_cost * optimization_savings * energy_improvement\n",
    "\n",
    "print(f\"\\n Energy Optimization ROI:\")\n",
    "print(f\"• Buildings managed: {buildings_managed:,}\")\n",
    "print(f\"• Avg annual consumption: {avg_annual_consumption:,.0f} kWh\")\n",
    "print(f\"• Total annual energy cost: ${total_energy_cost:,.0f}\")\n",
    "print(f\"• Optimization savings: ${prediction_improvement_savings:,.0f}\")\n",
    "\n",
    "# Implementation costs\n",
    "development_cost = 150000 # Initial development\n",
    "annual_maintenance = 30000 # Ongoing costs\n",
    "\n",
    "total_annual_value = retention_value + prediction_improvement_savings\n",
    "net_roi_year1 = (total_annual_value - development_cost - annual_maintenance) / (development_cost + annual_maintenance) * 100\n",
    "net_roi_ongoing = (total_annual_value - annual_maintenance) / annual_maintenance * 100\n",
    "\n",
    "print(f\"\\n Combined ROI Analysis:\")\n",
    "print(f\"• Total annual value: ${total_annual_value:,.0f}\")\n",
    "print(f\"• Development cost: ${development_cost:,}\")\n",
    "print(f\"• Annual maintenance: ${annual_maintenance:,}\")\n",
    "print(f\"• Year 1 ROI: {net_roi_year1:.0f}%\")\n",
    "print(f\"• Ongoing ROI: {net_roi_ongoing:.0f}%\")\n",
    "\n",
    "print(f\"\\n NEURAL NETWORKS ADVANTAGES:\")\n",
    "print(f\"• Captures complex non-linear relationships\")\n",
    "print(f\"• Automatic feature interaction detection\")\n",
    "print(f\"• Scalable to large datasets\")\n",
    "print(f\"• Universal function approximation\")\n",
    "print(f\"• Handles high-dimensional data well\")\n",
    "\n",
    "print(f\"\\n KEY CONSIDERATIONS:\")\n",
    "print(f\"• Requires large datasets for optimal performance\")\n",
    "print(f\"• Computationally intensive training\")\n",
    "print(f\"• Hyperparameter sensitivity\")\n",
    "print(f\"• Black box (limited interpretability)\")\n",
    "print(f\"• Risk of overfitting without regularization\")\n",
    "\n",
    "print(f\"\\n OPTIMIZATION GUIDELINES:\")\n",
    "print(f\"• Start with 1-2 hidden layers\")\n",
    "print(f\"• Use ReLU activation for hidden layers\")\n",
    "print(f\"• Apply early stopping to prevent overfitting\")\n",
    "print(f\"• Scale/normalize input features\")\n",
    "print(f\"• Use dropout for regularization (0.2-0.5)\")\n",
    "print(f\"• Adam optimizer with lr=0.001 as default\")\n",
    "\n",
    "print(f\"\\n IMPLEMENTATION ROADMAP:\")\n",
    "print(f\"• Phase 1: Pilot with churn prediction (Month 1-3)\")\n",
    "print(f\"• Phase 2: Energy optimization rollout (Month 4-6)\")\n",
    "print(f\"• Phase 3: Advanced deep learning features (Month 7-12)\")\n",
    "print(f\"• Phase 4: Real-time inference deployment\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\" NEURAL NETWORKS LEARNING SUMMARY:\")\n",
    "print(f\" Mastered multi-layer perceptron architecture\")\n",
    "print(f\" Compared activation functions and optimizers\")\n",
    "print(f\" Applied regularization and early stopping\")\n",
    "print(f\" Implemented both classification and regression\")\n",
    "print(f\" Analyzed hyperparameter optimization strategies\")\n",
    "print(f\" Generated comprehensive business ROI analysis\")\n",
    "print(f\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}