{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 2: Ridge & Lasso Regression\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** 56f0a5c5-f6a1-4ea3-ba51-bf3222d84127\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 2: Ridge & Lasso Regression,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** 56f0a5c5-f6a1-4ea3-ba51-bf3222d84127\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f180bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Essential Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Statistical libraries\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 2: Ridge & Lasso Regression - Libraries Loaded Successfully!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Available Regularization Techniques:\")\n",
    "print(\"• Ridge Regression (L2) - Shrinks coefficients toward zero\")\n",
    "print(\"• Lasso Regression (L1) - Automatic feature selection\")\n",
    "print(\"• Elastic Net - Combines L1 + L2 regularization\")\n",
    "print(\"• Cross-Validation - Optimal hyperparameter selection\")\n",
    "print(\"• Regularization Path - Coefficient evolution analysis\")\n",
    "print(\"• Feature Importance - Variable significance ranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Comprehensive Dataset for Regularization Demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_regularization_dataset(n_samples=1000, n_features=50, n_informative=15, noise_level=0.1):\n",
    " \"\"\"\n",
    " Generate dataset with varying feature importance levels for regularization analysis\n",
    " \"\"\"\n",
    "\n",
    " # Create informative features with different importance levels\n",
    " X_informative = np.random.randn(n_samples, n_informative)\n",
    "\n",
    " # True coefficients with decreasing importance\n",
    " true_coefficients = np.array([10, 8, 6, 5, 4, 3, 2, 1.5, 1, 0.8, 0.6, 0.4, 0.3, 0.2, 0.1])\n",
    "\n",
    " # Generate target variable from informative features\n",
    " y_signal = X_informative @ true_coefficients\n",
    "\n",
    " # Add noise features (should be eliminated by Lasso)\n",
    " X_noise = np.random.randn(n_samples, n_features - n_informative)\n",
    "\n",
    " # Combine informative and noise features\n",
    " X = np.hstack([X_informative, X_noise])\n",
    "\n",
    " # Add noise to target\n",
    " noise = np.random.normal(0, noise_level * np.std(y_signal), n_samples)\n",
    " y = y_signal + noise\n",
    "\n",
    " # Create feature names\n",
    " feature_names = ([f'Important_{i+1}' for i in range(n_informative)] +\n",
    " [f'Noise_{i+1}' for i in range(n_features - n_informative)])\n",
    "\n",
    " # Create DataFrame\n",
    " df = pd.DataFrame(X, columns=feature_names)\n",
    " df['target'] = y\n",
    "\n",
    " # Add some business context variables\n",
    " df['marketing_spend'] = df['Important_1'] * 1000 + 5000 + np.random.normal(0, 200, n_samples)\n",
    " df['product_quality'] = df['Important_2'] * 0.5 + 7 + np.random.normal(0, 0.3, n_samples)\n",
    " df['customer_satisfaction'] = df['Important_3'] * 0.3 + 8 + np.random.normal(0, 0.2, n_samples)\n",
    " df['competition_index'] = -df['Important_4'] * 0.2 + 5 + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    " return df, true_coefficients, feature_names\n",
    "\n",
    "# Generate dataset\n",
    "print(\" Generating regularization demonstration dataset...\")\n",
    "df, true_coefficients, feature_names = generate_regularization_dataset()\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Informative Features: {len([f for f in feature_names if 'Important' in f])}\")\n",
    "print(f\"Noise Features: {len([f for f in feature_names if 'Noise' in f])}\")\n",
    "\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(df.head())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATA PREPARATION AND FEATURE ANALYSIS\n",
    "print(\" 1. DATA PREPARATION AND FEATURE ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = [col for col in df.columns if col not in ['target']]\n",
    "X = df[feature_cols].values\n",
    "y = df['target'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features (important for regularization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Analyze feature correlations\n",
    "correlation_matrix = df[feature_cols].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "fig_corr = go.Figure(data=go.Heatmap(\n",
    " z=correlation_matrix.values,\n",
    " x=correlation_matrix.columns,\n",
    " y=correlation_matrix.columns,\n",
    " colorscale='RdBu_r',\n",
    " zmid=0,\n",
    " text=correlation_matrix.round(2).values,\n",
    " texttemplate=\"%{text}\",\n",
    " textfont={\"size\": 8},\n",
    " hoverongaps=False\n",
    "))\n",
    "\n",
    "fig_corr.update_layout(\n",
    " title=\"Feature Correlation Matrix (First 20 Features)\",\n",
    " width=800,\n",
    " height=600,\n",
    " xaxis_title=\"Features\",\n",
    " yaxis_title=\"Features\"\n",
    ")\n",
    "\n",
    "# Show only first 20 features for readability\n",
    "if len(feature_cols) > 20:\n",
    " corr_subset = correlation_matrix.iloc[:20, :20]\n",
    " fig_corr.data[0].z = corr_subset.values\n",
    " fig_corr.data[0].x = corr_subset.columns\n",
    " fig_corr.data[0].y = corr_subset.columns\n",
    " fig_corr.data[0].text = corr_subset.round(2).values\n",
    "\n",
    "fig_corr.show()\n",
    "\n",
    "# Feature importance analysis using basic correlation with target\n",
    "feature_importance = pd.DataFrame({\n",
    " 'feature': feature_cols,\n",
    " 'correlation_with_target': [np.corrcoef(df[col], df['target'])[0,1] for col in feature_cols],\n",
    " 'abs_correlation': [abs(np.corrcoef(df[col], df['target'])[0,1]) for col in feature_cols]\n",
    "})\n",
    "\n",
    "feature_importance = feature_importance.sort_values('abs_correlation', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "fig_importance = go.Figure()\n",
    "\n",
    "# Color by feature type\n",
    "colors = ['red' if 'Important' in feat else 'blue' for feat in feature_importance['feature']]\n",
    "\n",
    "fig_importance.add_trace(\n",
    " go.Bar(\n",
    " x=feature_importance['feature'][:20], # Top 20 features\n",
    " y=feature_importance['abs_correlation'][:20],\n",
    " marker_color=colors[:20],\n",
    " name='Feature Importance',\n",
    " hovertemplate=\"<b>%{x}</b><br>Correlation: %{y:.3f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_importance.update_layout(\n",
    " title=\"Feature Importance: Correlation with Target (Top 20)\",\n",
    " xaxis_title=\"Features\",\n",
    " yaxis_title=\"Absolute Correlation with Target\",\n",
    " height=500,\n",
    " xaxis_tickangle=-45\n",
    ")\n",
    "fig_importance.show()\n",
    "\n",
    "print(\" Feature Analysis Summary:\")\n",
    "print(f\"• Strongest predictor: {feature_importance.iloc[0]['feature']} (r={feature_importance.iloc[0]['correlation_with_target']:.3f})\")\n",
    "print(f\"• Number of features with |r| > 0.1: {(feature_importance['abs_correlation'] > 0.1).sum()}\")\n",
    "print(f\"• Number of features with |r| < 0.05: {(feature_importance['abs_correlation'] < 0.05).sum()}\")\n",
    "\n",
    "# Show true vs discovered important features\n",
    "print(f\"\\n True vs Discovered Important Features:\")\n",
    "important_features = feature_importance[feature_importance['abs_correlation'] > 0.1]['feature'].tolist()\n",
    "true_important = [f for f in feature_cols if 'Important' in f]\n",
    "discovered_important = [f for f in important_features if 'Important' in f]\n",
    "\n",
    "print(f\"• True important features: {len(true_important)}\")\n",
    "print(f\"• Discovered important features: {len(discovered_important)}\")\n",
    "print(f\"• Discovery accuracy: {len(discovered_important)/len(true_important):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BASELINE LINEAR REGRESSION ANALYSIS\n",
    "print(\"\\n 2. BASELINE LINEAR REGRESSION ANALYSIS\")\n",
    "print(\"=\" * 43)\n",
    "\n",
    "# Fit standard linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr.predict(X_train_scaled)\n",
    "y_test_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse_lr = mean_squared_error(y_train, y_train_pred_lr)\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_pred_lr)\n",
    "train_r2_lr = r2_score(y_train, y_train_pred_lr)\n",
    "test_r2_lr = r2_score(y_test, y_test_pred_lr)\n",
    "\n",
    "print(\" Linear Regression Performance:\")\n",
    "print(f\"• Training MSE: {train_mse_lr:.4f}\")\n",
    "print(f\"• Test MSE: {test_mse_lr:.4f}\")\n",
    "print(f\"• Training R²: {train_r2_lr:.4f}\")\n",
    "print(f\"• Test R²: {test_r2_lr:.4f}\")\n",
    "print(f\"• Overfitting indicator: {train_r2_lr - test_r2_lr:.4f}\")\n",
    "\n",
    "# Analyze coefficient magnitudes\n",
    "lr_coefficients = pd.DataFrame({\n",
    " 'feature': feature_cols,\n",
    " 'coefficient': lr.coef_,\n",
    " 'abs_coefficient': np.abs(lr.coef_)\n",
    "})\n",
    "lr_coefficients = lr_coefficients.sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "# Plot coefficient magnitudes\n",
    "fig_coef = go.Figure()\n",
    "\n",
    "colors_coef = ['red' if 'Important' in feat else 'lightblue' for feat in lr_coefficients['feature']]\n",
    "\n",
    "fig_coef.add_trace(\n",
    " go.Bar(\n",
    " x=lr_coefficients['feature'][:20],\n",
    " y=lr_coefficients['abs_coefficient'][:20],\n",
    " marker_color=colors_coef[:20],\n",
    " name='Coefficient Magnitude',\n",
    " hovertemplate=\"<b>%{x}</b><br>|Coefficient|: %{y:.3f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_coef.update_layout(\n",
    " title=\"Linear Regression: Coefficient Magnitudes (Top 20)\",\n",
    " xaxis_title=\"Features\",\n",
    " yaxis_title=\"Absolute Coefficient Value\",\n",
    " height=500,\n",
    " xaxis_tickangle=-45\n",
    ")\n",
    "fig_coef.show()\n",
    "\n",
    "# Check for overfitting indicators\n",
    "print(f\"\\n Overfitting Analysis:\")\n",
    "coefficient_variance = np.var(lr.coef_)\n",
    "large_coefficients = (np.abs(lr.coef_) > 1).sum()\n",
    "print(f\"• Coefficient variance: {coefficient_variance:.4f}\")\n",
    "print(f\"• Number of large coefficients (|coef| > 1): {large_coefficients}\")\n",
    "print(f\"• Max coefficient magnitude: {np.max(np.abs(lr.coef_)):.4f}\")\n",
    "\n",
    "if train_r2_lr - test_r2_lr > 0.1:\n",
    " print(\" High overfitting detected - regularization recommended!\")\n",
    "elif large_coefficients > 10:\n",
    " print(\" Many large coefficients - potential instability!\")\n",
    "else:\n",
    " print(\" Model appears reasonably stable\")\n",
    "\n",
    "# Residual analysis\n",
    "residuals_train = y_train - y_train_pred_lr\n",
    "residuals_test = y_test - y_test_pred_lr\n",
    "\n",
    "# Create residual plots\n",
    "fig_residuals = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=(\"Training Residuals\", \"Test Residuals\")\n",
    ")\n",
    "\n",
    "# Training residuals\n",
    "fig_residuals.add_trace(\n",
    " go.Scatter(\n",
    " x=y_train_pred_lr,\n",
    " y=residuals_train,\n",
    " mode='markers',\n",
    " marker=dict(color='blue', opacity=0.6),\n",
    " name='Training',\n",
    " hovertemplate=\"Predicted: %{x:.2f}<br>Residual: %{y:.2f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Test residuals\n",
    "fig_residuals.add_trace(\n",
    " go.Scatter(\n",
    " x=y_test_pred_lr,\n",
    " y=residuals_test,\n",
    " mode='markers',\n",
    " marker=dict(color='red', opacity=0.6),\n",
    " name='Test',\n",
    " hovertemplate=\"Predicted: %{x:.2f}<br>Residual: %{y:.2f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Add zero lines\n",
    "for col in [1, 2]:\n",
    " fig_residuals.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", row=1, col=col)\n",
    "\n",
    "fig_residuals.update_layout(\n",
    " title=\"Linear Regression: Residual Analysis\",\n",
    " height=400\n",
    ")\n",
    "fig_residuals.show()\n",
    "\n",
    "print(f\"\\n Residual Analysis:\")\n",
    "print(f\"• Training residual std: {np.std(residuals_train):.4f}\")\n",
    "print(f\"• Test residual std: {np.std(residuals_test):.4f}\")\n",
    "print(f\"• Residual correlation: {np.corrcoef(residuals_train, residuals_test)[0,1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceabf65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. RIDGE REGRESSION (L2 REGULARIZATION)\n",
    "print(\"\\n 3. RIDGE REGRESSION (L2 REGULARIZATION)\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Ridge regression with cross-validation for alpha selection\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Define alpha range for Ridge\n",
    "alphas_ridge = np.logspace(-4, 4, 50) # From 0.0001 to 10000\n",
    "\n",
    "# Fit Ridge with cross-validation\n",
    "ridge_cv = RidgeCV(alphas=alphas_ridge, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "optimal_alpha_ridge = ridge_cv.alpha_\n",
    "print(f\" Optimal Ridge alpha: {optimal_alpha_ridge:.6f}\")\n",
    "\n",
    "# Fit Ridge with optimal alpha\n",
    "ridge = Ridge(alpha=optimal_alpha_ridge)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_ridge = ridge.predict(X_train_scaled)\n",
    "y_test_pred_ridge = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse_ridge = mean_squared_error(y_train, y_train_pred_ridge)\n",
    "test_mse_ridge = mean_squared_error(y_test, y_test_pred_ridge)\n",
    "train_r2_ridge = r2_score(y_train, y_train_pred_ridge)\n",
    "test_r2_ridge = r2_score(y_test, y_test_pred_ridge)\n",
    "\n",
    "print(\" Ridge Regression Performance:\")\n",
    "print(f\"• Training MSE: {train_mse_ridge:.4f}\")\n",
    "print(f\"• Test MSE: {test_mse_ridge:.4f}\")\n",
    "print(f\"• Training R²: {train_r2_ridge:.4f}\")\n",
    "print(f\"• Test R²: {test_r2_ridge:.4f}\")\n",
    "print(f\"• Overfitting indicator: {train_r2_ridge - test_r2_ridge:.4f}\")\n",
    "\n",
    "# Compare with linear regression\n",
    "print(f\"\\n Improvement over Linear Regression:\")\n",
    "print(f\"• Test MSE improvement: {((test_mse_lr - test_mse_ridge) / test_mse_lr * 100):.2f}%\")\n",
    "print(f\"• Test R² improvement: {test_r2_ridge - test_r2_lr:.4f}\")\n",
    "print(f\"• Overfitting reduction: {(train_r2_lr - test_r2_lr) - (train_r2_ridge - test_r2_ridge):.4f}\")\n",
    "\n",
    "# Ridge regularization path analysis\n",
    "alphas_path = np.logspace(-4, 4, 100)\n",
    "ridge_coefficients = []\n",
    "ridge_scores = []\n",
    "\n",
    "for alpha in alphas_path:\n",
    " ridge_temp = Ridge(alpha=alpha)\n",
    " ridge_temp.fit(X_train_scaled, y_train)\n",
    " ridge_coefficients.append(ridge_temp.coef_)\n",
    " ridge_scores.append(ridge_temp.score(X_test_scaled, y_test))\n",
    "\n",
    "ridge_coefficients = np.array(ridge_coefficients)\n",
    "\n",
    "# Plot regularization path\n",
    "fig_path = go.Figure()\n",
    "\n",
    "# Plot coefficient paths for important features\n",
    "important_indices = [i for i, name in enumerate(feature_cols) if 'Important' in name][:10]\n",
    "\n",
    "for idx in important_indices:\n",
    " fig_path.add_trace(\n",
    " go.Scatter(\n",
    " x=alphas_path,\n",
    " y=ridge_coefficients[:, idx],\n",
    " mode='lines',\n",
    " name=feature_cols[idx],\n",
    " line=dict(width=2),\n",
    " hovertemplate=f\"<b>{feature_cols[idx]}</b><br>Alpha: %{{x:.6f}}<br>Coefficient: %{{y:.3f}}<extra></extra>\"\n",
    " )\n",
    " )\n",
    "\n",
    "fig_path.update_layout(\n",
    " title=\"Ridge Regression: Regularization Path (Important Features)\",\n",
    " xaxis_title=\"Alpha (log scale)\",\n",
    " yaxis_title=\"Coefficient Value\",\n",
    " xaxis_type=\"log\",\n",
    " height=500,\n",
    " showlegend=True\n",
    ")\n",
    "fig_path.show()\n",
    "\n",
    "# Plot alpha vs model performance\n",
    "fig_alpha = go.Figure()\n",
    "\n",
    "fig_alpha.add_trace(\n",
    " go.Scatter(\n",
    " x=alphas_path,\n",
    " y=ridge_scores,\n",
    " mode='lines',\n",
    " name='Test R² Score',\n",
    " line=dict(color='blue', width=3),\n",
    " hovertemplate=\"Alpha: %{x:.6f}<br>R² Score: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "# Mark optimal alpha\n",
    "fig_alpha.add_vline(\n",
    " x=optimal_alpha_ridge,\n",
    " line_dash=\"dash\",\n",
    " line_color=\"red\",\n",
    " annotation_text=f\"Optimal α = {optimal_alpha_ridge:.6f}\"\n",
    ")\n",
    "\n",
    "fig_alpha.update_layout(\n",
    " title=\"Ridge Regression: Alpha vs Model Performance\",\n",
    " xaxis_title=\"Alpha (log scale)\",\n",
    " yaxis_title=\"Test R² Score\",\n",
    " xaxis_type=\"log\",\n",
    " height=500\n",
    ")\n",
    "fig_alpha.show()\n",
    "\n",
    "# Coefficient comparison\n",
    "ridge_coefficients_df = pd.DataFrame({\n",
    " 'feature': feature_cols,\n",
    " 'linear_coef': lr.coef_,\n",
    " 'ridge_coef': ridge.coef_,\n",
    " 'abs_linear': np.abs(lr.coef_),\n",
    " 'abs_ridge': np.abs(ridge.coef_),\n",
    " 'shrinkage': np.abs(lr.coef_) - np.abs(ridge.coef_)\n",
    "})\n",
    "ridge_coefficients_df = ridge_coefficients_df.sort_values('abs_linear', ascending=False)\n",
    "\n",
    "# Plot coefficient shrinkage\n",
    "fig_shrink = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=(\"Coefficient Comparison\", \"Shrinkage Effect\")\n",
    ")\n",
    "\n",
    "# Coefficient comparison\n",
    "fig_shrink.add_trace(\n",
    " go.Scatter(\n",
    " x=ridge_coefficients_df['abs_linear'][:20],\n",
    " y=ridge_coefficients_df['abs_ridge'][:20],\n",
    " mode='markers',\n",
    " marker=dict(size=8, color='blue'),\n",
    " name='Ridge vs Linear',\n",
    " text=ridge_coefficients_df['feature'][:20],\n",
    " hovertemplate=\"<b>%{text}</b><br>Linear: %{x:.3f}<br>Ridge: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Add diagonal line (no shrinkage)\n",
    "max_coef = max(ridge_coefficients_df['abs_linear'].max(), ridge_coefficients_df['abs_ridge'].max())\n",
    "fig_shrink.add_trace(\n",
    " go.Scatter(\n",
    " x=[0, max_coef],\n",
    " y=[0, max_coef],\n",
    " mode='lines',\n",
    " line=dict(color='red', dash='dash'),\n",
    " name='No Shrinkage',\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Shrinkage magnitude\n",
    "colors_shrink = ['red' if 'Important' in feat else 'lightblue'\n",
    " for feat in ridge_coefficients_df['feature'][:20]]\n",
    "\n",
    "fig_shrink.add_trace(\n",
    " go.Bar(\n",
    " x=ridge_coefficients_df['feature'][:20],\n",
    " y=ridge_coefficients_df['shrinkage'][:20],\n",
    " marker_color=colors_shrink,\n",
    " name='Shrinkage',\n",
    " showlegend=False,\n",
    " hovertemplate=\"<b>%{x}</b><br>Shrinkage: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_shrink.update_layout(\n",
    " title=\"Ridge Regression: Coefficient Shrinkage Analysis\",\n",
    " height=500\n",
    ")\n",
    "fig_shrink.update_xaxes(title_text=\"Linear Regression |Coef|\", row=1, col=1)\n",
    "fig_shrink.update_yaxes(title_text=\"Ridge Regression |Coef|\", row=1, col=1)\n",
    "fig_shrink.update_xaxes(title_text=\"Features\", row=1, col=2, tickangle=-45)\n",
    "fig_shrink.update_yaxes(title_text=\"Shrinkage Amount\", row=1, col=2)\n",
    "\n",
    "fig_shrink.show()\n",
    "\n",
    "print(f\"\\n Ridge Coefficient Analysis:\")\n",
    "avg_shrinkage = ridge_coefficients_df['shrinkage'].mean()\n",
    "max_shrinkage = ridge_coefficients_df['shrinkage'].max()\n",
    "print(f\"• Average coefficient shrinkage: {avg_shrinkage:.4f}\")\n",
    "print(f\"• Maximum coefficient shrinkage: {max_shrinkage:.4f}\")\n",
    "print(f\"• Coefficient variance reduction: {np.var(lr.coef_) - np.var(ridge.coef_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LASSO REGRESSION (L1 REGULARIZATION)\n",
    "print(\"\\n 4. LASSO REGRESSION (L1 REGULARIZATION)\")\n",
    "print(\"=\" * 43)\n",
    "\n",
    "# Lasso regression with cross-validation for alpha selection\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Define alpha range for Lasso\n",
    "alphas_lasso = np.logspace(-4, 2, 50) # From 0.0001 to 100\n",
    "\n",
    "# Fit Lasso with cross-validation\n",
    "lasso_cv = LassoCV(alphas=alphas_lasso, cv=5, random_state=42, max_iter=2000)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "optimal_alpha_lasso = lasso_cv.alpha_\n",
    "print(f\" Optimal Lasso alpha: {optimal_alpha_lasso:.6f}\")\n",
    "\n",
    "# Fit Lasso with optimal alpha\n",
    "lasso = Lasso(alpha=optimal_alpha_lasso, random_state=42, max_iter=2000)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lasso = lasso.predict(X_train_scaled)\n",
    "y_test_pred_lasso = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse_lasso = mean_squared_error(y_train, y_train_pred_lasso)\n",
    "test_mse_lasso = mean_squared_error(y_test, y_test_pred_lasso)\n",
    "train_r2_lasso = r2_score(y_train, y_train_pred_lasso)\n",
    "test_r2_lasso = r2_score(y_test, y_test_pred_lasso)\n",
    "\n",
    "print(\" Lasso Regression Performance:\")\n",
    "print(f\"• Training MSE: {train_mse_lasso:.4f}\")\n",
    "print(f\"• Test MSE: {test_mse_lasso:.4f}\")\n",
    "print(f\"• Training R²: {train_r2_lasso:.4f}\")\n",
    "print(f\"• Test R²: {test_r2_lasso:.4f}\")\n",
    "print(f\"• Overfitting indicator: {train_r2_lasso - test_r2_lasso:.4f}\")\n",
    "\n",
    "# Feature selection analysis\n",
    "n_selected_features = np.sum(lasso.coef_ != 0)\n",
    "n_zero_coefficients = np.sum(lasso.coef_ == 0)\n",
    "\n",
    "print(f\"\\n Lasso Feature Selection:\")\n",
    "print(f\"• Selected features: {n_selected_features}/{len(feature_cols)}\")\n",
    "print(f\"• Eliminated features: {n_zero_coefficients}/{len(feature_cols)}\")\n",
    "print(f\"• Feature selection rate: {n_zero_coefficients/len(feature_cols):.1%}\")\n",
    "\n",
    "# Analyze which features were selected\n",
    "selected_features = [feature_cols[i] for i, coef in enumerate(lasso.coef_) if coef != 0]\n",
    "eliminated_features = [feature_cols[i] for i, coef in enumerate(lasso.coef_) if coef == 0]\n",
    "\n",
    "selected_important = [f for f in selected_features if 'Important' in f]\n",
    "eliminated_important = [f for f in eliminated_features if 'Important' in f]\n",
    "selected_noise = [f for f in selected_features if 'Noise' in f]\n",
    "eliminated_noise = [f for f in eliminated_features if 'Noise' in f]\n",
    "\n",
    "print(f\"\\n Feature Selection Accuracy:\")\n",
    "print(f\"• Important features selected: {len(selected_important)}/{len([f for f in feature_cols if 'Important' in f])}\")\n",
    "print(f\"• Noise features eliminated: {len(eliminated_noise)}/{len([f for f in feature_cols if 'Noise' in f])}\")\n",
    "print(f\"• Important features incorrectly eliminated: {len(eliminated_important)}\")\n",
    "print(f\"• Noise features incorrectly selected: {len(selected_noise)}\")\n",
    "\n",
    "# Lasso regularization path\n",
    "alphas_lasso_path = np.logspace(-4, 2, 100)\n",
    "lasso_coefficients = []\n",
    "lasso_scores = []\n",
    "n_features_selected = []\n",
    "\n",
    "for alpha in alphas_lasso_path:\n",
    " lasso_temp = Lasso(alpha=alpha, random_state=42, max_iter=2000)\n",
    " lasso_temp.fit(X_train_scaled, y_train)\n",
    " lasso_coefficients.append(lasso_temp.coef_)\n",
    " lasso_scores.append(lasso_temp.score(X_test_scaled, y_test))\n",
    " n_features_selected.append(np.sum(lasso_temp.coef_ != 0))\n",
    "\n",
    "lasso_coefficients = np.array(lasso_coefficients)\n",
    "\n",
    "# Plot Lasso regularization path\n",
    "fig_lasso_path = go.Figure()\n",
    "\n",
    "# Plot coefficient paths for important features\n",
    "for idx in important_indices[:10]:\n",
    " fig_lasso_path.add_trace(\n",
    " go.Scatter(\n",
    " x=alphas_lasso_path,\n",
    " y=lasso_coefficients[:, idx],\n",
    " mode='lines',\n",
    " name=feature_cols[idx],\n",
    " line=dict(width=2),\n",
    " hovertemplate=f\"<b>{feature_cols[idx]}</b><br>Alpha: %{{x:.6f}}<br>Coefficient: %{{y:.3f}}<extra></extra>\"\n",
    " )\n",
    " )\n",
    "\n",
    "fig_lasso_path.update_layout(\n",
    " title=\"Lasso Regression: Regularization Path (Important Features)\",\n",
    " xaxis_title=\"Alpha (log scale)\",\n",
    " yaxis_title=\"Coefficient Value\",\n",
    " xaxis_type=\"log\",\n",
    " height=500,\n",
    " showlegend=True\n",
    ")\n",
    "fig_lasso_path.show()\n",
    "\n",
    "# Plot alpha vs number of features and performance\n",
    "fig_lasso_analysis = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=(\"Alpha vs Performance\", \"Alpha vs Feature Count\")\n",
    ")\n",
    "\n",
    "# Performance vs alpha\n",
    "fig_lasso_analysis.add_trace(\n",
    " go.Scatter(\n",
    " x=alphas_lasso_path,\n",
    " y=lasso_scores,\n",
    " mode='lines',\n",
    " name='Test R² Score',\n",
    " line=dict(color='blue', width=3),\n",
    " hovertemplate=\"Alpha: %{x:.6f}<br>R² Score: %{y:.4f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Mark optimal alpha\n",
    "fig_lasso_analysis.add_vline(\n",
    " x=optimal_alpha_lasso,\n",
    " line_dash=\"dash\",\n",
    " line_color=\"red\",\n",
    " annotation_text=f\"Optimal α = {optimal_alpha_lasso:.6f}\",\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Feature count vs alpha\n",
    "fig_lasso_analysis.add_trace(\n",
    " go.Scatter(\n",
    " x=alphas_lasso_path,\n",
    " y=n_features_selected,\n",
    " mode='lines',\n",
    " name='Selected Features',\n",
    " line=dict(color='green', width=3),\n",
    " hovertemplate=\"Alpha: %{x:.6f}<br>Features: %{y}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_lasso_analysis.add_vline(\n",
    " x=optimal_alpha_lasso,\n",
    " line_dash=\"dash\",\n",
    " line_color=\"red\",\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_lasso_analysis.update_layout(\n",
    " title=\"Lasso Regression: Alpha Analysis\",\n",
    " height=500\n",
    ")\n",
    "fig_lasso_analysis.update_xaxes(title_text=\"Alpha (log scale)\", type=\"log\")\n",
    "fig_lasso_analysis.update_yaxes(title_text=\"Test R² Score\", row=1, col=1)\n",
    "fig_lasso_analysis.update_yaxes(title_text=\"Number of Selected Features\", row=1, col=2)\n",
    "\n",
    "fig_lasso_analysis.show()\n",
    "\n",
    "# Feature selection visualization\n",
    "lasso_coefficients_df = pd.DataFrame({\n",
    " 'feature': feature_cols,\n",
    " 'lasso_coef': lasso.coef_,\n",
    " 'abs_lasso': np.abs(lasso.coef_),\n",
    " 'selected': lasso.coef_ != 0,\n",
    " 'feature_type': ['Important' if 'Important' in f else 'Noise' for f in feature_cols]\n",
    "})\n",
    "\n",
    "# Sort by absolute coefficient value\n",
    "lasso_coefficients_df = lasso_coefficients_df.sort_values('abs_lasso', ascending=False)\n",
    "\n",
    "# Plot selected features\n",
    "selected_df = lasso_coefficients_df[lasso_coefficients_df['selected']].head(20)\n",
    "\n",
    "fig_selected = go.Figure()\n",
    "\n",
    "colors_selected = ['red' if ft == 'Important' else 'lightblue' for ft in selected_df['feature_type']]\n",
    "\n",
    "fig_selected.add_trace(\n",
    " go.Bar(\n",
    " x=selected_df['feature'],\n",
    " y=selected_df['abs_lasso'],\n",
    " marker_color=colors_selected,\n",
    " name='Selected Features',\n",
    " hovertemplate=\"<b>%{x}</b><br>|Coefficient|: %{y:.3f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_selected.update_layout(\n",
    " title=\"Lasso Regression: Selected Features and Coefficients\",\n",
    " xaxis_title=\"Features\",\n",
    " yaxis_title=\"Absolute Coefficient Value\",\n",
    " height=500,\n",
    " xaxis_tickangle=-45\n",
    ")\n",
    "fig_selected.show()\n",
    "\n",
    "print(f\"\\n Lasso vs Other Methods Comparison:\")\n",
    "print(f\"• Lasso vs Linear - Test MSE improvement: {((test_mse_lr - test_mse_lasso) / test_mse_lr * 100):.2f}%\")\n",
    "print(f\"• Lasso vs Ridge - Test MSE: {test_mse_lasso:.4f} vs {test_mse_ridge:.4f}\")\n",
    "print(f\"• Feature reduction: {(1 - n_selected_features/len(feature_cols)):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ELASTIC NET REGRESSION (L1 + L2 REGULARIZATION)\n",
    "print(\"\\n 5. ELASTIC NET REGRESSION (L1 + L2 REGULARIZATION)\")\n",
    "print(\"=\" * 52)\n",
    "\n",
    "# Elastic Net with cross-validation\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# Define parameter ranges\n",
    "l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9] # Mix of L1 and L2\n",
    "alphas_elasticnet = np.logspace(-4, 2, 50)\n",
    "\n",
    "# Fit Elastic Net with cross-validation\n",
    "elasticnet_cv = ElasticNetCV(\n",
    " l1_ratio=l1_ratios,\n",
    " alphas=alphas_elasticnet,\n",
    " cv=5,\n",
    " random_state=42,\n",
    " max_iter=2000\n",
    ")\n",
    "elasticnet_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "optimal_alpha_en = elasticnet_cv.alpha_\n",
    "optimal_l1_ratio_en = elasticnet_cv.l1_ratio_\n",
    "\n",
    "print(f\" Optimal Elastic Net parameters:\")\n",
    "print(f\"• Alpha: {optimal_alpha_en:.6f}\")\n",
    "print(f\"• L1 ratio: {optimal_l1_ratio_en:.3f}\")\n",
    "print(f\"• Effective L1 penalty: {optimal_alpha_en * optimal_l1_ratio_en:.6f}\")\n",
    "print(f\"• Effective L2 penalty: {optimal_alpha_en * (1 - optimal_l1_ratio_en):.6f}\")\n",
    "\n",
    "# Fit Elastic Net with optimal parameters\n",
    "elasticnet = ElasticNet(\n",
    " alpha=optimal_alpha_en,\n",
    " l1_ratio=optimal_l1_ratio_en,\n",
    " random_state=42,\n",
    " max_iter=2000\n",
    ")\n",
    "elasticnet.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_en = elasticnet.predict(X_train_scaled)\n",
    "y_test_pred_en = elasticnet.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse_en = mean_squared_error(y_train, y_train_pred_en)\n",
    "test_mse_en = mean_squared_error(y_test, y_test_pred_en)\n",
    "train_r2_en = r2_score(y_train, y_train_pred_en)\n",
    "test_r2_en = r2_score(y_test, y_test_pred_en)\n",
    "\n",
    "print(\" Elastic Net Performance:\")\n",
    "print(f\"• Training MSE: {train_mse_en:.4f}\")\n",
    "print(f\"• Test MSE: {test_mse_en:.4f}\")\n",
    "print(f\"• Training R²: {train_r2_en:.4f}\")\n",
    "print(f\"• Test R²: {test_r2_en:.4f}\")\n",
    "print(f\"• Overfitting indicator: {train_r2_en - test_r2_en:.4f}\")\n",
    "\n",
    "# Feature selection analysis\n",
    "n_selected_en = np.sum(elasticnet.coef_ != 0)\n",
    "print(f\"\\n Elastic Net Feature Selection:\")\n",
    "print(f\"• Selected features: {n_selected_en}/{len(feature_cols)}\")\n",
    "print(f\"• Feature selection rate: {(1 - n_selected_en/len(feature_cols)):.1%}\")\n",
    "\n",
    "# Compare all methods\n",
    "comparison_df = pd.DataFrame({\n",
    " 'Method': ['Linear Regression', 'Ridge', 'Lasso', 'Elastic Net'],\n",
    " 'Train_MSE': [train_mse_lr, train_mse_ridge, train_mse_lasso, train_mse_en],\n",
    " 'Test_MSE': [test_mse_lr, test_mse_ridge, test_mse_lasso, test_mse_en],\n",
    " 'Train_R2': [train_r2_lr, train_r2_ridge, train_r2_lasso, train_r2_en],\n",
    " 'Test_R2': [test_r2_lr, test_r2_ridge, test_r2_lasso, test_r2_en],\n",
    " 'Overfitting': [train_r2_lr - test_r2_lr, train_r2_ridge - test_r2_ridge,\n",
    " train_r2_lasso - test_r2_lasso, train_r2_en - test_r2_en],\n",
    " 'Features_Used': [len(feature_cols), len(feature_cols), n_selected_features, n_selected_en]\n",
    "})\n",
    "\n",
    "print(f\"\\n MODEL COMPARISON SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Visualize model comparison\n",
    "fig_comparison = make_subplots(\n",
    " rows=2, cols=2,\n",
    " subplot_titles=(\"Test Performance\", \"Overfitting Analysis\",\n",
    " \"Feature Usage\", \"Train vs Test R²\")\n",
    ")\n",
    "\n",
    "# Test performance\n",
    "fig_comparison.add_trace(\n",
    " go.Bar(\n",
    " x=comparison_df['Method'],\n",
    " y=comparison_df['Test_R2'],\n",
    " name='Test R²',\n",
    " marker_color=['blue', 'green', 'red', 'purple'],\n",
    " text=comparison_df['Test_R2'].round(3),\n",
    " textposition='auto'\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Overfitting\n",
    "fig_comparison.add_trace(\n",
    " go.Bar(\n",
    " x=comparison_df['Method'],\n",
    " y=comparison_df['Overfitting'],\n",
    " name='Overfitting',\n",
    " marker_color=['blue', 'green', 'red', 'purple'],\n",
    " text=comparison_df['Overfitting'].round(3),\n",
    " textposition='auto'\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Feature usage\n",
    "fig_comparison.add_trace(\n",
    " go.Bar(\n",
    " x=comparison_df['Method'],\n",
    " y=comparison_df['Features_Used'],\n",
    " name='Features Used',\n",
    " marker_color=['blue', 'green', 'red', 'purple'],\n",
    " text=comparison_df['Features_Used'],\n",
    " textposition='auto'\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# Train vs Test R²\n",
    "fig_comparison.add_trace(\n",
    " go.Scatter(\n",
    " x=comparison_df['Train_R2'],\n",
    " y=comparison_df['Test_R2'],\n",
    " mode='markers+text',\n",
    " text=comparison_df['Method'],\n",
    " textposition=\"top center\",\n",
    " marker=dict(size=12, color=['blue', 'green', 'red', 'purple']),\n",
    " name='Methods',\n",
    " hovertemplate=\"<b>%{text}</b><br>Train R²: %{x:.3f}<br>Test R²: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "# Add diagonal line for perfect generalization\n",
    "fig_comparison.add_trace(\n",
    " go.Scatter(\n",
    " x=[0, 1],\n",
    " y=[0, 1],\n",
    " mode='lines',\n",
    " line=dict(color='black', dash='dash'),\n",
    " name='Perfect Generalization',\n",
    " showlegend=False\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "fig_comparison.update_layout(\n",
    " title=\"Regularization Methods: Comprehensive Comparison\",\n",
    " height=700,\n",
    " showlegend=False\n",
    ")\n",
    "\n",
    "fig_comparison.show()\n",
    "\n",
    "# Best method recommendation\n",
    "best_method_idx = comparison_df['Test_R2'].idxmax()\n",
    "best_method = comparison_df.loc[best_method_idx, 'Method']\n",
    "best_r2 = comparison_df.loc[best_method_idx, 'Test_R2']\n",
    "best_overfitting = comparison_df.loc[best_method_idx, 'Overfitting']\n",
    "\n",
    "print(f\"\\n RECOMMENDATION:\")\n",
    "print(f\"• Best performing method: {best_method}\")\n",
    "print(f\"• Test R²: {best_r2:.4f}\")\n",
    "print(f\"• Overfitting level: {best_overfitting:.4f}\")\n",
    "\n",
    "# Additional analysis\n",
    "if best_method == 'Lasso':\n",
    " print(f\"• Features eliminated: {len(feature_cols) - n_selected_features}\")\n",
    " print(\"• Advantage: Automatic feature selection\")\n",
    "elif best_method == 'Ridge':\n",
    " print(\"• Advantage: Handles multicollinearity well\")\n",
    " print(\"• All features retained with shrinkage\")\n",
    "elif best_method == 'Elastic Net':\n",
    " print(f\"• Features eliminated: {len(feature_cols) - n_selected_en}\")\n",
    " print(\"• Advantage: Balanced L1/L2 regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. ADVANCED CROSS-VALIDATION AND HYPERPARAMETER TUNING\n",
    "print(\"\\n 6. ADVANCED CROSS-VALIDATION AND HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.model_selection import validation_curve, learning_curve\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# 6.1 Validation curves for different regularization strengths\n",
    "print(\"6.1 Validation Curves Analysis:\")\n",
    "\n",
    "# Ridge validation curve\n",
    "ridge_alphas = np.logspace(-4, 4, 20)\n",
    "ridge_train_scores, ridge_val_scores = validation_curve(\n",
    " Ridge(), X_train_scaled, y_train,\n",
    " param_name='alpha', param_range=ridge_alphas,\n",
    " cv=5, scoring='r2', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Lasso validation curve\n",
    "lasso_alphas = np.logspace(-4, 2, 20)\n",
    "lasso_train_scores, lasso_val_scores = validation_curve(\n",
    " Lasso(max_iter=2000), X_train_scaled, y_train,\n",
    " param_name='alpha', param_range=lasso_alphas,\n",
    " cv=5, scoring='r2', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Plot validation curves\n",
    "fig_validation = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=(\"Ridge Validation Curve\", \"Lasso Validation Curve\")\n",
    ")\n",
    "\n",
    "# Ridge validation curve\n",
    "ridge_train_mean = np.mean(ridge_train_scores, axis=1)\n",
    "ridge_train_std = np.std(ridge_train_scores, axis=1)\n",
    "ridge_val_mean = np.mean(ridge_val_scores, axis=1)\n",
    "ridge_val_std = np.std(ridge_val_scores, axis=1)\n",
    "\n",
    "fig_validation.add_trace(\n",
    " go.Scatter(\n",
    " x=ridge_alphas,\n",
    " y=ridge_train_mean,\n",
    " mode='lines',\n",
    " name='Training',\n",
    " line=dict(color='blue'),\n",
    " hovertemplate=\"Alpha: %{x:.6f}<br>R²: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig_validation.add_trace(\n",
    " go.Scatter(\n",
    " x=ridge_alphas,\n",
    " y=ridge_val_mean,\n",
    " mode='lines',\n",
    " name='Validation',\n",
    " line=dict(color='red'),\n",
    " hovertemplate=\"Alpha: %{x:.6f}<br>R²: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Lasso validation curve\n",
    "lasso_train_mean = np.mean(lasso_train_scores, axis=1)\n",
    "lasso_val_mean = np.mean(lasso_val_scores, axis=1)\n",
    "\n",
    "fig_validation.add_trace(\n",
    " go.Scatter(\n",
    " x=lasso_alphas,\n",
    " y=lasso_train_mean,\n",
    " mode='lines',\n",
    " name='Training',\n",
    " line=dict(color='blue'),\n",
    " showlegend=False,\n",
    " hovertemplate=\"Alpha: %{x:.6f}<br>R²: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_validation.add_trace(\n",
    " go.Scatter(\n",
    " x=lasso_alphas,\n",
    " y=lasso_val_mean,\n",
    " mode='lines',\n",
    " name='Validation',\n",
    " line=dict(color='red'),\n",
    " showlegend=False,\n",
    " hovertemplate=\"Alpha: %{x:.6f}<br>R²: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_validation.update_layout(\n",
    " title=\"Validation Curves: Optimal Regularization Strength\",\n",
    " height=500\n",
    ")\n",
    "fig_validation.update_xaxes(type=\"log\", title_text=\"Alpha\")\n",
    "fig_validation.update_yaxes(title_text=\"R² Score\")\n",
    "\n",
    "fig_validation.show()\n",
    "\n",
    "# 6.2 Learning curves to assess model complexity\n",
    "print(\"\\n6.2 Learning Curves Analysis:\")\n",
    "\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Linear regression learning curve\n",
    "lr_train_sizes, lr_train_scores, lr_val_scores = learning_curve(\n",
    " LinearRegression(), X_train_scaled, y_train,\n",
    " train_sizes=train_sizes, cv=5, scoring='r2', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Ridge learning curve\n",
    "ridge_train_sizes, ridge_train_scores_lc, ridge_val_scores_lc = learning_curve(\n",
    " Ridge(alpha=optimal_alpha_ridge), X_train_scaled, y_train,\n",
    " train_sizes=train_sizes, cv=5, scoring='r2', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Lasso learning curve\n",
    "lasso_train_sizes, lasso_train_scores_lc, lasso_val_scores_lc = learning_curve(\n",
    " Lasso(alpha=optimal_alpha_lasso, max_iter=2000), X_train_scaled, y_train,\n",
    " train_sizes=train_sizes, cv=5, scoring='r2', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Plot learning curves\n",
    "fig_learning = go.Figure()\n",
    "\n",
    "methods = ['Linear', 'Ridge', 'Lasso']\n",
    "colors = ['blue', 'green', 'red']\n",
    "train_scores_all = [lr_train_scores, ridge_train_scores_lc, lasso_train_scores_lc]\n",
    "val_scores_all = [lr_val_scores, ridge_val_scores_lc, lasso_val_scores_lc]\n",
    "\n",
    "for i, (method, color) in enumerate(zip(methods, colors)):\n",
    " train_mean = np.mean(train_scores_all[i], axis=1)\n",
    " val_mean = np.mean(val_scores_all[i], axis=1)\n",
    "\n",
    " # Training scores\n",
    " fig_learning.add_trace(\n",
    " go.Scatter(\n",
    " x=lr_train_sizes,\n",
    " y=train_mean,\n",
    " mode='lines',\n",
    " name=f'{method} (Train)',\n",
    " line=dict(color=color, dash='solid'),\n",
    " hovertemplate=f\"<b>{method} Training</b><br>Size: %{{x}}<br>R²: %{{y:.3f}}<extra></extra>\"\n",
    " )\n",
    " )\n",
    "\n",
    " # Validation scores\n",
    " fig_learning.add_trace(\n",
    " go.Scatter(\n",
    " x=lr_train_sizes,\n",
    " y=val_mean,\n",
    " mode='lines',\n",
    " name=f'{method} (Val)',\n",
    " line=dict(color=color, dash='dash'),\n",
    " hovertemplate=f\"<b>{method} Validation</b><br>Size: %{{x}}<br>R²: %{{y:.3f}}<extra></extra>\"\n",
    " )\n",
    " )\n",
    "\n",
    "fig_learning.update_layout(\n",
    " title=\"Learning Curves: Model Performance vs Training Set Size\",\n",
    " xaxis_title=\"Training Set Size\",\n",
    " yaxis_title=\"R² Score\",\n",
    " height=500\n",
    ")\n",
    "fig_learning.show()\n",
    "\n",
    "# 6.3 Grid search for optimal hyperparameters\n",
    "print(\"\\n6.3 Grid Search Optimization:\")\n",
    "\n",
    "# Define parameter grids\n",
    "ridge_param_grid = {'alpha': np.logspace(-4, 4, 20)}\n",
    "lasso_param_grid = {'alpha': np.logspace(-4, 2, 20)}\n",
    "elasticnet_param_grid = {\n",
    " 'alpha': np.logspace(-4, 2, 10),\n",
    " 'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "# Grid search for each method\n",
    "ridge_grid = GridSearchCV(Ridge(), ridge_param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "lasso_grid = GridSearchCV(Lasso(max_iter=2000), lasso_param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "elasticnet_grid = GridSearchCV(ElasticNet(max_iter=2000), elasticnet_param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "# Fit grid searches\n",
    "ridge_grid.fit(X_train_scaled, y_train)\n",
    "lasso_grid.fit(X_train_scaled, y_train)\n",
    "elasticnet_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Grid Search Results:\")\n",
    "print(f\"• Ridge - Best alpha: {ridge_grid.best_params_['alpha']:.6f}, CV Score: {ridge_grid.best_score_:.4f}\")\n",
    "print(f\"• Lasso - Best alpha: {lasso_grid.best_params_['alpha']:.6f}, CV Score: {lasso_grid.best_score_:.4f}\")\n",
    "print(f\"• Elastic Net - Best params: α={elasticnet_grid.best_params_['alpha']:.6f}, \"\n",
    " f\"l1_ratio={elasticnet_grid.best_params_['l1_ratio']:.3f}, CV Score: {elasticnet_grid.best_score_:.4f}\")\n",
    "\n",
    "# Compare grid search results\n",
    "grid_comparison = pd.DataFrame({\n",
    " 'Method': ['Ridge (Grid)', 'Lasso (Grid)', 'Elastic Net (Grid)'],\n",
    " 'CV_Score': [ridge_grid.best_score_, lasso_grid.best_score_, elasticnet_grid.best_score_],\n",
    " 'Best_Alpha': [ridge_grid.best_params_['alpha'],\n",
    " lasso_grid.best_params_['alpha'],\n",
    " elasticnet_grid.best_params_['alpha']],\n",
    " 'Test_Score': [ridge_grid.score(X_test_scaled, y_test),\n",
    " lasso_grid.score(X_test_scaled, y_test),\n",
    " elasticnet_grid.score(X_test_scaled, y_test)]\n",
    "})\n",
    "\n",
    "print(f\"\\n Grid Search Performance Summary:\")\n",
    "print(grid_comparison.round(6))\n",
    "\n",
    "# Visualize grid search results\n",
    "fig_grid = go.Figure()\n",
    "\n",
    "fig_grid.add_trace(\n",
    " go.Bar(\n",
    " x=grid_comparison['Method'],\n",
    " y=grid_comparison['Test_Score'],\n",
    " name='Test Score',\n",
    " marker_color=['green', 'red', 'purple'],\n",
    " text=grid_comparison['Test_Score'].round(4),\n",
    " textposition='auto',\n",
    " hovertemplate=\"<b>%{x}</b><br>Test R²: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_grid.update_layout(\n",
    " title=\"Grid Search Results: Test Set Performance\",\n",
    " xaxis_title=\"Method\",\n",
    " yaxis_title=\"Test R² Score\",\n",
    " height=400\n",
    ")\n",
    "fig_grid.show()\n",
    "\n",
    "# Final model selection\n",
    "best_grid_idx = grid_comparison['Test_Score'].idxmax()\n",
    "best_grid_method = grid_comparison.loc[best_grid_idx, 'Method']\n",
    "best_grid_score = grid_comparison.loc[best_grid_idx, 'Test_Score']\n",
    "\n",
    "print(f\"\\n FINAL MODEL RECOMMENDATION:\")\n",
    "print(f\"• Best method: {best_grid_method}\")\n",
    "print(f\"• Test R² score: {best_grid_score:.4f}\")\n",
    "print(f\"• Improvement over linear regression: {((best_grid_score - test_r2_lr) / test_r2_lr * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae9d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. BUSINESS INSIGHTS AND FEATURE IMPORTANCE ANALYSIS\n",
    "print(\"\\n 7. BUSINESS INSIGHTS AND FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "# Use the best performing model for business insights\n",
    "if best_grid_method == 'Ridge (Grid)':\n",
    " best_model = ridge_grid.best_estimator_\n",
    " model_name = \"Ridge Regression\"\n",
    "elif best_grid_method == 'Lasso (Grid)':\n",
    " best_model = lasso_grid.best_estimator_\n",
    " model_name = \"Lasso Regression\"\n",
    "else:\n",
    " best_model = elasticnet_grid.best_estimator_\n",
    " model_name = \"Elastic Net\"\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance_final = pd.DataFrame({\n",
    " 'feature': feature_cols,\n",
    " 'coefficient': best_model.coef_,\n",
    " 'abs_coefficient': np.abs(best_model.coef_),\n",
    " 'selected': best_model.coef_ != 0\n",
    "})\n",
    "\n",
    "# Add business context for key features\n",
    "business_features = ['marketing_spend', 'product_quality', 'customer_satisfaction', 'competition_index']\n",
    "business_importance = feature_importance_final[\n",
    " feature_importance_final['feature'].isin(business_features)\n",
    "].sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(f\" {model_name} - Key Business Insights:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for _, row in business_importance.iterrows():\n",
    " feature = row['feature']\n",
    " coef = row['coefficient']\n",
    "\n",
    " if coef > 0:\n",
    " direction = \"increases\"\n",
    " impact = \"positive\"\n",
    " else:\n",
    " direction = \"decreases\"\n",
    " impact = \"negative\"\n",
    "\n",
    " print(f\"• {feature.replace('_', ' ').title()}: {impact} impact (coef: {coef:.3f})\")\n",
    " print(f\" - 1 unit increase {direction} target by {abs(coef):.3f}\")\n",
    "\n",
    "# Create feature importance visualization for business features\n",
    "fig_business = go.Figure()\n",
    "\n",
    "fig_business.add_trace(\n",
    " go.Bar(\n",
    " x=business_importance['feature'],\n",
    " y=business_importance['coefficient'],\n",
    " marker_color=['red' if c < 0 else 'green' for c in business_importance['coefficient']],\n",
    " name='Coefficient',\n",
    " text=business_importance['coefficient'].round(3),\n",
    " textposition='auto',\n",
    " hovertemplate=\"<b>%{x}</b><br>Coefficient: %{y:.3f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_business.update_layout(\n",
    " title=f\"{model_name}: Business Feature Impact Analysis\",\n",
    " xaxis_title=\"Business Features\",\n",
    " yaxis_title=\"Coefficient Value\",\n",
    " height=400\n",
    ")\n",
    "fig_business.show()\n",
    "\n",
    "# ROI and actionability analysis\n",
    "print(f\"\\n Business ROI Analysis:\")\n",
    "\n",
    "# Simulate business scenarios\n",
    "scenarios = {\n",
    " 'Increase Marketing': {'marketing_spend': 1000, 'others': 0},\n",
    " 'Improve Quality': {'product_quality': 1, 'others': 0},\n",
    " 'Boost Satisfaction': {'customer_satisfaction': 1, 'others': 0},\n",
    " 'Reduce Competition': {'competition_index': -1, 'others': 0}\n",
    "}\n",
    "\n",
    "for scenario_name, changes in scenarios.items():\n",
    " total_impact = 0\n",
    "\n",
    " for feature, change in changes.items():\n",
    " if feature != 'others' and feature in business_features:\n",
    " # Find coefficient for this feature\n",
    " feature_coef = business_importance[\n",
    " business_importance['feature'] == feature\n",
    " ]['coefficient'].iloc[0] if not business_importance[\n",
    " business_importance['feature'] == feature\n",
    " ].empty else 0\n",
    "\n",
    " total_impact += feature_coef * change\n",
    "\n",
    " print(f\"• {scenario_name}: Expected target change = {total_impact:.3f}\")\n",
    "\n",
    "# Model stability analysis\n",
    "print(f\"\\n Model Stability Analysis:\")\n",
    "\n",
    "# Check coefficient stability across CV folds\n",
    "cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "cv_mean = cv_scores.mean()\n",
    "cv_std = cv_scores.std()\n",
    "\n",
    "print(f\"• Cross-validation R² mean: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"• Coefficient of variation: {cv_std/cv_mean:.3f}\")\n",
    "print(f\"• Model stability: {'High' if cv_std < 0.02 else 'Medium' if cv_std < 0.05 else 'Low'}\")\n",
    "\n",
    "# Feature selection stability (for Lasso/Elastic Net)\n",
    "if hasattr(best_model, 'l1_ratio') or 'Lasso' in model_name:\n",
    " selected_features_count = np.sum(best_model.coef_ != 0)\n",
    " selection_rate = selected_features_count / len(feature_cols)\n",
    "\n",
    " print(f\"• Feature selection rate: {selection_rate:.1%}\")\n",
    " print(f\"• Features eliminated: {len(feature_cols) - selected_features_count}\")\n",
    "\n",
    " # Identify most stable features\n",
    " important_selected = [f for f in feature_cols if 'Important' in f and\n",
    " feature_importance_final[feature_importance_final['feature'] == f]['selected'].iloc[0]]\n",
    " noise_eliminated = [f for f in feature_cols if 'Noise' in f and\n",
    " not feature_importance_final[feature_importance_final['feature'] == f]['selected'].iloc[0]]\n",
    "\n",
    " print(f\"• Important features retained: {len(important_selected)}\")\n",
    " print(f\"• Noise features eliminated: {len(noise_eliminated)}\")\n",
    "\n",
    "# Risk assessment\n",
    "print(f\"\\n Model Risk Assessment:\")\n",
    "\n",
    "# Check for potential issues\n",
    "max_coef = np.max(np.abs(best_model.coef_))\n",
    "coef_range = np.max(best_model.coef_) - np.min(best_model.coef_)\n",
    "\n",
    "if max_coef > 10:\n",
    " print(\"• HIGH RISK: Very large coefficients detected\")\n",
    "elif max_coef > 5:\n",
    " print(\"• MEDIUM RISK: Moderately large coefficients\")\n",
    "else:\n",
    " print(\"• LOW RISK: Coefficients well-controlled\")\n",
    "\n",
    "if coef_range > 20:\n",
    " print(\"• HIGH RISK: Very wide coefficient range\")\n",
    "elif coef_range > 10:\n",
    " print(\"• MEDIUM RISK: Moderate coefficient range\")\n",
    "else:\n",
    " print(\"• LOW RISK: Narrow coefficient range\")\n",
    "\n",
    "# Multicollinearity check\n",
    "if 'Ridge' in model_name:\n",
    " print(\"• Ridge handles multicollinearity well\")\n",
    "elif 'Lasso' in model_name:\n",
    " print(\"• Lasso provides automatic feature selection\")\n",
    "else:\n",
    " print(\"• Elastic Net balances feature selection and multicollinearity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da6abb3",
   "metadata": {},
   "source": [
    "# LEARNING SUMMARY: Ridge & Lasso Regression\n",
    "\n",
    "## Key Concepts Mastered\n",
    "\n",
    "### 1. **Regularization Fundamentals**\n",
    "- **Ridge (L2)**: Shrinks coefficients toward zero, handles multicollinearity\n",
    "- **Lasso (L1)**: Forces coefficients to exactly zero, automatic feature selection\n",
    "- **Elastic Net**: Combines L1 + L2 benefits, balanced approach\n",
    "- **Bias-Variance Tradeoff**: Understanding the regularization effect\n",
    "\n",
    "### 2. **Hyperparameter Optimization**\n",
    "- **Cross-Validation**: Robust parameter selection methodology\n",
    "- **Regularization Path**: Understanding coefficient evolution with alpha\n",
    "- **Grid Search**: Systematic hyperparameter optimization\n",
    "- **Validation Curves**: Visualizing model complexity vs performance\n",
    "\n",
    "### 3. **Feature Selection & Interpretation**\n",
    "- **Automatic Selection**: Lasso's ability to eliminate irrelevant features\n",
    "- **Coefficient Stability**: Measuring feature importance reliability\n",
    "- **Business Impact**: Translating coefficients to actionable insights\n",
    "- **Model Interpretability**: Understanding feature contributions\n",
    "\n",
    "## Business Applications\n",
    "\n",
    "### Predictive Modeling\n",
    "- **High-Dimensional Data**: Genomics, text analysis, sensor data\n",
    "- **Feature Engineering**: Automated selection from large feature sets\n",
    "- **Risk Modeling**: Financial credit scoring, insurance pricing\n",
    "- **Marketing Analytics**: Attribution modeling, budget optimization\n",
    "\n",
    "### Decision Support\n",
    "- Regularized models provide:\n",
    " - Stable predictions with many variables\n",
    " - Clear feature importance rankings\n",
    " - Reduced overfitting risk\n",
    " - Interpretable business insights\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Tier 3: Time Series Models** - Apply regularization to temporal data\n",
    "2. **Advanced Regularization** - Group Lasso, adaptive methods\n",
    "3. **Ensemble Methods** - Combine regularized models\n",
    "4. **Deep Learning** - Regularization in neural networks\n",
    "\n",
    "## Pro Tips\n",
    "\n",
    "- **Always scale features** before applying regularization\n",
    "- **Use cross-validation** for reliable hyperparameter selection\n",
    "- **Ridge for multicollinearity**, Lasso for feature selection\n",
    "- **Elastic Net for best of both worlds**\n",
    "- **Monitor coefficient paths** to understand model behavior\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "- **Scale Sensitivity**: Regularization severely affected by feature scales\n",
    "- **Alpha Selection**: Too high causes underfitting, too low allows overfitting\n",
    "- **Feature Leakage**: Ensure temporal validity in time series data\n",
    "- **Interpretation Limits**: Coefficients don't always imply causation\n",
    "- **Stability Issues**: Feature selection can be unstable with small datasets\n",
    "\n",
    "## Advanced Considerations\n",
    "\n",
    "### When to Use Each Method:\n",
    "- **Ridge**: Multicollinear features, all features potentially useful\n",
    "- **Lasso**: Need feature selection, sparse solutions preferred\n",
    "- **Elastic Net**: Grouped variables, balanced selection and shrinkage\n",
    "- **Linear Regression**: Small datasets, well-understood feature relationships\n",
    "\n",
    "### Performance Optimization:\n",
    "- Use warm starts for regularization path computation\n",
    "- Consider coordinate descent algorithms for large datasets\n",
    "- Implement early stopping for convergence efficiency\n",
    "- Use cross-validation strategically to balance accuracy and speed\n",
    "\n",
    "**Remember**: *Regularization is about finding the sweet spot between model complexity and generalization - let the data guide your choice of regularization strength!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}