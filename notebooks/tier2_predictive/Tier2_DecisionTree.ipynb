{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 2: Decision Tree Analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** 7c6dbde2-98b5-42f9-b57e-8a36653ff77c\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 2: Decision Tree Analysis,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** 7c6dbde2-98b5-42f9-b57e-8a36653ff77c\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Essential Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "# Additional imports\n",
    "import scipy.stats as stats\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 2: Decision Tree Analysis - Libraries Loaded Successfully!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Available Decision Tree Techniques:\")\n",
    "print(\"• Decision Tree Regression - Continuous target prediction\")\n",
    "print(\"• Decision Tree Classification - Categorical target prediction\")\n",
    "print(\"• Tree Pruning - Overfitting prevention strategies\")\n",
    "print(\"• Feature Importance - Variable significance ranking\")\n",
    "print(\"• Tree Visualization - Structure and decision path analysis\")\n",
    "print(\"• Hyperparameter Tuning - Optimal tree complexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1705c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Comprehensive Datasets for Decision Tree Analysis\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_decision_tree_datasets():\n",
    " \"\"\"Generate both regression and classification datasets for tree analysis\"\"\"\n",
    "\n",
    " # 1. REGRESSION DATASET - Business Revenue Prediction\n",
    " n_samples = 1000\n",
    "\n",
    " # Create hierarchical business factors\n",
    " company_size = np.random.choice(['Small', 'Medium', 'Large'], n_samples, p=[0.4, 0.4, 0.2])\n",
    " industry = np.random.choice(['Tech', 'Retail', 'Manufacturing', 'Services'], n_samples, p=[0.3, 0.25, 0.25, 0.2])\n",
    " region = np.random.choice(['North', 'South', 'East', 'West'], n_samples, p=[0.3, 0.25, 0.25, 0.2])\n",
    "\n",
    " # Encode categorical variables\n",
    " le_size = LabelEncoder()\n",
    " le_industry = LabelEncoder()\n",
    " le_region = LabelEncoder()\n",
    "\n",
    " size_encoded = le_size.fit_transform(company_size)\n",
    " industry_encoded = le_industry.fit_transform(industry)\n",
    " region_encoded = le_region.fit_transform(region)\n",
    "\n",
    " # Continuous features\n",
    " marketing_spend = np.random.exponential(scale=5000, size=n_samples) + 1000\n",
    " employee_count = np.random.poisson(lam=50, size=n_samples) + 5\n",
    " years_in_business = np.random.gamma(shape=2, scale=3, size=n_samples) + 1\n",
    " customer_satisfaction = np.random.beta(a=2, b=0.5, size=n_samples) * 10\n",
    "\n",
    " # Create hierarchical decision rules for revenue\n",
    " revenue = np.zeros(n_samples)\n",
    "\n",
    " for i in range(n_samples):\n",
    " base_revenue = 50000\n",
    "\n",
    " # Company size effect\n",
    " if company_size[i] == 'Large':\n",
    " base_revenue *= 3\n",
    " elif company_size[i] == 'Medium':\n",
    " base_revenue *= 1.5\n",
    "\n",
    " # Industry effect\n",
    " if industry[i] == 'Tech':\n",
    " base_revenue *= 2\n",
    " elif industry[i] == 'Manufacturing':\n",
    " base_revenue *= 1.2\n",
    "\n",
    " # Marketing spend effect (non-linear)\n",
    " if marketing_spend[i] > 8000:\n",
    " base_revenue *= 1.5\n",
    " elif marketing_spend[i] > 4000:\n",
    " base_revenue *= 1.2\n",
    "\n",
    " # Employee productivity\n",
    " revenue_per_employee = base_revenue / max(employee_count[i], 1)\n",
    " if revenue_per_employee > 2000:\n",
    " base_revenue *= 1.3\n",
    "\n",
    " # Customer satisfaction threshold\n",
    " if customer_satisfaction[i] > 8:\n",
    " base_revenue *= 1.4\n",
    " elif customer_satisfaction[i] < 5:\n",
    " base_revenue *= 0.7\n",
    "\n",
    " revenue[i] = base_revenue + np.random.normal(0, base_revenue * 0.1)\n",
    "\n",
    " # Create regression DataFrame\n",
    " regression_df = pd.DataFrame({\n",
    " 'company_size': company_size,\n",
    " 'industry': industry,\n",
    " 'region': region,\n",
    " 'marketing_spend': marketing_spend,\n",
    " 'employee_count': employee_count,\n",
    " 'years_in_business': years_in_business,\n",
    " 'customer_satisfaction': customer_satisfaction,\n",
    " 'revenue': revenue\n",
    " })\n",
    "\n",
    " # 2. CLASSIFICATION DATASET - Customer Churn Prediction\n",
    " # Generate features for classification\n",
    " monthly_charges = np.random.gamma(shape=2, scale=30, size=n_samples) + 20\n",
    " tenure_months = np.random.exponential(scale=24, size=n_samples) + 1\n",
    " total_charges = monthly_charges * tenure_months + np.random.normal(0, 100, n_samples)\n",
    " support_calls = np.random.poisson(lam=2, size=n_samples)\n",
    "\n",
    " contract_type = np.random.choice(['Month-to-month', 'One year', 'Two year'],\n",
    " n_samples, p=[0.5, 0.3, 0.2])\n",
    " payment_method = np.random.choice(['Credit card', 'Bank transfer', 'Electronic check', 'Mailed check'],\n",
    " n_samples, p=[0.3, 0.25, 0.25, 0.2])\n",
    "\n",
    " # Create hierarchical churn rules\n",
    " churn_probability = np.zeros(n_samples)\n",
    "\n",
    " for i in range(n_samples):\n",
    " prob = 0.1 # Base churn rate\n",
    "\n",
    " # Tenure effect\n",
    " if tenure_months[i] < 6:\n",
    " prob += 0.4\n",
    " elif tenure_months[i] < 12:\n",
    " prob += 0.2\n",
    "\n",
    " # Contract type effect\n",
    " if contract_type[i] == 'Month-to-month':\n",
    " prob += 0.3\n",
    " elif contract_type[i] == 'One year':\n",
    " prob += 0.1\n",
    "\n",
    " # Support calls effect\n",
    " if support_calls[i] > 3:\n",
    " prob += 0.3\n",
    " elif support_calls[i] > 1:\n",
    " prob += 0.1\n",
    "\n",
    " # Monthly charges effect\n",
    " if monthly_charges[i] > 80:\n",
    " prob += 0.2\n",
    " elif monthly_charges[i] < 30:\n",
    " prob += 0.1\n",
    "\n",
    " churn_probability[i] = min(prob, 0.9)\n",
    "\n",
    " # Generate churn labels\n",
    " churn = np.random.binomial(1, churn_probability, n_samples)\n",
    "\n",
    " # Create classification DataFrame\n",
    " classification_df = pd.DataFrame({\n",
    " 'monthly_charges': monthly_charges,\n",
    " 'tenure_months': tenure_months,\n",
    " 'total_charges': total_charges,\n",
    " 'support_calls': support_calls,\n",
    " 'contract_type': contract_type,\n",
    " 'payment_method': payment_method,\n",
    " 'customer_satisfaction': customer_satisfaction[:n_samples],\n",
    " 'churn': churn\n",
    " })\n",
    "\n",
    " return regression_df, classification_df\n",
    "\n",
    "# Generate datasets\n",
    "print(\" Generating decision tree datasets...\")\n",
    "regression_df, classification_df = generate_decision_tree_datasets()\n",
    "\n",
    "print(f\"Regression Dataset Shape: {regression_df.shape}\")\n",
    "print(f\"Classification Dataset Shape: {classification_df.shape}\")\n",
    "\n",
    "print(\"\\nRegression Dataset (Revenue Prediction):\")\n",
    "print(regression_df.head())\n",
    "print(\"\\nRegression Target Statistics:\")\n",
    "print(regression_df['revenue'].describe())\n",
    "\n",
    "print(\"\\nClassification Dataset (Churn Prediction):\")\n",
    "print(classification_df.head())\n",
    "print(\"\\nChurn Distribution:\")\n",
    "print(classification_df['churn'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3888cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DECISION TREE REGRESSION ANALYSIS\n",
    "print(\" 1. DECISION TREE REGRESSION ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Prepare regression data\n",
    "# Encode categorical variables for regression\n",
    "regression_df_encoded = regression_df.copy()\n",
    "categorical_cols = ['company_size', 'industry', 'region']\n",
    "\n",
    "for col in categorical_cols:\n",
    " le = LabelEncoder()\n",
    " regression_df_encoded[col + '_encoded'] = le.fit_transform(regression_df_encoded[col])\n",
    "\n",
    "# Features and target\n",
    "reg_features = ['company_size_encoded', 'industry_encoded', 'region_encoded',\n",
    " 'marketing_spend', 'employee_count', 'years_in_business', 'customer_satisfaction']\n",
    "X_reg = regression_df_encoded[reg_features]\n",
    "y_reg = regression_df_encoded['revenue']\n",
    "\n",
    "# Split data\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    " X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit decision tree regressor with default parameters\n",
    "dt_reg_default = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg_default.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "# Predictions\n",
    "y_reg_train_pred = dt_reg_default.predict(X_reg_train)\n",
    "y_reg_test_pred = dt_reg_default.predict(X_reg_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse_reg = mean_squared_error(y_reg_train, y_reg_train_pred)\n",
    "test_mse_reg = mean_squared_error(y_reg_test, y_reg_test_pred)\n",
    "train_r2_reg = r2_score(y_reg_train, y_reg_train_pred)\n",
    "test_r2_reg = r2_score(y_reg_test, y_reg_test_pred)\n",
    "\n",
    "print(\" Decision Tree Regression Performance (Default):\")\n",
    "print(f\"• Training MSE: {train_mse_reg:,.0f}\")\n",
    "print(f\"• Test MSE: {test_mse_reg:,.0f}\")\n",
    "print(f\"• Training R²: {train_r2_reg:.4f}\")\n",
    "print(f\"• Test R²: {test_r2_reg:.4f}\")\n",
    "print(f\"• Overfitting indicator: {train_r2_reg - test_r2_reg:.4f}\")\n",
    "\n",
    "# Tree structure analysis\n",
    "print(f\"\\n Tree Structure Analysis:\")\n",
    "print(f\"• Tree depth: {dt_reg_default.get_depth()}\")\n",
    "print(f\"• Number of leaves: {dt_reg_default.get_n_leaves()}\")\n",
    "print(f\"• Total nodes: {dt_reg_default.tree_.node_count}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance_reg = pd.DataFrame({\n",
    " 'feature': reg_features,\n",
    " 'importance': dt_reg_default.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n Feature Importance (Regression):\")\n",
    "for _, row in feature_importance_reg.iterrows():\n",
    " print(f\"• {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "fig_importance_reg = go.Figure()\n",
    "\n",
    "fig_importance_reg.add_trace(\n",
    " go.Bar(\n",
    " x=feature_importance_reg['feature'],\n",
    " y=feature_importance_reg['importance'],\n",
    " marker_color='green',\n",
    " text=feature_importance_reg['importance'].round(3),\n",
    " textposition='auto',\n",
    " hovertemplate=\"<b>%{x}</b><br>Importance: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_importance_reg.update_layout(\n",
    " title=\"Decision Tree Regression: Feature Importance\",\n",
    " xaxis_title=\"Features\",\n",
    " yaxis_title=\"Importance Score\",\n",
    " height=400,\n",
    " xaxis_tickangle=-45\n",
    ")\n",
    "fig_importance_reg.show()\n",
    "\n",
    "# Residual analysis\n",
    "residuals_reg_train = y_reg_train - y_reg_train_pred\n",
    "residuals_reg_test = y_reg_test - y_reg_test_pred\n",
    "\n",
    "fig_residuals_reg = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=(\"Training Residuals\", \"Test Residuals\")\n",
    ")\n",
    "\n",
    "# Training residuals\n",
    "fig_residuals_reg.add_trace(\n",
    " go.Scatter(\n",
    " x=y_reg_train_pred,\n",
    " y=residuals_reg_train,\n",
    " mode='markers',\n",
    " marker=dict(color='blue', opacity=0.6),\n",
    " name='Training',\n",
    " hovertemplate=\"Predicted: %{x:,.0f}<br>Residual: %{y:,.0f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Test residuals\n",
    "fig_residuals_reg.add_trace(\n",
    " go.Scatter(\n",
    " x=y_reg_test_pred,\n",
    " y=residuals_reg_test,\n",
    " mode='markers',\n",
    " marker=dict(color='red', opacity=0.6),\n",
    " name='Test',\n",
    " hovertemplate=\"Predicted: %{x:,.0f}<br>Residual: %{y:,.0f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Add zero lines\n",
    "for col in [1, 2]:\n",
    " fig_residuals_reg.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", row=1, col=col)\n",
    "\n",
    "fig_residuals_reg.update_layout(\n",
    " title=\"Decision Tree Regression: Residual Analysis\",\n",
    " height=400\n",
    ")\n",
    "fig_residuals_reg.show()\n",
    "\n",
    "# Actual vs Predicted\n",
    "fig_pred_reg = go.Figure()\n",
    "\n",
    "# Training predictions\n",
    "fig_pred_reg.add_trace(\n",
    " go.Scatter(\n",
    " x=y_reg_train,\n",
    " y=y_reg_train_pred,\n",
    " mode='markers',\n",
    " marker=dict(color='blue', opacity=0.6),\n",
    " name='Training',\n",
    " hovertemplate=\"Actual: %{x:,.0f}<br>Predicted: %{y:,.0f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "# Test predictions\n",
    "fig_pred_reg.add_trace(\n",
    " go.Scatter(\n",
    " x=y_reg_test,\n",
    " y=y_reg_test_pred,\n",
    " mode='markers',\n",
    " marker=dict(color='red', opacity=0.6),\n",
    " name='Test',\n",
    " hovertemplate=\"Actual: %{x:,.0f}<br>Predicted: %{y:,.0f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "# Perfect prediction line\n",
    "min_val = min(y_reg.min(), dt_reg_default.predict(X_reg).min())\n",
    "max_val = max(y_reg.max(), dt_reg_default.predict(X_reg).max())\n",
    "\n",
    "fig_pred_reg.add_trace(\n",
    " go.Scatter(\n",
    " x=[min_val, max_val],\n",
    " y=[min_val, max_val],\n",
    " mode='lines',\n",
    " line=dict(color='black', dash='dash'),\n",
    " name='Perfect Prediction',\n",
    " showlegend=True\n",
    " )\n",
    ")\n",
    "\n",
    "fig_pred_reg.update_layout(\n",
    " title=\"Decision Tree Regression: Actual vs Predicted\",\n",
    " xaxis_title=\"Actual Revenue\",\n",
    " yaxis_title=\"Predicted Revenue\",\n",
    " height=500\n",
    ")\n",
    "fig_pred_reg.show()\n",
    "\n",
    "if train_r2_reg > 0.95 and test_r2_reg < 0.8:\n",
    " print(\" HIGH OVERFITTING DETECTED - Tree pruning recommended!\")\n",
    "elif train_r2_reg - test_r2_reg > 0.2:\n",
    " print(\" Moderate overfitting - consider reducing tree complexity\")\n",
    "else:\n",
    " print(\" Reasonable model performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DECISION TREE CLASSIFICATION ANALYSIS\n",
    "print(\"\\n 2. DECISION TREE CLASSIFICATION ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Prepare classification data\n",
    "classification_df_encoded = classification_df.copy()\n",
    "categorical_cols_clf = ['contract_type', 'payment_method']\n",
    "\n",
    "for col in categorical_cols_clf:\n",
    " le = LabelEncoder()\n",
    " classification_df_encoded[col + '_encoded'] = le.fit_transform(classification_df_encoded[col])\n",
    "\n",
    "# Features and target\n",
    "clf_features = ['monthly_charges', 'tenure_months', 'total_charges', 'support_calls',\n",
    " 'contract_type_encoded', 'payment_method_encoded', 'customer_satisfaction']\n",
    "X_clf = classification_df_encoded[clf_features]\n",
    "y_clf = classification_df_encoded['churn']\n",
    "\n",
    "# Split data\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(\n",
    " X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n",
    ")\n",
    "\n",
    "# Fit decision tree classifier with default parameters\n",
    "dt_clf_default = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf_default.fit(X_clf_train, y_clf_train)\n",
    "\n",
    "# Predictions\n",
    "y_clf_train_pred = dt_clf_default.predict(X_clf_train)\n",
    "y_clf_test_pred = dt_clf_default.predict(X_clf_test)\n",
    "y_clf_test_proba = dt_clf_default.predict_proba(X_clf_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc_clf = accuracy_score(y_clf_train, y_clf_train_pred)\n",
    "test_acc_clf = accuracy_score(y_clf_test, y_clf_test_pred)\n",
    "\n",
    "print(\" Decision Tree Classification Performance (Default):\")\n",
    "print(f\"• Training Accuracy: {train_acc_clf:.4f}\")\n",
    "print(f\"• Test Accuracy: {test_acc_clf:.4f}\")\n",
    "print(f\"• Overfitting indicator: {train_acc_clf - test_acc_clf:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\n Detailed Classification Report:\")\n",
    "print(classification_report(y_clf_test, y_clf_test_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "# Tree structure analysis\n",
    "print(f\"\\n Tree Structure Analysis (Classification):\")\n",
    "print(f\"• Tree depth: {dt_clf_default.get_depth()}\")\n",
    "print(f\"• Number of leaves: {dt_clf_default.get_n_leaves()}\")\n",
    "print(f\"• Total nodes: {dt_clf_default.tree_.node_count}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance_clf = pd.DataFrame({\n",
    " 'feature': clf_features,\n",
    " 'importance': dt_clf_default.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n Feature Importance (Classification):\")\n",
    "for _, row in feature_importance_clf.iterrows():\n",
    " print(f\"• {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "fig_importance_clf = go.Figure()\n",
    "\n",
    "fig_importance_clf.add_trace(\n",
    " go.Bar(\n",
    " x=feature_importance_clf['feature'],\n",
    " y=feature_importance_clf['importance'],\n",
    " marker_color='red',\n",
    " text=feature_importance_clf['importance'].round(3),\n",
    " textposition='auto',\n",
    " hovertemplate=\"<b>%{x}</b><br>Importance: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_importance_clf.update_layout(\n",
    " title=\"Decision Tree Classification: Feature Importance\",\n",
    " xaxis_title=\"Features\",\n",
    " yaxis_title=\"Importance Score\",\n",
    " height=400,\n",
    " xaxis_tickangle=-45\n",
    ")\n",
    "fig_importance_clf.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_clf_test, y_clf_test_pred)\n",
    "cm_normalized = confusion_matrix(y_clf_test, y_clf_test_pred, normalize='true')\n",
    "\n",
    "fig_cm = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=(\"Confusion Matrix (Counts)\", \"Confusion Matrix (Normalized)\")\n",
    ")\n",
    "\n",
    "# Counts confusion matrix\n",
    "fig_cm.add_trace(\n",
    " go.Heatmap(\n",
    " z=cm,\n",
    " x=['No Churn', 'Churn'],\n",
    " y=['No Churn', 'Churn'],\n",
    " colorscale='Blues',\n",
    " text=cm,\n",
    " texttemplate=\"%{text}\",\n",
    " textfont={\"size\": 16},\n",
    " hoverongaps=False\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Normalized confusion matrix\n",
    "fig_cm.add_trace(\n",
    " go.Heatmap(\n",
    " z=cm_normalized,\n",
    " x=['No Churn', 'Churn'],\n",
    " y=['No Churn', 'Churn'],\n",
    " colorscale='Reds',\n",
    " text=cm_normalized.round(3),\n",
    " texttemplate=\"%{text}\",\n",
    " textfont={\"size\": 16},\n",
    " hoverongaps=False\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_cm.update_layout(\n",
    " title=\"Decision Tree Classification: Confusion Matrix Analysis\",\n",
    " height=400\n",
    ")\n",
    "fig_cm.show()\n",
    "\n",
    "# ROC Curve Analysis\n",
    "fpr, tpr, thresholds = roc_curve(y_clf_test, y_clf_test_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_clf_test, y_clf_test_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "fig_curves = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=(\"ROC Curve\", \"Precision-Recall Curve\")\n",
    ")\n",
    "\n",
    "# ROC Curve\n",
    "fig_curves.add_trace(\n",
    " go.Scatter(\n",
    " x=fpr,\n",
    " y=tpr,\n",
    " mode='lines',\n",
    " name=f'ROC (AUC = {roc_auc:.3f})',\n",
    " line=dict(color='blue', width=3),\n",
    " hovertemplate=\"FPR: %{x:.3f}<br>TPR: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Diagonal line for ROC\n",
    "fig_curves.add_trace(\n",
    " go.Scatter(\n",
    " x=[0, 1],\n",
    " y=[0, 1],\n",
    " mode='lines',\n",
    " line=dict(color='red', dash='dash'),\n",
    " name='Random',\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Precision-Recall Curve\n",
    "fig_curves.add_trace(\n",
    " go.Scatter(\n",
    " x=recall,\n",
    " y=precision,\n",
    " mode='lines',\n",
    " name=f'PR (AUC = {pr_auc:.3f})',\n",
    " line=dict(color='green', width=3),\n",
    " hovertemplate=\"Recall: %{x:.3f}<br>Precision: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_curves.update_layout(\n",
    " title=\"Decision Tree Classification: Performance Curves\",\n",
    " height=400\n",
    ")\n",
    "fig_curves.update_xaxes(title_text=\"False Positive Rate\", row=1, col=1)\n",
    "fig_curves.update_yaxes(title_text=\"True Positive Rate\", row=1, col=1)\n",
    "fig_curves.update_xaxes(title_text=\"Recall\", row=1, col=2)\n",
    "fig_curves.update_yaxes(title_text=\"Precision\", row=1, col=2)\n",
    "\n",
    "fig_curves.show()\n",
    "\n",
    "print(f\"\\n Performance Metrics Summary:\")\n",
    "print(f\"• ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"• Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "print(f\"• Precision (Churn): {precision[precision.shape[0]//2]:.4f}\")\n",
    "print(f\"• Recall (Churn): {recall[recall.shape[0]//2]:.4f}\")\n",
    "\n",
    "if train_acc_clf > 0.95 and test_acc_clf < 0.8:\n",
    " print(\" HIGH OVERFITTING DETECTED - Tree pruning recommended!\")\n",
    "elif train_acc_clf - test_acc_clf > 0.15:\n",
    " print(\" Moderate overfitting - consider reducing tree complexity\")\n",
    "else:\n",
    " print(\" Reasonable classification performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. TREE PRUNING AND HYPERPARAMETER OPTIMIZATION\n",
    "print(\"\\n 3. TREE PRUNING AND HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\" * 52)\n",
    "\n",
    "# 3.1 Regression Tree Optimization\n",
    "print(\"3.1 Regression Tree Optimization:\")\n",
    "\n",
    "# Define parameter grid for regression\n",
    "reg_param_grid = {\n",
    " 'max_depth': [3, 5, 7, 10, 15, None],\n",
    " 'min_samples_split': [2, 5, 10, 20],\n",
    " 'min_samples_leaf': [1, 2, 5, 10],\n",
    " 'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Grid search for regression\n",
    "reg_grid_search = GridSearchCV(\n",
    " DecisionTreeRegressor(random_state=42),\n",
    " reg_param_grid,\n",
    " cv=5,\n",
    " scoring='r2',\n",
    " n_jobs=-1\n",
    ")\n",
    "\n",
    "reg_grid_search.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "print(f\" Best Regression Tree Parameters:\")\n",
    "for param, value in reg_grid_search.best_params_.items():\n",
    " print(f\"• {param}: {value}\")\n",
    "print(f\"• Best CV R²: {reg_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Fit optimized regression tree\n",
    "dt_reg_optimized = reg_grid_search.best_estimator_\n",
    "y_reg_test_pred_opt = dt_reg_optimized.predict(X_reg_test)\n",
    "test_r2_reg_opt = r2_score(y_reg_test, y_reg_test_pred_opt)\n",
    "\n",
    "print(f\"• Test R² (optimized): {test_r2_reg_opt:.4f}\")\n",
    "print(f\"• Improvement: {test_r2_reg_opt - test_r2_reg:.4f}\")\n",
    "\n",
    "# 3.2 Classification Tree Optimization\n",
    "print(f\"\\n3.2 Classification Tree Optimization:\")\n",
    "\n",
    "# Define parameter grid for classification\n",
    "clf_param_grid = {\n",
    " 'max_depth': [3, 5, 7, 10, 15, None],\n",
    " 'min_samples_split': [2, 5, 10, 20],\n",
    " 'min_samples_leaf': [1, 2, 5, 10],\n",
    " 'criterion': ['gini', 'entropy'],\n",
    " 'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Grid search for classification\n",
    "clf_grid_search = GridSearchCV(\n",
    " DecisionTreeClassifier(random_state=42),\n",
    " clf_param_grid,\n",
    " cv=5,\n",
    " scoring='roc_auc',\n",
    " n_jobs=-1\n",
    ")\n",
    "\n",
    "clf_grid_search.fit(X_clf_train, y_clf_train)\n",
    "\n",
    "print(f\" Best Classification Tree Parameters:\")\n",
    "for param, value in clf_grid_search.best_params_.items():\n",
    " print(f\"• {param}: {value}\")\n",
    "print(f\"• Best CV AUC: {clf_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Fit optimized classification tree\n",
    "dt_clf_optimized = clf_grid_search.best_estimator_\n",
    "y_clf_test_pred_opt = dt_clf_optimized.predict(X_clf_test)\n",
    "y_clf_test_proba_opt = dt_clf_optimized.predict_proba(X_clf_test)[:, 1]\n",
    "\n",
    "test_acc_clf_opt = accuracy_score(y_clf_test, y_clf_test_pred_opt)\n",
    "fpr_opt, tpr_opt, _ = roc_curve(y_clf_test, y_clf_test_proba_opt)\n",
    "roc_auc_opt = auc(fpr_opt, tpr_opt)\n",
    "\n",
    "print(f\"• Test Accuracy (optimized): {test_acc_clf_opt:.4f}\")\n",
    "print(f\"• Test AUC (optimized): {roc_auc_opt:.4f}\")\n",
    "print(f\"• Accuracy improvement: {test_acc_clf_opt - test_acc_clf:.4f}\")\n",
    "print(f\"• AUC improvement: {roc_auc_opt - roc_auc:.4f}\")\n",
    "\n",
    "# 3.3 Complexity Analysis\n",
    "print(f\"\\n3.3 Tree Complexity Comparison:\")\n",
    "\n",
    "# Compare tree structures\n",
    "trees_comparison = pd.DataFrame({\n",
    " 'Model': ['Regression (Default)', 'Regression (Optimized)',\n",
    " 'Classification (Default)', 'Classification (Optimized)'],\n",
    " 'Max_Depth': [dt_reg_default.get_depth(), dt_reg_optimized.get_depth(),\n",
    " dt_clf_default.get_depth(), dt_clf_optimized.get_depth()],\n",
    " 'Num_Leaves': [dt_reg_default.get_n_leaves(), dt_reg_optimized.get_n_leaves(),\n",
    " dt_clf_default.get_n_leaves(), dt_clf_optimized.get_n_leaves()],\n",
    " 'Total_Nodes': [dt_reg_default.tree_.node_count, dt_reg_optimized.tree_.node_count,\n",
    " dt_clf_default.tree_.node_count, dt_clf_optimized.tree_.node_count],\n",
    " 'Performance': [test_r2_reg, test_r2_reg_opt, roc_auc, roc_auc_opt]\n",
    "})\n",
    "\n",
    "print(\"Tree Complexity Comparison:\")\n",
    "print(trees_comparison)\n",
    "\n",
    "# Visualize complexity vs performance\n",
    "fig_complexity = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=(\"Tree Depth vs Performance\", \"Number of Leaves vs Performance\")\n",
    ")\n",
    "\n",
    "# Depth vs Performance\n",
    "fig_complexity.add_trace(\n",
    " go.Scatter(\n",
    " x=trees_comparison['Max_Depth'],\n",
    " y=trees_comparison['Performance'],\n",
    " mode='markers+text',\n",
    " text=trees_comparison['Model'],\n",
    " textposition=\"top center\",\n",
    " marker=dict(size=12, color=['blue', 'darkblue', 'red', 'darkred']),\n",
    " name='Models',\n",
    " hovertemplate=\"<b>%{text}</b><br>Depth: %{x}<br>Performance: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Leaves vs Performance\n",
    "fig_complexity.add_trace(\n",
    " go.Scatter(\n",
    " x=trees_comparison['Num_Leaves'],\n",
    " y=trees_comparison['Performance'],\n",
    " mode='markers+text',\n",
    " text=trees_comparison['Model'],\n",
    " textposition=\"top center\",\n",
    " marker=dict(size=12, color=['blue', 'darkblue', 'red', 'darkred']),\n",
    " name='Models',\n",
    " showlegend=False,\n",
    " hovertemplate=\"<b>%{text}</b><br>Leaves: %{x}<br>Performance: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_complexity.update_layout(\n",
    " title=\"Tree Complexity vs Performance Analysis\",\n",
    " height=500\n",
    ")\n",
    "fig_complexity.update_xaxes(title_text=\"Tree Depth\", row=1, col=1)\n",
    "fig_complexity.update_yaxes(title_text=\"Performance Score\", row=1, col=1)\n",
    "fig_complexity.update_xaxes(title_text=\"Number of Leaves\", row=1, col=2)\n",
    "fig_complexity.update_yaxes(title_text=\"Performance Score\", row=1, col=2)\n",
    "\n",
    "fig_complexity.show()\n",
    "\n",
    "# 3.4 Pruning Path Analysis\n",
    "print(f\"\\n3.4 Pruning Path Analysis:\")\n",
    "\n",
    "# Cost complexity pruning for regression\n",
    "path_reg = dt_reg_default.cost_complexity_pruning_path(X_reg_train, y_reg_train)\n",
    "ccp_alphas_reg = path_reg.ccp_alphas\n",
    "impurities_reg = path_reg.impurities\n",
    "\n",
    "# Cost complexity pruning for classification\n",
    "path_clf = dt_clf_default.cost_complexity_pruning_path(X_clf_train, y_clf_train)\n",
    "ccp_alphas_clf = path_clf.ccp_alphas\n",
    "impurities_clf = path_clf.impurities\n",
    "\n",
    "# Train trees with different alpha values (regression)\n",
    "reg_scores_train = []\n",
    "reg_scores_test = []\n",
    "\n",
    "for ccp_alpha in ccp_alphas_reg:\n",
    " dt_reg_temp = DecisionTreeRegressor(random_state=42, ccp_alpha=ccp_alpha)\n",
    " dt_reg_temp.fit(X_reg_train, y_reg_train)\n",
    " reg_scores_train.append(dt_reg_temp.score(X_reg_train, y_reg_train))\n",
    " reg_scores_test.append(dt_reg_temp.score(X_reg_test, y_reg_test))\n",
    "\n",
    "# Train trees with different alpha values (classification)\n",
    "clf_scores_train = []\n",
    "clf_scores_test = []\n",
    "\n",
    "for ccp_alpha in ccp_alphas_clf:\n",
    " dt_clf_temp = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
    " dt_clf_temp.fit(X_clf_train, y_clf_train)\n",
    " clf_scores_train.append(dt_clf_temp.score(X_clf_train, y_clf_train))\n",
    " clf_scores_test.append(dt_clf_temp.score(X_clf_test, y_clf_test))\n",
    "\n",
    "# Plot pruning paths\n",
    "fig_pruning = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=(\"Regression Pruning Path\", \"Classification Pruning Path\")\n",
    ")\n",
    "\n",
    "# Regression pruning\n",
    "fig_pruning.add_trace(\n",
    " go.Scatter(\n",
    " x=ccp_alphas_reg,\n",
    " y=reg_scores_train,\n",
    " mode='lines+markers',\n",
    " name='Training',\n",
    " line=dict(color='blue'),\n",
    " hovertemplate=\"Alpha: %{x:.6f}<br>R²: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig_pruning.add_trace(\n",
    " go.Scatter(\n",
    " x=ccp_alphas_reg,\n",
    " y=reg_scores_test,\n",
    " mode='lines+markers',\n",
    " name='Test',\n",
    " line=dict(color='red'),\n",
    " hovertemplate=\"Alpha: %{x:.6f}<br>R²: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Classification pruning\n",
    "fig_pruning.add_trace(\n",
    " go.Scatter(\n",
    " x=ccp_alphas_clf,\n",
    " y=clf_scores_train,\n",
    " mode='lines+markers',\n",
    " name='Training',\n",
    " line=dict(color='blue'),\n",
    " showlegend=False,\n",
    " hovertemplate=\"Alpha: %{x:.6f}<br>Accuracy: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_pruning.add_trace(\n",
    " go.Scatter(\n",
    " x=ccp_alphas_clf,\n",
    " y=clf_scores_test,\n",
    " mode='lines+markers',\n",
    " name='Test',\n",
    " line=dict(color='red'),\n",
    " showlegend=False,\n",
    " hovertemplate=\"Alpha: %{x:.6f}<br>Accuracy: %{y:.3f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_pruning.update_layout(\n",
    " title=\"Cost Complexity Pruning Analysis\",\n",
    " height=500\n",
    ")\n",
    "fig_pruning.update_xaxes(title_text=\"Alpha\", type=\"log\")\n",
    "fig_pruning.update_yaxes(title_text=\"R² Score\", row=1, col=1)\n",
    "fig_pruning.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "\n",
    "fig_pruning.show()\n",
    "\n",
    "# Find optimal alpha values\n",
    "optimal_alpha_reg = ccp_alphas_reg[np.argmax(reg_scores_test)]\n",
    "optimal_alpha_clf = ccp_alphas_clf[np.argmax(clf_scores_test)]\n",
    "\n",
    "print(f\"• Optimal alpha (regression): {optimal_alpha_reg:.6f}\")\n",
    "print(f\"• Optimal alpha (classification): {optimal_alpha_clf:.6f}\")\n",
    "print(f\"• Max test R² (regression): {max(reg_scores_test):.4f}\")\n",
    "print(f\"• Max test accuracy (classification): {max(clf_scores_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eef289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. TREE VISUALIZATION AND INTERPRETATION\n",
    "print(\"\\n 4. TREE VISUALIZATION AND INTERPRETATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 4.1 Tree Structure Visualization\n",
    "print(\"4.1 Tree Structure Analysis:\")\n",
    "\n",
    "# Create a simple tree for visualization (limited depth)\n",
    "dt_reg_simple = DecisionTreeRegressor(max_depth=3, min_samples_split=20, random_state=42)\n",
    "dt_reg_simple.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "dt_clf_simple = DecisionTreeClassifier(max_depth=3, min_samples_split=20, random_state=42)\n",
    "dt_clf_simple.fit(X_clf_train, y_clf_train)\n",
    "\n",
    "# Text representation of trees\n",
    "print(\" Regression Tree Structure (Depth=3):\")\n",
    "tree_rules_reg = export_text(dt_reg_simple, feature_names=reg_features, max_depth=3)\n",
    "print(tree_rules_reg[:1000] + \"...\" if len(tree_rules_reg) > 1000 else tree_rules_reg)\n",
    "\n",
    "print(\"\\n Classification Tree Structure (Depth=3):\")\n",
    "tree_rules_clf = export_text(dt_clf_simple, feature_names=clf_features, max_depth=3)\n",
    "print(tree_rules_clf[:1000] + \"...\" if len(tree_rules_clf) > 1000 else tree_rules_clf)\n",
    "\n",
    "# 4.2 Decision Boundary Analysis (for 2D visualization)\n",
    "print(f\"\\n4.2 Decision Boundary Analysis:\")\n",
    "\n",
    "# Select two most important features for visualization\n",
    "top_features_reg = feature_importance_reg.head(2)['feature'].tolist()\n",
    "top_features_clf = feature_importance_clf.head(2)['feature'].tolist()\n",
    "\n",
    "# Create 2D datasets\n",
    "X_reg_2d = X_reg_train[top_features_reg].values\n",
    "X_clf_2d = X_clf_train[top_features_clf].values\n",
    "\n",
    "# Fit simple trees on 2D data\n",
    "dt_reg_2d = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
    "dt_reg_2d.fit(X_reg_2d, y_reg_train)\n",
    "\n",
    "dt_clf_2d = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "dt_clf_2d.fit(X_clf_2d, y_clf_train)\n",
    "\n",
    "# Create meshgrid for decision boundary\n",
    "def create_meshgrid(X, h=0.02):\n",
    " x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    " y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    " xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    " np.arange(y_min, y_max, h))\n",
    " return xx, yy\n",
    "\n",
    "# Classification decision boundary\n",
    "xx_clf, yy_clf = create_meshgrid(X_clf_2d)\n",
    "mesh_points_clf = np.c_[xx_clf.ravel(), yy_clf.ravel()]\n",
    "Z_clf = dt_clf_2d.predict(mesh_points_clf)\n",
    "Z_clf = Z_clf.reshape(xx_clf.shape)\n",
    "\n",
    "# Regression decision boundary\n",
    "xx_reg, yy_reg = create_meshgrid(X_reg_2d, h=100)\n",
    "mesh_points_reg = np.c_[xx_reg.ravel(), yy_reg.ravel()]\n",
    "Z_reg = dt_reg_2d.predict(mesh_points_reg)\n",
    "Z_reg = Z_reg.reshape(xx_reg.shape)\n",
    "\n",
    "# Visualize decision boundaries\n",
    "fig_boundaries = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=(f\"Regression: {top_features_reg[0]} vs {top_features_reg[1]}\",\n",
    " f\"Classification: {top_features_clf[0]} vs {top_features_clf[1]}\")\n",
    ")\n",
    "\n",
    "# Regression boundary\n",
    "fig_boundaries.add_trace(\n",
    " go.Contour(\n",
    " x=xx_reg[0],\n",
    " y=yy_reg[:, 0],\n",
    " z=Z_reg,\n",
    " colorscale='Viridis',\n",
    " opacity=0.3,\n",
    " showscale=False,\n",
    " hovertemplate=\"X: %{x}<br>Y: %{y}<br>Prediction: %{z:,.0f}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Add regression data points\n",
    "fig_boundaries.add_trace(\n",
    " go.Scatter(\n",
    " x=X_reg_2d[:, 0],\n",
    " y=X_reg_2d[:, 1],\n",
    " mode='markers',\n",
    " marker=dict(\n",
    " color=y_reg_train,\n",
    " colorscale='Viridis',\n",
    " size=6,\n",
    " opacity=0.7,\n",
    " colorbar=dict(title=\"Revenue\")\n",
    " ),\n",
    " name='Training Data',\n",
    " hovertemplate=f\"<b>{top_features_reg[0]}</b>: %{{x}}<br><b>{top_features_reg[1]}</b>: %{{y}}<br>Revenue: %{{marker.color:,.0f}}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Classification boundary\n",
    "fig_boundaries.add_trace(\n",
    " go.Contour(\n",
    " x=xx_clf[0],\n",
    " y=yy_clf[:, 0],\n",
    " z=Z_clf,\n",
    " colorscale='RdYlBu',\n",
    " opacity=0.3,\n",
    " showscale=False,\n",
    " hovertemplate=\"X: %{x}<br>Y: %{y}<br>Prediction: %{z}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Add classification data points\n",
    "colors_clf = ['blue' if c == 0 else 'red' for c in y_clf_train]\n",
    "fig_boundaries.add_trace(\n",
    " go.Scatter(\n",
    " x=X_clf_2d[:, 0],\n",
    " y=X_clf_2d[:, 1],\n",
    " mode='markers',\n",
    " marker=dict(\n",
    " color=colors_clf,\n",
    " size=6,\n",
    " opacity=0.7\n",
    " ),\n",
    " name='Training Data',\n",
    " showlegend=False,\n",
    " hovertemplate=f\"<b>{top_features_clf[0]}</b>: %{{x}}<br><b>{top_features_clf[1]}</b>: %{{y}}<br>Churn: %{{marker.color}}<extra></extra>\"\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_boundaries.update_layout(\n",
    " title=\"Decision Tree: Decision Boundaries (2D Projection)\",\n",
    " height=500\n",
    ")\n",
    "fig_boundaries.update_xaxes(title_text=top_features_reg[0], row=1, col=1)\n",
    "fig_boundaries.update_yaxes(title_text=top_features_reg[1], row=1, col=1)\n",
    "fig_boundaries.update_xaxes(title_text=top_features_clf[0], row=1, col=2)\n",
    "fig_boundaries.update_yaxes(title_text=top_features_clf[1], row=1, col=2)\n",
    "\n",
    "fig_boundaries.show()\n",
    "\n",
    "# 4.3 Feature Interaction Analysis\n",
    "print(f\"\\n4.3 Feature Interaction Analysis:\")\n",
    "\n",
    "# Analyze how features split data at different tree levels\n",
    "def analyze_tree_splits(tree_model, feature_names, max_depth=3):\n",
    " \"\"\"Analyze decision tree splits and feature usage by depth\"\"\"\n",
    "\n",
    " tree_structure = tree_model.tree_\n",
    " feature_usage = {}\n",
    "\n",
    " def traverse_tree(node_id, depth):\n",
    " if depth > max_depth:\n",
    " return\n",
    "\n",
    " if tree_structure.feature[node_id] != -2: # Not a leaf\n",
    " feature_idx = tree_structure.feature[node_id]\n",
    " feature_name = feature_names[feature_idx]\n",
    " threshold = tree_structure.threshold[node_id]\n",
    "\n",
    " if depth not in feature_usage:\n",
    " feature_usage[depth] = []\n",
    "\n",
    " feature_usage[depth].append({\n",
    " 'feature': feature_name,\n",
    " 'threshold': threshold,\n",
    " 'samples': tree_structure.n_node_samples[node_id]\n",
    " })\n",
    "\n",
    " # Traverse children\n",
    " traverse_tree(tree_structure.children_left[node_id], depth + 1)\n",
    " traverse_tree(tree_structure.children_right[node_id], depth + 1)\n",
    "\n",
    " traverse_tree(0, 0)\n",
    " return feature_usage\n",
    "\n",
    "# Analyze optimized trees\n",
    "reg_splits = analyze_tree_splits(dt_reg_optimized, reg_features)\n",
    "clf_splits = analyze_tree_splits(dt_clf_optimized, clf_features)\n",
    "\n",
    "print(\" Regression Tree - Feature Usage by Depth:\")\n",
    "for depth, splits in reg_splits.items():\n",
    " print(f\" Depth {depth}:\")\n",
    " for split in splits:\n",
    " print(f\" • {split['feature']} <= {split['threshold']:.3f} (samples: {split['samples']})\")\n",
    "\n",
    "print(\"\\n Classification Tree - Feature Usage by Depth:\")\n",
    "for depth, splits in clf_splits.items():\n",
    " print(f\" Depth {depth}:\")\n",
    " for split in splits:\n",
    " print(f\" • {split['feature']} <= {split['threshold']:.3f} (samples: {split['samples']})\")\n",
    "\n",
    "# 4.4 Prediction Path Analysis\n",
    "print(f\"\\n4.4 Sample Prediction Path Analysis:\")\n",
    "\n",
    "# Get prediction paths for a few samples\n",
    "def get_decision_path(tree_model, X_sample, feature_names):\n",
    " \"\"\"Get the decision path for a sample\"\"\"\n",
    "\n",
    " tree_structure = tree_model.tree_\n",
    " decision_path = tree_model.decision_path(X_sample)\n",
    "\n",
    " paths = []\n",
    " for sample_id in range(X_sample.shape[0]):\n",
    " sample_path = []\n",
    " node_indicator = decision_path[sample_id]\n",
    "\n",
    " for node_id in node_indicator.indices:\n",
    " if tree_structure.feature[node_id] != -2: # Not a leaf\n",
    " feature_idx = tree_structure.feature[node_id]\n",
    " feature_name = feature_names[feature_idx]\n",
    " threshold = tree_structure.threshold[node_id]\n",
    " feature_value = X_sample[sample_id, feature_idx]\n",
    "\n",
    " if feature_value <= threshold:\n",
    " condition = f\"{feature_name} <= {threshold:.3f}\"\n",
    " direction = \"left\"\n",
    " else:\n",
    " condition = f\"{feature_name} > {threshold:.3f}\"\n",
    " direction = \"right\"\n",
    "\n",
    " sample_path.append({\n",
    " 'condition': condition,\n",
    " 'feature_value': feature_value,\n",
    " 'direction': direction\n",
    " })\n",
    "\n",
    " paths.append(sample_path)\n",
    "\n",
    " return paths\n",
    "\n",
    "# Analyze a few test samples\n",
    "sample_indices = [0, 1, 2]\n",
    "reg_sample_paths = get_decision_path(dt_reg_optimized, X_reg_test.iloc[sample_indices].values, reg_features)\n",
    "clf_sample_paths = get_decision_path(dt_clf_optimized, X_clf_test.iloc[sample_indices].values, clf_features)\n",
    "\n",
    "print(\" Sample Regression Predictions:\")\n",
    "for i, path in enumerate(reg_sample_paths):\n",
    " actual = y_reg_test.iloc[sample_indices[i]]\n",
    " predicted = dt_reg_optimized.predict(X_reg_test.iloc[sample_indices:sample_indices+1])[0]\n",
    " print(f\" Sample {i+1}: Actual=${actual:,.0f}, Predicted=${predicted:,.0f}\")\n",
    " print(f\" Decision path: {' → '.join([step['condition'] for step in path])}\")\n",
    "\n",
    "print(\"\\n Sample Classification Predictions:\")\n",
    "for i, path in enumerate(clf_sample_paths):\n",
    " actual = y_clf_test.iloc[sample_indices[i]]\n",
    " predicted = dt_clf_optimized.predict(X_clf_test.iloc[sample_indices:sample_indices+1])[0]\n",
    " probability = dt_clf_optimized.predict_proba(X_clf_test.iloc[sample_indices:sample_indices+1])[0]\n",
    " print(f\" Sample {i+1}: Actual={actual}, Predicted={predicted}, Prob=[{probability[0]:.3f}, {probability[1]:.3f}]\")\n",
    " print(f\" Decision path: {' → '.join([step['condition'] for step in path])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc813a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\n",
    "print(\"\\n 5. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "# 5.1 Revenue Prediction Insights\n",
    "print(\"5.1 Revenue Prediction Business Rules:\")\n",
    "\n",
    "# Extract business rules from regression tree\n",
    "def extract_business_rules(tree_model, feature_names, target_name, threshold_percentile=80):\n",
    " \"\"\"Extract actionable business rules from decision tree\"\"\"\n",
    "\n",
    " tree_structure = tree_model.tree_\n",
    " rules = []\n",
    "\n",
    " def traverse_for_rules(node_id, conditions, depth=0):\n",
    " if tree_structure.feature[node_id] != -2: # Not a leaf\n",
    " feature_idx = tree_structure.feature[node_id]\n",
    " feature_name = feature_names[feature_idx]\n",
    " threshold = tree_structure.threshold[node_id]\n",
    "\n",
    " # Left child (<=)\n",
    " left_conditions = conditions + [f\"{feature_name} <= {threshold:.2f}\"]\n",
    " traverse_for_rules(tree_structure.children_left[node_id], left_conditions, depth + 1)\n",
    "\n",
    " # Right child (>)\n",
    " right_conditions = conditions + [f\"{feature_name} > {threshold:.2f}\"]\n",
    " traverse_for_rules(tree_structure.children_right[node_id], right_conditions, depth + 1)\n",
    " else:\n",
    " # Leaf node - extract rule\n",
    " prediction = tree_structure.value[node_id][0][0]\n",
    " samples = tree_structure.n_node_samples[node_id]\n",
    "\n",
    " rules.append({\n",
    " 'conditions': conditions,\n",
    " 'prediction': prediction,\n",
    " 'samples': samples,\n",
    " 'rule': ' AND '.join(conditions) if conditions else 'All samples'\n",
    " })\n",
    "\n",
    " traverse_for_rules(0, [])\n",
    "\n",
    " # Filter rules by prediction value (high-value rules)\n",
    " all_predictions = [rule['prediction'] for rule in rules]\n",
    " high_threshold = np.percentile(all_predictions, threshold_percentile)\n",
    "\n",
    " high_value_rules = [rule for rule in rules if rule['prediction'] >= high_threshold]\n",
    " high_value_rules.sort(key=lambda x: x['prediction'], reverse=True)\n",
    "\n",
    " return rules, high_value_rules\n",
    "\n",
    "# Extract revenue rules\n",
    "all_revenue_rules, high_revenue_rules = extract_business_rules(\n",
    " dt_reg_optimized, reg_features, 'revenue', threshold_percentile=80\n",
    ")\n",
    "\n",
    "print(\" High-Revenue Business Rules (Top 80th percentile):\")\n",
    "for i, rule in enumerate(high_revenue_rules[:5], 1):\n",
    " print(f\" {i}. Expected Revenue: ${rule['prediction']:,.0f} (n={rule['samples']})\")\n",
    " print(f\" Conditions: {rule['rule']}\")\n",
    " print()\n",
    "\n",
    "# 5.2 Churn Prevention Insights\n",
    "print(\"5.2 Churn Prevention Business Rules:\")\n",
    "\n",
    "# Extract churn rules (focusing on high-churn probability leaves)\n",
    "def extract_churn_rules(tree_model, feature_names):\n",
    " \"\"\"Extract churn probability rules\"\"\"\n",
    "\n",
    " tree_structure = tree_model.tree_\n",
    " rules = []\n",
    "\n",
    " def traverse_for_churn(node_id, conditions):\n",
    " if tree_structure.feature[node_id] != -2: # Not a leaf\n",
    " feature_idx = tree_structure.feature[node_id]\n",
    " feature_name = feature_names[feature_idx]\n",
    " threshold = tree_structure.threshold[node_id]\n",
    "\n",
    " # Left child\n",
    " left_conditions = conditions + [f\"{feature_name} <= {threshold:.2f}\"]\n",
    " traverse_for_churn(tree_structure.children_left[node_id], left_conditions)\n",
    "\n",
    " # Right child\n",
    " right_conditions = conditions + [f\"{feature_name} > {threshold:.2f}\"]\n",
    " traverse_for_churn(tree_structure.children_right[node_id], right_conditions)\n",
    " else:\n",
    " # Leaf node\n",
    " class_counts = tree_structure.value[node_id][0]\n",
    " total_samples = tree_structure.n_node_samples[node_id]\n",
    " churn_probability = class_counts[1] / total_samples if total_samples > 0 else 0\n",
    "\n",
    " rules.append({\n",
    " 'conditions': conditions,\n",
    " 'churn_probability': churn_probability,\n",
    " 'samples': total_samples,\n",
    " 'churn_count': int(class_counts[1]),\n",
    " 'rule': ' AND '.join(conditions) if conditions else 'All samples'\n",
    " })\n",
    "\n",
    " traverse_for_churn(0, [])\n",
    "\n",
    " # Filter high-risk rules\n",
    " high_risk_rules = [rule for rule in rules if rule['churn_probability'] >= 0.6 and rule['samples'] >= 10]\n",
    " high_risk_rules.sort(key=lambda x: x['churn_probability'], reverse=True)\n",
    "\n",
    " return rules, high_risk_rules\n",
    "\n",
    "all_churn_rules, high_risk_rules = extract_churn_rules(dt_clf_optimized, clf_features)\n",
    "\n",
    "print(\" High-Risk Churn Segments (>60% churn probability):\")\n",
    "for i, rule in enumerate(high_risk_rules[:5], 1):\n",
    " print(f\" {i}. Churn Risk: {rule['churn_probability']:.1%} (n={rule['samples']}, churned={rule['churn_count']})\")\n",
    " print(f\" Conditions: {rule['rule']}\")\n",
    " print()\n",
    "\n",
    "# 5.3 Feature-Based Recommendations\n",
    "print(\"5.3 Strategic Recommendations by Feature Importance:\")\n",
    "\n",
    "# Revenue recommendations\n",
    "print(\" Revenue Optimization Strategies:\")\n",
    "for i, (_, row) in enumerate(feature_importance_reg.head(3).iterrows(), 1):\n",
    " feature = row['feature']\n",
    " importance = row['importance']\n",
    "\n",
    " if 'marketing_spend' in feature:\n",
    " print(f\" {i}. Marketing Investment (Importance: {importance:.3f})\")\n",
    " print(\" • Increase marketing spend for companies with high employee counts\")\n",
    " print(\" • Focus on digital marketing for tech industry segments\")\n",
    " elif 'customer_satisfaction' in feature:\n",
    " print(f\" {i}. Customer Experience (Importance: {importance:.3f})\")\n",
    " print(\" • Implement customer satisfaction monitoring systems\")\n",
    " print(\" • Prioritize satisfaction improvements for large companies\")\n",
    " elif 'employee_count' in feature:\n",
    " print(f\" {i}. Workforce Optimization (Importance: {importance:.3f})\")\n",
    " print(\" • Monitor revenue per employee ratios\")\n",
    " print(\" • Scale workforce based on industry-specific productivity metrics\")\n",
    "\n",
    "print(\"\\n Churn Prevention Strategies:\")\n",
    "for i, (_, row) in enumerate(feature_importance_clf.head(3).iterrows(), 1):\n",
    " feature = row['feature']\n",
    " importance = row['importance']\n",
    "\n",
    " if 'tenure_months' in feature:\n",
    " print(f\" {i}. Early Retention Focus (Importance: {importance:.3f})\")\n",
    " print(\" • Implement 90-day onboarding program\")\n",
    " print(\" • Provide extra support for customers in first 6 months\")\n",
    " elif 'support_calls' in feature:\n",
    " print(f\" {i}. Proactive Support (Importance: {importance:.3f})\")\n",
    " print(\" • Flag customers with >2 support calls per month\")\n",
    " print(\" • Implement proactive outreach for high-contact customers\")\n",
    " elif 'monthly_charges' in feature:\n",
    " print(f\" {i}. Pricing Strategy (Importance: {importance:.3f})\")\n",
    " print(\" • Review pricing for high-charge, short-tenure customers\")\n",
    " print(\" • Consider loyalty discounts for long-term customers\")\n",
    "\n",
    "# 5.4 ROI Analysis\n",
    "print(f\"\\n5.4 ROI and Business Impact Analysis:\")\n",
    "\n",
    "# Calculate potential impact of interventions\n",
    "def calculate_intervention_impact(rules, baseline_metric, improvement_rate=0.2):\n",
    " \"\"\"Calculate potential business impact of interventions\"\"\"\n",
    "\n",
    " total_impact = 0\n",
    " for rule in rules[:3]: # Top 3 actionable rules\n",
    " samples_affected = rule['samples']\n",
    " current_value = rule.get('prediction', rule.get('churn_probability', 0))\n",
    "\n",
    " if 'prediction' in rule: # Revenue rules\n",
    " potential_increase = current_value * improvement_rate\n",
    " total_impact += potential_increase * samples_affected\n",
    " else: # Churn rules\n",
    " churn_reduction = current_value * improvement_rate\n",
    " total_impact += churn_reduction * samples_affected\n",
    "\n",
    " return total_impact\n",
    "\n",
    "# Revenue impact\n",
    "revenue_impact = calculate_intervention_impact(high_revenue_rules)\n",
    "print(f\" Potential Revenue Impact:\")\n",
    "print(f\"• High-value segment optimization: ${revenue_impact:,.0f} potential increase\")\n",
    "print(f\"• Target segments: {sum(rule['samples'] for rule in high_revenue_rules[:3])} companies\")\n",
    "\n",
    "# Churn impact\n",
    "churn_impact = calculate_intervention_impact(high_risk_rules)\n",
    "print(f\"\\n Potential Churn Reduction Impact:\")\n",
    "print(f\"• High-risk customers prevented from churning: {churn_impact:.0f} customers\")\n",
    "print(f\"• Target segments: {sum(rule['samples'] for rule in high_risk_rules[:3])} at-risk customers\")\n",
    "\n",
    "# 5.5 Model Reliability Assessment\n",
    "print(f\"\\n5.5 Model Reliability and Risk Assessment:\")\n",
    "\n",
    "print(\" Model Performance Summary:\")\n",
    "print(f\"• Regression Model R²: {test_r2_reg_opt:.3f}\")\n",
    "print(f\"• Classification Model AUC: {roc_auc_opt:.3f}\")\n",
    "print(f\"• Regression Tree Depth: {dt_reg_optimized.get_depth()}\")\n",
    "print(f\"• Classification Tree Depth: {dt_clf_optimized.get_depth()}\")\n",
    "\n",
    "reliability_score = (test_r2_reg_opt + roc_auc_opt) / 2\n",
    "if reliability_score > 0.8:\n",
    " reliability = \"HIGH\"\n",
    "elif reliability_score > 0.6:\n",
    " reliability = \"MEDIUM\"\n",
    "else:\n",
    " reliability = \"LOW\"\n",
    "\n",
    "print(f\"• Overall Model Reliability: {reliability} ({reliability_score:.3f})\")\n",
    "\n",
    "print(f\"\\n Risk Factors:\")\n",
    "if dt_reg_optimized.get_depth() > 10:\n",
    " print(\"• CAUTION: Deep regression tree may overfit to training data\")\n",
    "if dt_clf_optimized.get_depth() > 10:\n",
    " print(\"• CAUTION: Deep classification tree may overfit to training data\")\n",
    "\n",
    "feature_concentration_reg = feature_importance_reg.iloc[0]['importance']\n",
    "feature_concentration_clf = feature_importance_clf.iloc[0]['importance']\n",
    "\n",
    "if feature_concentration_reg > 0.5:\n",
    " print(f\"• RISK: High dependence on single feature in regression ({feature_importance_reg.iloc[0]['feature']})\")\n",
    "if feature_concentration_clf > 0.5:\n",
    " print(f\"• RISK: High dependence on single feature in classification ({feature_importance_clf.iloc[0]['feature']})\")\n",
    "\n",
    "if reliability_score > 0.7:\n",
    " print(\" Models are suitable for business decision support\")\n",
    "else:\n",
    " print(\" Models need improvement before deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4efdea",
   "metadata": {},
   "source": [
    "# LEARNING SUMMARY: Decision Tree Analysis\n",
    "\n",
    "## Key Concepts Mastered\n",
    "\n",
    "### 1. **Decision Tree Fundamentals**\n",
    "- **Tree Construction**: Recursive binary splitting based on feature thresholds\n",
    "- **Splitting Criteria**: Gini impurity, entropy, and MSE for optimal splits\n",
    "- **Tree Structure**: Understanding nodes, leaves, depth, and branching logic\n",
    "- **Interpretability**: Clear decision rules and prediction paths\n",
    "\n",
    "### 2. **Overfitting Prevention**\n",
    "- **Pruning Techniques**: Pre-pruning (early stopping) and post-pruning methods\n",
    "- **Hyperparameter Tuning**: max_depth, min_samples_split, min_samples_leaf\n",
    "- **Cost Complexity Pruning**: Alpha-based pruning for optimal tree size\n",
    "- **Cross-Validation**: Robust model selection and performance estimation\n",
    "\n",
    "### 3. **Feature Analysis & Business Rules**\n",
    "- **Feature Importance**: Automatic ranking of variable significance\n",
    "- **Decision Boundaries**: Understanding how trees partition feature space\n",
    "- **Rule Extraction**: Converting tree logic into actionable business rules\n",
    "- **Path Analysis**: Tracing prediction logic for individual samples\n",
    "\n",
    "## Business Applications\n",
    "\n",
    "### Decision Support Systems\n",
    "- **Credit Scoring**: Automated loan approval with clear criteria\n",
    "- **Medical Diagnosis**: Rule-based diagnostic support systems\n",
    "- **Quality Control**: Defect detection with interpretable rules\n",
    "- **Customer Segmentation**: Clear criteria for marketing targeting\n",
    "\n",
    "### Strategic Planning\n",
    "- Decision trees provide:\n",
    " - Transparent decision logic for stakeholder buy-in\n",
    " - Actionable business rules for operational implementation\n",
    " - Feature importance for resource allocation priorities\n",
    " - Risk assessment with quantifiable decision paths\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Ensemble Methods** - Random Forests and Gradient Boosting\n",
    "2. **Advanced Pruning** - Minimal cost-complexity pruning\n",
    "3. **Multi-output Trees** - Simultaneous prediction of multiple targets\n",
    "4. **Tree-based Feature Selection** - Using trees for dimensionality reduction\n",
    "\n",
    "## Pro Tips\n",
    "\n",
    "- **Balance interpretability vs accuracy** - deeper trees = higher accuracy but less interpretable\n",
    "- **Use cross-validation** for hyperparameter selection to avoid overfitting\n",
    "- **Feature engineering matters** - trees work well with well-prepared features\n",
    "- **Consider ensemble methods** when single trees underperform\n",
    "- **Visualize decision boundaries** to understand model behavior\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "- **Overfitting**: Deep trees memorize training data rather than learning patterns\n",
    "- **Bias in Splits**: Trees favor features with more levels or higher cardinality\n",
    "- **Instability**: Small data changes can lead to very different trees\n",
    "- **Linear Relationships**: Trees struggle with simple linear relationships\n",
    "- **Missing Values**: Require preprocessing as trees can't handle NaN directly\n",
    "\n",
    "## Advanced Considerations\n",
    "\n",
    "### When to Use Decision Trees:\n",
    "- **Need interpretability**: Stakeholders require explainable decisions\n",
    "- **Mixed data types**: Handling both categorical and numerical features\n",
    "- **Non-linear relationships**: Complex interactions between features\n",
    "- **Rule-based systems**: Converting expert knowledge into automated systems\n",
    "\n",
    "### Performance Optimization:\n",
    "- **Feature selection**: Remove irrelevant features to reduce noise\n",
    "- **Balanced datasets**: Address class imbalance for better classification\n",
    "- **Ensemble methods**: Combine multiple trees for better performance\n",
    "- **Regular retraining**: Update models as business conditions change\n",
    "\n",
    "### Business Implementation:\n",
    "- **Rule documentation**: Maintain clear records of decision logic\n",
    "- **Performance monitoring**: Track model accuracy over time\n",
    "- **Stakeholder training**: Ensure users understand tree-based decisions\n",
    "- **Compliance considerations**: Ensure rules meet regulatory requirements\n",
    "\n",
    "**Remember**: *Decision trees excel when you need both accuracy AND interpretability - they're your go-to method when stakeholders need to understand \"why\" the model made a specific prediction!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}