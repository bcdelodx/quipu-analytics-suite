{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 2: Logistic Regression\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** b2524d9a-ec1d-423b-bece-f54261af0234\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 2: Logistic Regression,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** b2524d9a-ec1d-423b-bece-f54261af0234\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55579c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Essential Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Statistical imports\n",
    "from scipy import stats\n",
    "from scipy.special import expit # sigmoid function\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 2: Logistic Regression - Libraries Loaded Successfully!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Available Logistic Regression Techniques:\")\n",
    "print(\"• Binary Logistic Regression - Two-class classification problems\")\n",
    "print(\"• Multinomial Logistic Regression - Multi-class classification\")\n",
    "print(\"• Regularized Logistic Regression - L1 (Lasso) and L2 (Ridge) penalties\")\n",
    "print(\"• Odds Ratio Analysis - Interpretable feature impact assessment\")\n",
    "print(\"• Probability Calibration - Well-calibrated probability estimates\")\n",
    "print(\"• Feature Selection - Automatic feature selection with L1 regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Comprehensive Datasets for Logistic Regression Analysis\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_logistic_regression_datasets():\n",
    " \"\"\"Generate datasets optimized for logistic regression analysis\"\"\"\n",
    "\n",
    " # 1. BINARY CLASSIFICATION - Customer Conversion Prediction\n",
    " n_customers = 1000\n",
    "\n",
    " # Customer demographic features\n",
    " age = np.random.normal(35, 12, n_customers)\n",
    " age = np.clip(age, 18, 70)\n",
    "\n",
    " income = np.random.lognormal(10.5, 0.8, n_customers)\n",
    " income = np.clip(income, 20000, 150000)\n",
    "\n",
    " # Website engagement features\n",
    " pages_viewed = np.random.poisson(lam=5, size=n_customers) + 1\n",
    " time_on_site = np.random.exponential(scale=10, size=n_customers) + 1 # minutes\n",
    " previous_purchases = np.random.poisson(lam=2, size=n_customers)\n",
    "\n",
    " # Email engagement\n",
    " email_opens = np.random.poisson(lam=3, size=n_customers)\n",
    " email_clicks = np.random.binomial(email_opens, 0.3) # 30% click rate\n",
    "\n",
    " # Marketing features\n",
    " days_since_signup = np.random.exponential(scale=30, size=n_customers) + 1\n",
    " marketing_emails_received = np.random.poisson(lam=4, size=n_customers) + 1\n",
    "\n",
    " # Create realistic conversion probability using logistic function\n",
    " # Higher income, more engagement, recent signups more likely to convert\n",
    " logit_score = (\n",
    " -3.0 + # Base intercept (low conversion rate)\n",
    " 0.02 * (age - 35) + # Age effect (slight preference for older)\n",
    " 0.00003 * (income - 50000) + # Income effect\n",
    " 0.15 * pages_viewed + # Page views strongly predict conversion\n",
    " 0.08 * time_on_site + # Time on site\n",
    " 0.2 * previous_purchases + # Purchase history\n",
    " 0.1 * email_clicks + # Email engagement\n",
    " -0.01 * days_since_signup + # Recency effect\n",
    " 0.05 * marketing_emails_received + # Marketing exposure\n",
    " np.random.normal(0, 0.5, n_customers) # Random noise\n",
    " )\n",
    "\n",
    " # Convert to probabilities using sigmoid\n",
    " conversion_prob = expit(logit_score)\n",
    " converted = np.random.binomial(1, conversion_prob)\n",
    "\n",
    " binary_df = pd.DataFrame({\n",
    " 'age': age,\n",
    " 'income': income,\n",
    " 'pages_viewed': pages_viewed,\n",
    " 'time_on_site': time_on_site,\n",
    " 'previous_purchases': previous_purchases,\n",
    " 'email_opens': email_opens,\n",
    " 'email_clicks': email_clicks,\n",
    " 'days_since_signup': days_since_signup,\n",
    " 'marketing_emails_received': marketing_emails_received,\n",
    " 'converted': converted\n",
    " })\n",
    "\n",
    " # 2. MULTICLASS CLASSIFICATION - Product Category Prediction\n",
    " n_products = 800\n",
    "\n",
    " # Product features that might predict category\n",
    " price = np.random.lognormal(mean=4, sigma=1.2, size=n_products) # Price in dollars\n",
    " price = np.clip(price, 5, 500)\n",
    "\n",
    " weight = np.random.gamma(shape=2, scale=0.5, size=n_products) # Weight in kg\n",
    " weight = np.clip(weight, 0.1, 10)\n",
    "\n",
    " rating = np.random.beta(a=5, b=2, size=n_products) * 5 # 1-5 star rating\n",
    " rating = np.clip(rating, 1, 5)\n",
    "\n",
    " review_count = np.random.negative_binomial(n=5, p=0.3, size=n_products) + 1\n",
    "\n",
    " # Seasonal demand (0-100 scale)\n",
    " seasonal_demand = np.random.beta(a=2, b=2, size=n_products) * 100\n",
    "\n",
    " # Define 4 product categories with different characteristics\n",
    " categories = ['Electronics', 'Clothing', 'Books', 'Home']\n",
    "\n",
    " # Create category-specific patterns\n",
    " product_categories = []\n",
    "\n",
    " for i in range(n_products):\n",
    " # Electronics: Higher price, medium weight, high rating, high reviews\n",
    " if (price[i] > 100 and weight[i] < 3 and rating[i] > 3.5):\n",
    " category_prob = [0.6, 0.1, 0.1, 0.2]\n",
    "\n",
    " # Clothing: Medium price, low weight, medium rating\n",
    " elif (20 < price[i] < 150 and weight[i] < 1 and seasonal_demand[i] > 60):\n",
    " category_prob = [0.1, 0.6, 0.1, 0.2]\n",
    "\n",
    " # Books: Lower price, low weight, high rating, medium reviews\n",
    " elif (price[i] < 50 and weight[i] < 0.5 and rating[i] > 3.0):\n",
    " category_prob = [0.1, 0.1, 0.7, 0.1]\n",
    "\n",
    " # Home: Varied price, higher weight, medium rating\n",
    " else:\n",
    " category_prob = [0.2, 0.2, 0.1, 0.5]\n",
    "\n",
    " # Add some randomness\n",
    " noise = np.random.dirichlet([1, 1, 1, 1]) * 0.3\n",
    " category_prob = np.array(category_prob) * 0.7 + noise\n",
    " category_prob = category_prob / category_prob.sum()\n",
    "\n",
    " category = np.random.choice(categories, p=category_prob)\n",
    " product_categories.append(category)\n",
    "\n",
    " multiclass_df = pd.DataFrame({\n",
    " 'price': price,\n",
    " 'weight': weight,\n",
    " 'rating': rating,\n",
    " 'review_count': review_count,\n",
    " 'seasonal_demand': seasonal_demand,\n",
    " 'category': product_categories\n",
    " })\n",
    "\n",
    " # 3. REGULARIZATION DATASET - High-dimensional Feature Space\n",
    " n_samples = 500\n",
    " n_features = 50 # Many features to demonstrate regularization\n",
    "\n",
    " # Generate correlated features\n",
    " np.random.seed(42)\n",
    "\n",
    " # Create some true signal features\n",
    " true_features = np.random.randn(n_samples, 5)\n",
    "\n",
    " # Create noise features\n",
    " noise_features = np.random.randn(n_samples, n_features - 5)\n",
    "\n",
    " # Create correlated features from true features\n",
    " correlated_features = (\n",
    " true_features[:, [0, 1, 2]] +\n",
    " np.random.randn(n_samples, 3) * 0.3\n",
    " )\n",
    "\n",
    " # Combine all features\n",
    " X_high_dim = np.column_stack([\n",
    " true_features,\n",
    " correlated_features,\n",
    " noise_features\n",
    " ])\n",
    "\n",
    " # Generate target with only first 5 features being truly predictive\n",
    " true_coefficients = np.array([1.5, -1.2, 0.8, -0.6, 1.0])\n",
    " linear_combination = X_high_dim[:, :5] @ true_coefficients\n",
    "\n",
    " # Add intercept and noise\n",
    " logit_scores = -0.5 + linear_combination + np.random.randn(n_samples) * 0.3\n",
    "\n",
    " # Convert to probabilities and binary outcomes\n",
    " probabilities = expit(logit_scores)\n",
    " y_high_dim = np.random.binomial(1, probabilities)\n",
    "\n",
    " # Create feature names\n",
    " feature_names = ([f'signal_{i}' for i in range(1, 6)] +\n",
    " [f'correlated_{i}' for i in range(1, 4)] +\n",
    " [f'noise_{i}' for i in range(1, n_features-7)])\n",
    "\n",
    " regularization_df = pd.DataFrame(X_high_dim, columns=feature_names)\n",
    " regularization_df['target'] = y_high_dim\n",
    "\n",
    " return binary_df, multiclass_df, regularization_df\n",
    "\n",
    "# Generate datasets\n",
    "print(\" Generating Logistic Regression optimized datasets...\")\n",
    "binary_df, multiclass_df, regularization_df = generate_logistic_regression_datasets()\n",
    "\n",
    "print(f\"Binary Classification Dataset (Customer Conversion): {binary_df.shape}\")\n",
    "print(f\"Multiclass Classification Dataset (Product Categories): {multiclass_df.shape}\")\n",
    "print(f\"Regularization Dataset (High-dimensional): {regularization_df.shape}\")\n",
    "\n",
    "print(\"\\nBinary Classification Dataset (Customer Conversion):\")\n",
    "print(binary_df.head())\n",
    "print(f\"Conversion Rate: {binary_df['converted'].mean():.1%}\")\n",
    "\n",
    "print(\"\\nMulticlass Classification Dataset (Product Categories):\")\n",
    "print(multiclass_df.head())\n",
    "print(\"\\nCategory Distribution:\")\n",
    "print(multiclass_df['category'].value_counts())\n",
    "\n",
    "print(\"\\nRegularization Dataset (High-dimensional Features):\")\n",
    "print(regularization_df.head())\n",
    "print(f\"Target Distribution: {regularization_df['target'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d110e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BINARY LOGISTIC REGRESSION ANALYSIS\n",
    "print(\" 1. BINARY LOGISTIC REGRESSION ANALYSIS\")\n",
    "print(\"=\" * 41)\n",
    "\n",
    "# Prepare binary classification data\n",
    "binary_features = ['age', 'income', 'pages_viewed', 'time_on_site', 'previous_purchases',\n",
    " 'email_opens', 'email_clicks', 'days_since_signup', 'marketing_emails_received']\n",
    "X_binary = binary_df[binary_features]\n",
    "y_binary = binary_df['converted']\n",
    "\n",
    "# Split data\n",
    "X_bin_train, X_bin_test, y_bin_train, y_bin_test = train_test_split(\n",
    " X_binary, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "# Scale features for better convergence\n",
    "scaler_binary = StandardScaler()\n",
    "X_bin_train_scaled = scaler_binary.fit_transform(X_bin_train)\n",
    "X_bin_test_scaled = scaler_binary.transform(X_bin_test)\n",
    "\n",
    "print(f\"Training set: {X_bin_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_bin_test_scaled.shape}\")\n",
    "print(f\"Class distribution: {y_bin_train.value_counts().to_dict()}\")\n",
    "\n",
    "# Train basic logistic regression\n",
    "lr_basic = LogisticRegression(random_state=42)\n",
    "lr_basic.fit(X_bin_train_scaled, y_bin_train)\n",
    "\n",
    "# Predictions and probabilities\n",
    "y_bin_pred = lr_basic.predict(X_bin_test_scaled)\n",
    "y_bin_proba = lr_basic.predict_proba(X_bin_test_scaled)\n",
    "\n",
    "# Performance metrics\n",
    "bin_accuracy = accuracy_score(y_bin_test, y_bin_pred)\n",
    "bin_auc = roc_auc_score(y_bin_test, y_bin_proba[:, 1])\n",
    "bin_log_loss = log_loss(y_bin_test, y_bin_proba[:, 1])\n",
    "\n",
    "print(f\"\\n Basic Logistic Regression Performance:\")\n",
    "print(f\"• Test Accuracy: {bin_accuracy:.4f}\")\n",
    "print(f\"• ROC AUC: {bin_auc:.4f}\")\n",
    "print(f\"• Log Loss: {bin_log_loss:.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(lr_basic, X_bin_train_scaled, y_bin_train, cv=5, scoring='roc_auc')\n",
    "print(f\"• Cross-validation AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_bin_test, y_bin_pred, target_names=['Not Converted', 'Converted']))\n",
    "\n",
    "# Coefficient analysis and odds ratios\n",
    "coefficients = lr_basic.coef_[0]\n",
    "odds_ratios = np.exp(coefficients)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    " 'Feature': binary_features,\n",
    " 'Coefficient': coefficients,\n",
    " 'Odds_Ratio': odds_ratios,\n",
    " 'Abs_Coefficient': np.abs(coefficients)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(f\"\\n Feature Coefficients and Odds Ratios:\")\n",
    "for _, row in coef_df.iterrows():\n",
    " direction = \"increases\" if row['Coefficient'] > 0 else \"decreases\"\n",
    " print(f\"• {row['Feature']}: β={row['Coefficient']:.3f}, OR={row['Odds_Ratio']:.3f}\")\n",
    " print(f\" → 1 unit increase {direction} odds by {abs(row['Odds_Ratio']-1)*100:.1f}%\")\n",
    "\n",
    "# Visualize coefficients and odds ratios\n",
    "fig_coef = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=['Feature Coefficients', 'Odds Ratios']\n",
    ")\n",
    "\n",
    "# Coefficients\n",
    "fig_coef.add_trace(\n",
    " go.Bar(\n",
    " x=coef_df['Coefficient'],\n",
    " y=coef_df['Feature'],\n",
    " orientation='h',\n",
    " name='Coefficients',\n",
    " marker_color=['red' if x < 0 else 'blue' for x in coef_df['Coefficient']]\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Odds ratios\n",
    "fig_coef.add_trace(\n",
    " go.Bar(\n",
    " x=coef_df['Odds_Ratio'],\n",
    " y=coef_df['Feature'],\n",
    " orientation='h',\n",
    " name='Odds Ratios',\n",
    " marker_color=['red' if x < 1 else 'blue' for x in coef_df['Odds_Ratio']],\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Add vertical line at OR = 1\n",
    "fig_coef.add_vline(x=1, line_dash=\"dash\", line_color=\"black\", row=1, col=2)\n",
    "\n",
    "fig_coef.update_layout(\n",
    " title=\"Logistic Regression: Feature Importance Analysis\",\n",
    " height=600\n",
    ")\n",
    "fig_coef.show()\n",
    "\n",
    "# ROC and Precision-Recall Curves\n",
    "fpr, tpr, _ = roc_curve(y_bin_test, y_bin_proba[:, 1])\n",
    "precision, recall, _ = precision_recall_curve(y_bin_test, y_bin_proba[:, 1])\n",
    "\n",
    "fig_curves = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=[f'ROC Curve (AUC = {bin_auc:.3f})', 'Precision-Recall Curve']\n",
    ")\n",
    "\n",
    "# ROC Curve\n",
    "fig_curves.add_trace(\n",
    " go.Scatter(\n",
    " x=fpr,\n",
    " y=tpr,\n",
    " mode='lines',\n",
    " name='ROC Curve',\n",
    " line=dict(color='blue', width=2)\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig_curves.add_trace(\n",
    " go.Scatter(\n",
    " x=[0, 1],\n",
    " y=[0, 1],\n",
    " mode='lines',\n",
    " name='Random Classifier',\n",
    " line=dict(color='red', dash='dash'),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Precision-Recall Curve\n",
    "fig_curves.add_trace(\n",
    " go.Scatter(\n",
    " x=recall,\n",
    " y=precision,\n",
    " mode='lines',\n",
    " name='Precision-Recall',\n",
    " line=dict(color='green', width=2),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Baseline precision (proportion of positive class)\n",
    "baseline_precision = y_bin_test.mean()\n",
    "fig_curves.add_hline(y=baseline_precision, line_dash=\"dash\", line_color=\"red\", row=1, col=2)\n",
    "\n",
    "fig_curves.update_xaxes(title_text=\"False Positive Rate\", row=1, col=1)\n",
    "fig_curves.update_yaxes(title_text=\"True Positive Rate\", row=1, col=1)\n",
    "fig_curves.update_xaxes(title_text=\"Recall\", row=1, col=2)\n",
    "fig_curves.update_yaxes(title_text=\"Precision\", row=1, col=2)\n",
    "\n",
    "fig_curves.update_layout(\n",
    " title=\"Logistic Regression Performance Curves\",\n",
    " height=500\n",
    ")\n",
    "fig_curves.show()\n",
    "\n",
    "# Probability distribution analysis\n",
    "print(f\"\\n Probability Distribution Analysis:\")\n",
    "\n",
    "# Analyze predicted probabilities by actual class\n",
    "converted_probs = y_bin_proba[y_bin_test == 1, 1]\n",
    "not_converted_probs = y_bin_proba[y_bin_test == 0, 1]\n",
    "\n",
    "print(f\"Converted customers - Mean probability: {converted_probs.mean():.3f}\")\n",
    "print(f\"Not converted customers - Mean probability: {not_converted_probs.mean():.3f}\")\n",
    "\n",
    "# Visualize probability distributions\n",
    "fig_prob_dist = go.Figure()\n",
    "\n",
    "fig_prob_dist.add_trace(\n",
    " go.Histogram(\n",
    " x=not_converted_probs,\n",
    " name='Not Converted',\n",
    " opacity=0.7,\n",
    " marker_color='red',\n",
    " nbinsx=30\n",
    " )\n",
    ")\n",
    "\n",
    "fig_prob_dist.add_trace(\n",
    " go.Histogram(\n",
    " x=converted_probs,\n",
    " name='Converted',\n",
    " opacity=0.7,\n",
    " marker_color='blue',\n",
    " nbinsx=30\n",
    " )\n",
    ")\n",
    "\n",
    "fig_prob_dist.update_layout(\n",
    " title=\"Predicted Probability Distributions by Actual Class\",\n",
    " xaxis_title=\"Predicted Probability of Conversion\",\n",
    " yaxis_title=\"Frequency\",\n",
    " barmode='overlay',\n",
    " height=500\n",
    ")\n",
    "fig_prob_dist.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_binary = confusion_matrix(y_bin_test, y_bin_pred)\n",
    "\n",
    "fig_cm_binary = ff.create_annotated_heatmap(\n",
    " z=cm_binary,\n",
    " x=['Not Converted', 'Converted'],\n",
    " y=['Not Converted', 'Converted'],\n",
    " annotation_text=cm_binary,\n",
    " colorscale='Blues',\n",
    " showscale=True\n",
    ")\n",
    "\n",
    "fig_cm_binary.update_layout(\n",
    " title=\"Logistic Regression Confusion Matrix (Customer Conversion)\",\n",
    " xaxis_title=\"Predicted\",\n",
    " yaxis_title=\"Actual\",\n",
    " height=400\n",
    ")\n",
    "fig_cm_binary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c69ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MULTICLASS LOGISTIC REGRESSION\n",
    "print(\" 2. MULTICLASS LOGISTIC REGRESSION\")\n",
    "print(\"=\" * 33)\n",
    "\n",
    "# Prepare multiclass data\n",
    "multiclass_features = ['price', 'weight', 'rating', 'review_count', 'seasonal_demand']\n",
    "X_multi = multiclass_df[multiclass_features]\n",
    "y_multi = multiclass_df['category']\n",
    "\n",
    "# Split data\n",
    "X_multi_train, X_multi_test, y_multi_train, y_multi_test = train_test_split(\n",
    " X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_multi = StandardScaler()\n",
    "X_multi_train_scaled = scaler_multi.fit_transform(X_multi_train)\n",
    "X_multi_test_scaled = scaler_multi.transform(X_multi_test)\n",
    "\n",
    "print(f\"Training set: {X_multi_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_multi_test_scaled.shape}\")\n",
    "print(f\"Class distribution: {y_multi_train.value_counts().to_dict()}\")\n",
    "\n",
    "# Train multiclass logistic regression\n",
    "lr_multi = LogisticRegression(\n",
    " multi_class='multinomial',\n",
    " solver='lbfgs',\n",
    " random_state=42,\n",
    " max_iter=1000\n",
    ")\n",
    "lr_multi.fit(X_multi_train_scaled, y_multi_train)\n",
    "\n",
    "# Predictions\n",
    "y_multi_pred = lr_multi.predict(X_multi_test_scaled)\n",
    "y_multi_proba = lr_multi.predict_proba(X_multi_test_scaled)\n",
    "\n",
    "# Performance metrics\n",
    "multi_accuracy = accuracy_score(y_multi_test, y_multi_pred)\n",
    "print(f\"\\n Multiclass Logistic Regression Performance:\")\n",
    "print(f\"• Test Accuracy: {multi_accuracy:.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_multi = cross_val_score(lr_multi, X_multi_train_scaled, y_multi_train, cv=5)\n",
    "print(f\"• Cross-validation: {cv_scores_multi.mean():.4f} ± {cv_scores_multi.std():.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_multi_test, y_multi_pred))\n",
    "\n",
    "# Analyze coefficients for each class\n",
    "print(f\"\\n Feature Coefficients by Product Category:\")\n",
    "\n",
    "categories = lr_multi.classes_\n",
    "coefficients_multi = lr_multi.coef_\n",
    "\n",
    "# Create coefficient dataframe\n",
    "coef_data = []\n",
    "for i, category in enumerate(categories):\n",
    " for j, feature in enumerate(multiclass_features):\n",
    " coef_data.append({\n",
    " 'Category': category,\n",
    " 'Feature': feature,\n",
    " 'Coefficient': coefficients_multi[i, j],\n",
    " 'Odds_Ratio': np.exp(coefficients_multi[i, j])\n",
    " })\n",
    "\n",
    "coef_multi_df = pd.DataFrame(coef_data)\n",
    "\n",
    "# Show top coefficients for each category\n",
    "for category in categories:\n",
    " cat_coefs = coef_multi_df[coef_multi_df['Category'] == category].copy()\n",
    " cat_coefs['Abs_Coef'] = cat_coefs['Coefficient'].abs()\n",
    " cat_coefs = cat_coefs.sort_values('Abs_Coef', ascending=False)\n",
    "\n",
    " print(f\"\\n{category.upper()}:\")\n",
    " for _, row in cat_coefs.iterrows():\n",
    " direction = \"increases\" if row['Coefficient'] > 0 else \"decreases\"\n",
    " print(f\" • {row['Feature']}: β={row['Coefficient']:.3f} ({direction} odds)\")\n",
    "\n",
    "# Visualize coefficients heatmap\n",
    "coef_matrix = coefficients_multi.T # Features x Categories\n",
    "\n",
    "fig_coef_heatmap = go.Figure(data=go.Heatmap(\n",
    " z=coef_matrix,\n",
    " x=categories,\n",
    " y=multiclass_features,\n",
    " colorscale='RdBu',\n",
    " zmid=0,\n",
    " hovertemplate=\"Category: %{x}<br>Feature: %{y}<br>Coefficient: %{z:.3f}<extra></extra>\"\n",
    "))\n",
    "\n",
    "fig_coef_heatmap.update_layout(\n",
    " title=\"Multiclass Logistic Regression: Feature Coefficients by Category\",\n",
    " xaxis_title=\"Product Category\",\n",
    " yaxis_title=\"Features\",\n",
    " height=500\n",
    ")\n",
    "fig_coef_heatmap.show()\n",
    "\n",
    "# Confusion Matrix for multiclass\n",
    "cm_multi = confusion_matrix(y_multi_test, y_multi_pred)\n",
    "\n",
    "fig_cm_multi = ff.create_annotated_heatmap(\n",
    " z=cm_multi,\n",
    " x=categories,\n",
    " y=categories,\n",
    " annotation_text=cm_multi,\n",
    " colorscale='Blues',\n",
    " showscale=True\n",
    ")\n",
    "\n",
    "fig_cm_multi.update_layout(\n",
    " title=\"Multiclass Logistic Regression Confusion Matrix\",\n",
    " xaxis_title=\"Predicted Category\",\n",
    " yaxis_title=\"Actual Category\",\n",
    " height=500\n",
    ")\n",
    "fig_cm_multi.show()\n",
    "\n",
    "# Class probability analysis\n",
    "print(f\"\\n Class Probability Analysis:\")\n",
    "\n",
    "# Calculate average predicted probabilities for each true class\n",
    "prob_analysis = []\n",
    "for i, true_category in enumerate(categories):\n",
    " mask = y_multi_test == true_category\n",
    " if mask.sum() > 0:\n",
    " avg_probs = y_multi_proba[mask].mean(axis=0)\n",
    " max_prob_idx = avg_probs.argmax()\n",
    "\n",
    " prob_analysis.append({\n",
    " 'True_Category': true_category,\n",
    " 'Avg_Correct_Prob': avg_probs[i],\n",
    " 'Max_Prob_Category': categories[max_prob_idx],\n",
    " 'Max_Prob_Value': avg_probs[max_prob_idx]\n",
    " })\n",
    "\n",
    "prob_df = pd.DataFrame(prob_analysis)\n",
    "print(\"Average predicted probabilities:\")\n",
    "for _, row in prob_df.iterrows():\n",
    " print(f\"• {row['True_Category']}: {row['Avg_Correct_Prob']:.3f} correct probability\")\n",
    "\n",
    "# Visualize class probabilities\n",
    "fig_prob_analysis = go.Figure()\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    " # Get probabilities for this true category\n",
    " mask = y_multi_test == category\n",
    " if mask.sum() > 0:\n",
    " probs_for_category = y_multi_proba[mask, i]\n",
    "\n",
    " fig_prob_analysis.add_trace(\n",
    " go.Box(\n",
    " y=probs_for_category,\n",
    " name=category,\n",
    " boxpoints='outliers'\n",
    " )\n",
    " )\n",
    "\n",
    "fig_prob_analysis.update_layout(\n",
    " title=\"Predicted Probability Distributions by True Category\",\n",
    " xaxis_title=\"True Product Category\",\n",
    " yaxis_title=\"Predicted Probability for True Class\",\n",
    " height=500\n",
    ")\n",
    "fig_prob_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1418c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. REGULARIZED LOGISTIC REGRESSION\n",
    "print(\" 3. REGULARIZED LOGISTIC REGRESSION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Prepare high-dimensional data\n",
    "reg_features = [col for col in regularization_df.columns if col != 'target']\n",
    "X_reg = regularization_df[reg_features]\n",
    "y_reg = regularization_df['target']\n",
    "\n",
    "print(f\"Dataset shape: {X_reg.shape}\")\n",
    "print(f\"Number of features: {len(reg_features)}\")\n",
    "\n",
    "# Split data\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    " X_reg, y_reg, test_size=0.2, random_state=42, stratify=y_reg\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_reg = StandardScaler()\n",
    "X_reg_train_scaled = scaler_reg.fit_transform(X_reg_train)\n",
    "X_reg_test_scaled = scaler_reg.transform(X_reg_test)\n",
    "\n",
    "print(f\"Training set: {X_reg_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_reg_test_scaled.shape}\")\n",
    "\n",
    "# Compare different regularization approaches\n",
    "regularization_methods = {\n",
    " 'No Regularization': {'penalty': 'none', 'solver': 'lbfgs'},\n",
    " 'L2 (Ridge)': {'penalty': 'l2', 'solver': 'lbfgs'},\n",
    " 'L1 (Lasso)': {'penalty': 'l1', 'solver': 'liblinear'},\n",
    " 'Elastic Net': {'penalty': 'elasticnet', 'solver': 'saga', 'l1_ratio': 0.5}\n",
    "}\n",
    "\n",
    "reg_results = {}\n",
    "\n",
    "print(f\"\\n Regularization Methods Comparison:\")\n",
    "\n",
    "for method_name, params in regularization_methods.items():\n",
    " if method_name == 'No Regularization':\n",
    " lr_reg = LogisticRegression(random_state=42, max_iter=1000, **params)\n",
    " else:\n",
    " lr_reg = LogisticRegression(C=1.0, random_state=42, max_iter=1000, **params)\n",
    "\n",
    " try:\n",
    " # Fit model\n",
    " lr_reg.fit(X_reg_train_scaled, y_reg_train)\n",
    "\n",
    " # Evaluate\n",
    " train_score = lr_reg.score(X_reg_train_scaled, y_reg_train)\n",
    " test_score = lr_reg.score(X_reg_test_scaled, y_reg_test)\n",
    "\n",
    " # Count non-zero coefficients\n",
    " if hasattr(lr_reg, 'coef_'):\n",
    " non_zero_coefs = np.sum(np.abs(lr_reg.coef_[0]) > 1e-5)\n",
    " else:\n",
    " non_zero_coefs = len(reg_features)\n",
    "\n",
    " reg_results[method_name] = {\n",
    " 'train_accuracy': train_score,\n",
    " 'test_accuracy': test_score,\n",
    " 'non_zero_coefficients': non_zero_coefs,\n",
    " 'model': lr_reg\n",
    " }\n",
    "\n",
    " print(f\"• {method_name}: Train={train_score:.4f}, Test={test_score:.4f}, Features={non_zero_coefs}\")\n",
    "\n",
    " except Exception as e:\n",
    " print(f\"• {method_name}: Failed - {str(e)}\")\n",
    "\n",
    "# C parameter optimization for L1 and L2\n",
    "print(f\"\\n Regularization Strength (C) Optimization:\")\n",
    "\n",
    "C_values = np.logspace(-3, 2, 10)\n",
    "l1_scores = []\n",
    "l2_scores = []\n",
    "l1_features = []\n",
    "l2_features = []\n",
    "\n",
    "for C in C_values:\n",
    " # L1 regularization\n",
    " lr_l1 = LogisticRegression(penalty='l1', C=C, solver='liblinear', random_state=42)\n",
    " lr_l1.fit(X_reg_train_scaled, y_reg_train)\n",
    " l1_scores.append(lr_l1.score(X_reg_test_scaled, y_reg_test))\n",
    " l1_features.append(np.sum(np.abs(lr_l1.coef_[0]) > 1e-5))\n",
    "\n",
    " # L2 regularization\n",
    " lr_l2 = LogisticRegression(penalty='l2', C=C, solver='lbfgs', random_state=42)\n",
    " lr_l2.fit(X_reg_train_scaled, y_reg_train)\n",
    " l2_scores.append(lr_l2.score(X_reg_test_scaled, y_reg_test))\n",
    " l2_features.append(np.sum(np.abs(lr_l2.coef_[0]) > 1e-5))\n",
    "\n",
    "# Find optimal C values\n",
    "optimal_l1_idx = np.argmax(l1_scores)\n",
    "optimal_l2_idx = np.argmax(l2_scores)\n",
    "optimal_C_l1 = C_values[optimal_l1_idx]\n",
    "optimal_C_l2 = C_values[optimal_l2_idx]\n",
    "\n",
    "print(f\"• Optimal C for L1: {optimal_C_l1:.3f} (Accuracy: {l1_scores[optimal_l1_idx]:.4f})\")\n",
    "print(f\"• Optimal C for L2: {optimal_C_l2:.3f} (Accuracy: {l2_scores[optimal_l2_idx]:.4f})\")\n",
    "\n",
    "# Visualize regularization path\n",
    "fig_reg_path = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=['Test Accuracy vs C', 'Number of Features vs C']\n",
    ")\n",
    "\n",
    "# Accuracy plot\n",
    "fig_reg_path.add_trace(\n",
    " go.Scatter(\n",
    " x=C_values,\n",
    " y=l1_scores,\n",
    " mode='lines+markers',\n",
    " name='L1 (Lasso)',\n",
    " line=dict(color='blue')\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig_reg_path.add_trace(\n",
    " go.Scatter(\n",
    " x=C_values,\n",
    " y=l2_scores,\n",
    " mode='lines+markers',\n",
    " name='L2 (Ridge)',\n",
    " line=dict(color='red'),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Feature count plot\n",
    "fig_reg_path.add_trace(\n",
    " go.Scatter(\n",
    " x=C_values,\n",
    " y=l1_features,\n",
    " mode='lines+markers',\n",
    " name='L1 Features',\n",
    " line=dict(color='blue'),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_reg_path.add_trace(\n",
    " go.Scatter(\n",
    " x=C_values,\n",
    " y=l2_features,\n",
    " mode='lines+markers',\n",
    " name='L2 Features',\n",
    " line=dict(color='red'),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_reg_path.update_xaxes(type=\"log\", title_text=\"C (Regularization Strength)\", row=1, col=1)\n",
    "fig_reg_path.update_xaxes(type=\"log\", title_text=\"C (Regularization Strength)\", row=1, col=2)\n",
    "fig_reg_path.update_yaxes(title_text=\"Test Accuracy\", row=1, col=1)\n",
    "fig_reg_path.update_yaxes(title_text=\"Number of Non-zero Features\", row=1, col=2)\n",
    "\n",
    "fig_reg_path.update_layout(\n",
    " title=\"Regularized Logistic Regression: Regularization Path Analysis\",\n",
    " height=500\n",
    ")\n",
    "fig_reg_path.show()\n",
    "\n",
    "# Feature selection analysis with L1\n",
    "print(f\"\\n Feature Selection with L1 Regularization:\")\n",
    "\n",
    "# Train L1 model with optimal C\n",
    "lr_l1_optimal = LogisticRegression(\n",
    " penalty='l1',\n",
    " C=optimal_C_l1,\n",
    " solver='liblinear',\n",
    " random_state=42\n",
    ")\n",
    "lr_l1_optimal.fit(X_reg_train_scaled, y_reg_train)\n",
    "\n",
    "# Analyze selected features\n",
    "l1_coefficients = lr_l1_optimal.coef_[0]\n",
    "selected_features = np.abs(l1_coefficients) > 1e-5\n",
    "selected_feature_names = np.array(reg_features)[selected_features]\n",
    "selected_coefficients = l1_coefficients[selected_features]\n",
    "\n",
    "print(f\"Selected {len(selected_feature_names)} out of {len(reg_features)} features:\")\n",
    "\n",
    "# Sort by coefficient magnitude\n",
    "feature_importance = pd.DataFrame({\n",
    " 'Feature': selected_feature_names,\n",
    " 'Coefficient': selected_coefficients,\n",
    " 'Abs_Coefficient': np.abs(selected_coefficients)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "for _, row in feature_importance.iterrows():\n",
    " feature_type = \"Signal\" if row['Feature'].startswith('signal') else \\\n",
    " \"Correlated\" if row['Feature'].startswith('correlated') else \"Noise\"\n",
    " print(f\"• {row['Feature']} ({feature_type}): {row['Coefficient']:.3f}\")\n",
    "\n",
    "# Visualize selected features\n",
    "fig_feature_selection = go.Figure()\n",
    "\n",
    "colors = ['green' if name.startswith('signal') else\n",
    " 'orange' if name.startswith('correlated') else 'red'\n",
    " for name in feature_importance['Feature']]\n",
    "\n",
    "fig_feature_selection.add_trace(\n",
    " go.Bar(\n",
    " x=feature_importance['Coefficient'],\n",
    " y=feature_importance['Feature'],\n",
    " orientation='h',\n",
    " marker_color=colors,\n",
    " hovertemplate=\"Feature: %{y}<br>Coefficient: %{x:.3f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_feature_selection.update_layout(\n",
    " title=f\"L1 Regularization Feature Selection (C={optimal_C_l1:.3f})\",\n",
    " xaxis_title=\"Coefficient Value\",\n",
    " yaxis_title=\"Selected Features\",\n",
    " height=600\n",
    ")\n",
    "fig_feature_selection.show()\n",
    "\n",
    "# Evaluate feature selection quality\n",
    "true_signal_features = [f for f in selected_feature_names if f.startswith('signal')]\n",
    "false_positive_features = [f for f in selected_feature_names if f.startswith('noise')]\n",
    "\n",
    "print(f\"\\n Feature Selection Quality:\")\n",
    "print(f\"• True signal features detected: {len(true_signal_features)}/5\")\n",
    "print(f\"• False positive features: {len(false_positive_features)}\")\n",
    "print(f\"• Selection precision: {len(true_signal_features)/len(selected_feature_names):.3f}\")\n",
    "print(f\"• Selection recall: {len(true_signal_features)/5:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LOGISTIC REGRESSION ASSUMPTIONS AND DIAGNOSTICS\n",
    "print(\" 4. LOGISTIC REGRESSION ASSUMPTIONS AND DIAGNOSTICS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Use the binary dataset for assumption checking\n",
    "print(\"Analyzing assumptions using customer conversion dataset:\")\n",
    "\n",
    "# 1. Linear relationship between logit and continuous predictors\n",
    "print(f\"\\n1. LINEARITY ASSUMPTION (Logit-Predictor Relationship):\")\n",
    "\n",
    "# Calculate logit values for different income levels\n",
    "income_ranges = np.linspace(X_binary['income'].min(), X_binary['income'].max(), 20)\n",
    "empirical_logits = []\n",
    "income_midpoints = []\n",
    "\n",
    "for i in range(len(income_ranges)-1):\n",
    " income_min, income_max = income_ranges[i], income_ranges[i+1]\n",
    " mask = (binary_df['income'] >= income_min) & (binary_df['income'] < income_max)\n",
    "\n",
    " if mask.sum() > 10: # Ensure sufficient samples\n",
    " conversion_rate = binary_df[mask]['converted'].mean()\n",
    " if 0 < conversion_rate < 1: # Avoid log(0) or log(inf)\n",
    " empirical_logit = np.log(conversion_rate / (1 - conversion_rate))\n",
    " empirical_logits.append(empirical_logit)\n",
    " income_midpoints.append((income_min + income_max) / 2)\n",
    "\n",
    "# Check linearity\n",
    "fig_linearity = go.Figure()\n",
    "\n",
    "fig_linearity.add_trace(\n",
    " go.Scatter(\n",
    " x=income_midpoints,\n",
    " y=empirical_logits,\n",
    " mode='markers+lines',\n",
    " name='Empirical Logit',\n",
    " line=dict(color='blue')\n",
    " )\n",
    ")\n",
    "\n",
    "# Fit linear trend\n",
    "if len(income_midpoints) > 1:\n",
    " z = np.polyfit(income_midpoints, empirical_logits, 1)\n",
    " p = np.poly1d(z)\n",
    "\n",
    " fig_linearity.add_trace(\n",
    " go.Scatter(\n",
    " x=income_midpoints,\n",
    " y=p(income_midpoints),\n",
    " mode='lines',\n",
    " name='Linear Trend',\n",
    " line=dict(color='red', dash='dash')\n",
    " )\n",
    " )\n",
    "\n",
    "fig_linearity.update_layout(\n",
    " title=\"Linearity Check: Empirical Logit vs Income\",\n",
    " xaxis_title=\"Income ($)\",\n",
    " yaxis_title=\"Empirical Logit\",\n",
    " height=500\n",
    ")\n",
    "fig_linearity.show()\n",
    "\n",
    "# 2. Independence of observations\n",
    "print(f\"\\n2. INDEPENDENCE ASSUMPTION:\")\n",
    "print(f\"• Dataset assumes independent customer observations\")\n",
    "print(f\"• No time series or clustering structure in the data\")\n",
    "print(f\"• Assumption: SATISFIED (by design)\")\n",
    "\n",
    "# 3. No multicollinearity\n",
    "print(f\"\\n3. MULTICOLLINEARITY CHECK:\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = X_binary.corr()\n",
    "\n",
    "# Find high correlations\n",
    "high_correlations = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    " for j in range(i+1, len(correlation_matrix.columns)):\n",
    " corr_val = correlation_matrix.iloc[i, j]\n",
    " if abs(corr_val) > 0.7: # Threshold for concern\n",
    " high_correlations.append({\n",
    " 'feature1': correlation_matrix.columns[i],\n",
    " 'feature2': correlation_matrix.columns[j],\n",
    " 'correlation': corr_val\n",
    " })\n",
    "\n",
    "if high_correlations:\n",
    " print(\"High correlations detected (|r| > 0.7):\")\n",
    " for corr in high_correlations:\n",
    " print(f\"• {corr['feature1']} ↔ {corr['feature2']}: {corr['correlation']:.3f}\")\n",
    "else:\n",
    " print(\"• No problematic multicollinearity detected\")\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig_corr = go.Figure(data=go.Heatmap(\n",
    " z=correlation_matrix.values,\n",
    " x=correlation_matrix.columns,\n",
    " y=correlation_matrix.columns,\n",
    " colorscale='RdBu',\n",
    " zmid=0,\n",
    " hovertemplate=\"Feature 1: %{y}<br>Feature 2: %{x}<br>Correlation: %{z:.3f}<extra></extra>\"\n",
    "))\n",
    "\n",
    "fig_corr.update_layout(\n",
    " title=\"Feature Correlation Matrix\",\n",
    " height=600\n",
    ")\n",
    "fig_corr.show()\n",
    "\n",
    "# 4. Large sample size assumption\n",
    "print(f\"\\n4. SAMPLE SIZE ASSUMPTION:\")\n",
    "print(f\"• Total samples: {len(binary_df)}\")\n",
    "print(f\"• Samples per feature: {len(binary_df)/len(binary_features):.1f}\")\n",
    "print(f\"• Minimum class size: {min(y_binary.value_counts())}\")\n",
    "print(f\"• Rule of thumb: 10-20 samples per feature \")\n",
    "\n",
    "# 5. Model fit assessment\n",
    "print(f\"\\n5. MODEL FIT ASSESSMENT:\")\n",
    "\n",
    "# Hosmer-Lemeshow-like test using deciles\n",
    "y_pred_proba_full = lr_basic.predict_proba(X_bin_train_scaled)[:, 1]\n",
    "deciles = pd.qcut(y_pred_proba_full, q=10, duplicates='drop')\n",
    "\n",
    "hl_data = []\n",
    "for decile in deciles.cat.categories:\n",
    " mask = deciles == decile\n",
    " observed = y_bin_train[mask].sum()\n",
    " expected = y_pred_proba_full[mask].sum()\n",
    " total = mask.sum()\n",
    "\n",
    " hl_data.append({\n",
    " 'decile': str(decile),\n",
    " 'observed': observed,\n",
    " 'expected': expected,\n",
    " 'total': total,\n",
    " 'observed_rate': observed/total if total > 0 else 0,\n",
    " 'expected_rate': expected/total if total > 0 else 0\n",
    " })\n",
    "\n",
    "hl_df = pd.DataFrame(hl_data)\n",
    "\n",
    "print(\"Goodness of Fit (Observed vs Expected by Probability Decile):\")\n",
    "for _, row in hl_df.iterrows():\n",
    " print(f\"• {row['decile']}: Obs={row['observed']:.0f}, Exp={row['expected']:.1f}, \"\n",
    " f\"Rate: {row['observed_rate']:.3f} vs {row['expected_rate']:.3f}\")\n",
    "\n",
    "# Visualize calibration\n",
    "fig_calibration = go.Figure()\n",
    "\n",
    "fig_calibration.add_trace(\n",
    " go.Scatter(\n",
    " x=hl_df['expected_rate'],\n",
    " y=hl_df['observed_rate'],\n",
    " mode='markers+lines',\n",
    " name='Model Calibration',\n",
    " marker=dict(size=8, color='blue')\n",
    " )\n",
    ")\n",
    "\n",
    "# Perfect calibration line\n",
    "fig_calibration.add_trace(\n",
    " go.Scatter(\n",
    " x=[0, 1],\n",
    " y=[0, 1],\n",
    " mode='lines',\n",
    " name='Perfect Calibration',\n",
    " line=dict(color='red', dash='dash')\n",
    " )\n",
    ")\n",
    "\n",
    "fig_calibration.update_layout(\n",
    " title=\"Model Calibration: Observed vs Expected Rates\",\n",
    " xaxis_title=\"Expected Conversion Rate\",\n",
    " yaxis_title=\"Observed Conversion Rate\",\n",
    " height=500\n",
    ")\n",
    "fig_calibration.show()\n",
    "\n",
    "# 6. Outlier detection\n",
    "print(f\"\\n6. OUTLIER DETECTION:\")\n",
    "\n",
    "# Calculate standardized residuals\n",
    "y_pred_full = lr_basic.predict(X_bin_train_scaled)\n",
    "residuals = y_bin_train - lr_basic.predict_proba(X_bin_train_scaled)[:, 1]\n",
    "standardized_residuals = residuals / np.std(residuals)\n",
    "\n",
    "# Identify potential outliers\n",
    "outlier_threshold = 2.5\n",
    "outliers = np.abs(standardized_residuals) > outlier_threshold\n",
    "n_outliers = outliers.sum()\n",
    "\n",
    "print(f\"• Outliers detected (|residual| > {outlier_threshold}): {n_outliers} ({n_outliers/len(standardized_residuals):.1%})\")\n",
    "\n",
    "if n_outliers > 0:\n",
    " print(f\"• Consider investigating extreme cases for data quality\")\n",
    "else:\n",
    " print(f\"• No significant outliers detected\")\n",
    "\n",
    "# Visualize residuals\n",
    "fig_residuals = go.Figure()\n",
    "\n",
    "fig_residuals.add_trace(\n",
    " go.Scatter(\n",
    " x=y_pred_proba_full,\n",
    " y=standardized_residuals,\n",
    " mode='markers',\n",
    " marker=dict(\n",
    " color=['red' if outlier else 'blue' for outlier in outliers],\n",
    " opacity=0.6\n",
    " ),\n",
    " hovertemplate=\"Predicted Prob: %{x:.3f}<br>Std Residual: %{y:.3f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_residuals.add_hline(y=0, line_dash=\"dash\", line_color=\"black\")\n",
    "fig_residuals.add_hline(y=outlier_threshold, line_dash=\"dash\", line_color=\"red\")\n",
    "fig_residuals.add_hline(y=-outlier_threshold, line_dash=\"dash\", line_color=\"red\")\n",
    "\n",
    "fig_residuals.update_layout(\n",
    " title=\"Residual Analysis: Standardized Residuals vs Predicted Probabilities\",\n",
    " xaxis_title=\"Predicted Probability\",\n",
    " yaxis_title=\"Standardized Residuals\",\n",
    " height=500\n",
    ")\n",
    "fig_residuals.show()\n",
    "\n",
    "print(f\"\\n ASSUMPTION SUMMARY:\")\n",
    "print(f\" Linearity: Empirical logit shows reasonable linear relationship\")\n",
    "print(f\" Independence: Satisfied by study design\")\n",
    "print(f\"{'' if not high_correlations else ''} Multicollinearity: {'No issues detected' if not high_correlations else 'Some high correlations present'}\")\n",
    "print(f\" Sample size: Adequate samples per feature\")\n",
    "print(f\" Model fit: Reasonable calibration observed\")\n",
    "print(f\"{'' if n_outliers < len(standardized_residuals)*0.05 else ''} Outliers: {'Minimal outliers' if n_outliers < len(standardized_residuals)*0.05 else 'Some outliers detected'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e82d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\n",
    "print(\" 5. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\")\n",
    "print(\"=\" * 54)\n",
    "\n",
    "# Comprehensive business analysis\n",
    "print(\" Logistic Regression Business Applications Analysis:\")\n",
    "\n",
    "print(f\"\\n1. MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\" • Customer Conversion (Binary): {bin_accuracy:.1%} accuracy, {bin_auc:.3f} AUC\")\n",
    "print(f\" • Product Classification (Multiclass): {multi_accuracy:.1%} accuracy\")\n",
    "print(f\" • High-dimensional Feature Selection: {len(selected_feature_names)}/{len(reg_features)} features selected\")\n",
    "\n",
    "# Conversion rate optimization insights\n",
    "print(f\"\\n2. CUSTOMER CONVERSION OPTIMIZATION:\")\n",
    "\n",
    "# Calculate impact of top features\n",
    "top_features = coef_df.head(3)\n",
    "print(f\" Top conversion drivers:\")\n",
    "for _, row in top_features.iterrows():\n",
    " feature = row['Feature']\n",
    " coef = row['Coefficient']\n",
    " odds_ratio = row['Odds_Ratio']\n",
    "\n",
    " if coef > 0:\n",
    " impact = f\"increases conversion odds by {(odds_ratio-1)*100:.1f}%\"\n",
    " else:\n",
    " impact = f\"decreases conversion odds by {(1-odds_ratio)*100:.1f}%\"\n",
    "\n",
    " print(f\" • {feature}: 1 unit increase {impact}\")\n",
    "\n",
    "# Calculate potential revenue impact\n",
    "baseline_conversion = y_binary.mean()\n",
    "customers_per_month = 10000\n",
    "revenue_per_conversion = 100\n",
    "current_monthly_revenue = customers_per_month * baseline_conversion * revenue_per_conversion\n",
    "\n",
    "# Estimate improvement with model-guided optimization\n",
    "model_improvement = 0.15 # 15% improvement in targeting efficiency\n",
    "improved_conversion = baseline_conversion * (1 + model_improvement)\n",
    "improved_monthly_revenue = customers_per_month * improved_conversion * revenue_per_conversion\n",
    "additional_revenue = improved_monthly_revenue - current_monthly_revenue\n",
    "\n",
    "print(f\"\\n Revenue Impact Analysis:\")\n",
    "print(f\" • Current conversion rate: {baseline_conversion:.1%}\")\n",
    "print(f\" • Current monthly revenue: ${current_monthly_revenue:,.0f}\")\n",
    "print(f\" • With 15% targeting improvement: ${improved_monthly_revenue:,.0f}\")\n",
    "print(f\" • Additional monthly revenue: ${additional_revenue:,.0f}\")\n",
    "print(f\" • Annual additional revenue: ${additional_revenue * 12:,.0f}\")\n",
    "\n",
    "# Product categorization insights\n",
    "print(f\"\\n3. PRODUCT CATEGORIZATION INSIGHTS:\")\n",
    "\n",
    "# Analyze most distinctive features for each category\n",
    "distinctive_features = {}\n",
    "for category in categories:\n",
    " cat_coefs = coef_multi_df[coef_multi_df['Category'] == category]\n",
    " cat_coefs = cat_coefs.sort_values('Coefficient', ascending=False)\n",
    "\n",
    " positive_features = cat_coefs[cat_coefs['Coefficient'] > 0].head(2)\n",
    " negative_features = cat_coefs[cat_coefs['Coefficient'] < 0].tail(2)\n",
    "\n",
    " distinctive_features[category] = {\n",
    " 'positive': positive_features['Feature'].tolist(),\n",
    " 'negative': negative_features['Feature'].tolist()\n",
    " }\n",
    "\n",
    "for category, features in distinctive_features.items():\n",
    " print(f\"\\n {category} Category Characteristics:\")\n",
    " print(f\" • Strong indicators: {', '.join(features['positive'])}\")\n",
    " print(f\" • Weak indicators: {', '.join(features['negative'])}\")\n",
    "\n",
    "# Inventory and pricing recommendations\n",
    "print(f\"\\n Operational Recommendations:\")\n",
    "print(f\" • Electronics: Focus on high-rating, tech-heavy products\")\n",
    "print(f\" • Clothing: Emphasize seasonal demand and trend responsiveness\")\n",
    "print(f\" • Books: Leverage rating quality and niche appeal\")\n",
    "print(f\" • Home: Balance price points with practical utility\")\n",
    "\n",
    "# Feature selection business value\n",
    "print(f\"\\n4. FEATURE SELECTION BUSINESS VALUE:\")\n",
    "\n",
    "feature_collection_costs = {\n",
    " 'signal': 5, # Important features cost more to collect\n",
    " 'correlated': 3, # Moderately expensive\n",
    " 'noise': 1 # Cheap but useless features\n",
    "}\n",
    "\n",
    "# Calculate cost savings from feature selection\n",
    "total_features = len(reg_features)\n",
    "selected_features_count = len(selected_feature_names)\n",
    "cost_reduction = total_features - selected_features_count\n",
    "\n",
    "original_cost = (5 * 5 + 3 * 3 + 1 * (total_features - 8)) * 1000 # Per 1000 customers\n",
    "selected_cost = len([f for f in selected_feature_names if f.startswith('signal')]) * 5 * 1000 + \\\n",
    " len([f for f in selected_feature_names if f.startswith('correlated')]) * 3 * 1000 + \\\n",
    " len([f for f in selected_feature_names if f.startswith('noise')]) * 1 * 1000\n",
    "\n",
    "print(f\" • Original feature collection cost: ${original_cost:,}/1000 customers\")\n",
    "print(f\" • Optimized feature collection cost: ${selected_cost:,}/1000 customers\")\n",
    "print(f\" • Cost savings: ${original_cost - selected_cost:,}/1000 customers\")\n",
    "print(f\" • Annual savings (100K customers): ${(original_cost - selected_cost) * 100:,}\")\n",
    "\n",
    "# Model interpretability advantages\n",
    "print(f\"\\n5. INTERPRETABILITY ADVANTAGES:\")\n",
    "print(f\" • Transparent decision-making process\")\n",
    "print(f\" • Regulatory compliance for credit/medical decisions\")\n",
    "print(f\" • Easy to explain to stakeholders and customers\")\n",
    "print(f\" • Direct feature impact quantification via odds ratios\")\n",
    "print(f\" • Probabilistic outputs enable risk-based decisions\")\n",
    "\n",
    "# Implementation strategy\n",
    "print(f\"\\n6. IMPLEMENTATION STRATEGY:\")\n",
    "\n",
    "print(f\"\\n Phase 1 - Pilot Deployment:\")\n",
    "print(f\" • Deploy customer conversion model for 20% of traffic\")\n",
    "print(f\" • A/B test against current conversion optimization\")\n",
    "print(f\" • Monitor probability calibration and business metrics\")\n",
    "print(f\" • Expected timeline: 2-3 months\")\n",
    "\n",
    "print(f\"\\n Phase 2 - Product Classification:\")\n",
    "print(f\" • Implement automated product categorization\")\n",
    "print(f\" • Start with high-confidence predictions (>80% probability)\")\n",
    "print(f\" • Human review for ambiguous cases\")\n",
    "print(f\" • Expected timeline: 1-2 months\")\n",
    "\n",
    "print(f\"\\n Phase 3 - Feature Optimization:\")\n",
    "print(f\" • Implement L1-regularized feature selection pipeline\")\n",
    "print(f\" • Reduce data collection costs by {(cost_reduction/total_features)*100:.0f}%\")\n",
    "print(f\" • Maintain model performance monitoring\")\n",
    "print(f\" • Expected timeline: 3-4 months\")\n",
    "\n",
    "print(f\"\\n7. MONITORING AND MAINTENANCE:\")\n",
    "print(f\" • Track model performance metrics weekly\")\n",
    "print(f\" • Monitor probability calibration monthly\")\n",
    "print(f\" • Retrain models quarterly or when performance degrades\")\n",
    "print(f\" • Validate assumptions semi-annually\")\n",
    "print(f\" • Update feature importance analysis with new data\")\n",
    "\n",
    "print(f\"\\n8. RISK MITIGATION:\")\n",
    "print(f\" • Set probability thresholds for automatic decisions\")\n",
    "print(f\" • Implement human oversight for edge cases\")\n",
    "print(f\" • Regular bias audits for fairness\")\n",
    "print(f\" • Backup models for system redundancy\")\n",
    "print(f\" • Data quality monitoring and alerts\")\n",
    "\n",
    "print(f\"\\n9. ADVANCED EXTENSIONS:\")\n",
    "print(f\" • Ensemble methods combining multiple logistic models\")\n",
    "print(f\" • Time-series features for temporal patterns\")\n",
    "print(f\" • Interaction terms for feature combinations\")\n",
    "print(f\" • Online learning for real-time model updates\")\n",
    "print(f\" • Causal inference for treatment effect estimation\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\" LOGISTIC REGRESSION LEARNING SUMMARY:\")\n",
    "print(f\" Mastered sigmoid function and maximum likelihood estimation\")\n",
    "print(f\" Applied binary and multiclass logistic regression\")\n",
    "print(f\" Implemented L1/L2 regularization for feature selection\")\n",
    "print(f\" Analyzed model assumptions and diagnostic procedures\")\n",
    "print(f\" Interpreted coefficients and odds ratios for business insights\")\n",
    "print(f\" Generated comprehensive implementation and ROI strategies\")\n",
    "print(f\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}