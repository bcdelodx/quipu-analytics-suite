{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 2: Random Forest\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** af9c8a07-a69a-44d9-bb82-a24d738f13c3\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 2: Random Forest,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** af9c8a07-a69a-44d9-bb82-a24d738f13c3\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Essential Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 2: Random Forest - Libraries Loaded Successfully!\")\n",
    "print(\"=\" * 65)\n",
    "print(\"Available Random Forest Techniques:\")\n",
    "print(\"• Random Forest Classification - Ensemble voting for robust classification\")\n",
    "print(\"• Random Forest Regression - Ensemble averaging for continuous prediction\")\n",
    "print(\"• Feature Importance Analysis - Variable ranking and selection\")\n",
    "print(\"• Out-of-Bag (OOB) Validation - Built-in model validation\")\n",
    "print(\"• Hyperparameter Optimization - n_estimators, max_depth, min_samples tuning\")\n",
    "print(\"• Ensemble Interpretation - Understanding collective decision making\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b2646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Comprehensive Datasets for Random Forest Analysis\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_random_forest_datasets():\n",
    " \"\"\"Generate datasets optimized for Random Forest analysis\"\"\"\n",
    "\n",
    " # 1. CLASSIFICATION DATASET - Employee Performance Prediction\n",
    " n_employees = 1200\n",
    "\n",
    " # Employee demographics\n",
    " age = np.random.normal(35, 10, n_employees)\n",
    " age = np.clip(age, 22, 65)\n",
    "\n",
    " years_experience = np.random.exponential(scale=5, size=n_employees) + 1\n",
    " years_experience = np.clip(years_experience, 1, 30)\n",
    "\n",
    " education_level = np.random.choice([1, 2, 3, 4], size=n_employees, p=[0.2, 0.3, 0.3, 0.2])\n",
    " # 1=High School, 2=Bachelor's, 3=Master's, 4=PhD\n",
    "\n",
    " # Performance-related features\n",
    " training_hours = np.random.gamma(shape=2, scale=20, size=n_employees)\n",
    " training_hours = np.clip(training_hours, 5, 120)\n",
    "\n",
    " projects_completed = np.random.poisson(lam=8, size=n_employees) + 1\n",
    "\n",
    " team_size = np.random.choice([3, 5, 8, 12, 15], size=n_employees, p=[0.2, 0.3, 0.25, 0.15, 0.1])\n",
    "\n",
    " work_from_home_days = np.random.poisson(lam=2, size=n_employees)\n",
    " work_from_home_days = np.clip(work_from_home_days, 0, 5)\n",
    "\n",
    " # Behavioral features\n",
    " meeting_attendance = np.random.beta(a=8, b=2, size=n_employees) # High attendance generally\n",
    "\n",
    " peer_collaboration_score = np.random.normal(7, 2, n_employees)\n",
    " peer_collaboration_score = np.clip(peer_collaboration_score, 1, 10)\n",
    "\n",
    " innovation_score = np.random.gamma(shape=3, scale=2, size=n_employees)\n",
    " innovation_score = np.clip(innovation_score, 1, 10)\n",
    "\n",
    " # Department (affects performance patterns)\n",
    " departments = ['Engineering', 'Sales', 'Marketing', 'HR', 'Finance']\n",
    " department = np.random.choice(departments, size=n_employees, p=[0.3, 0.25, 0.2, 0.15, 0.1])\n",
    "\n",
    " # Create realistic performance ratings with complex interactions\n",
    " performance_score = (\n",
    " 0.1 * (age - 35) / 10 + # Slight age effect\n",
    " 0.2 * (years_experience - 5) / 10 + # Experience matters\n",
    " 0.15 * (education_level - 2) + # Education impact\n",
    " 0.2 * (training_hours - 40) / 40 + # Training effect\n",
    " 0.1 * (projects_completed - 8) / 5 + # Project completion\n",
    " -0.05 * (team_size - 8) / 5 + # Smaller teams might be better\n",
    " 0.1 * (meeting_attendance - 0.8) / 0.2 + # Attendance matters\n",
    " 0.15 * (peer_collaboration_score - 7) / 3 + # Collaboration\n",
    " 0.1 * (innovation_score - 6) / 4 + # Innovation\n",
    " np.random.normal(0, 0.3, n_employees) # Random variation\n",
    " )\n",
    "\n",
    " # Adjust for department effects\n",
    " dept_effects = {'Engineering': 0.1, 'Sales': 0.05, 'Marketing': 0.0, 'HR': -0.05, 'Finance': 0.02}\n",
    " for i, dept in enumerate(department):\n",
    " performance_score[i] += dept_effects[dept]\n",
    "\n",
    " # Convert to performance categories\n",
    " # Use percentiles to create balanced classes\n",
    " performance_percentiles = np.percentile(performance_score, [33, 67])\n",
    " performance_rating = np.zeros(n_employees, dtype=int)\n",
    " performance_rating[performance_score <= performance_percentiles[0]] = 0 # Needs Improvement\n",
    " performance_rating[(performance_score > performance_percentiles[0]) &\n",
    " (performance_score <= performance_percentiles[1])] = 1 # Meets Expectations\n",
    " performance_rating[performance_score > performance_percentiles[1]] = 2 # Exceeds Expectations\n",
    "\n",
    " # Encode department as numerical\n",
    " dept_encoder = LabelEncoder()\n",
    " department_encoded = dept_encoder.fit_transform(department)\n",
    "\n",
    " classification_df = pd.DataFrame({\n",
    " 'age': age,\n",
    " 'years_experience': years_experience,\n",
    " 'education_level': education_level,\n",
    " 'training_hours': training_hours,\n",
    " 'projects_completed': projects_completed,\n",
    " 'team_size': team_size,\n",
    " 'work_from_home_days': work_from_home_days,\n",
    " 'meeting_attendance': meeting_attendance,\n",
    " 'peer_collaboration_score': peer_collaboration_score,\n",
    " 'innovation_score': innovation_score,\n",
    " 'department': department_encoded,\n",
    " 'performance_rating': performance_rating\n",
    " })\n",
    "\n",
    " # 2. REGRESSION DATASET - Real Estate Price Prediction\n",
    " n_houses = 1000\n",
    "\n",
    " # Property characteristics\n",
    " house_size = np.random.gamma(shape=3, scale=600, size=n_houses) + 800\n",
    " house_size = np.clip(house_size, 800, 4000)\n",
    "\n",
    " bedrooms = np.random.poisson(lam=3, size=n_houses) + 1\n",
    " bedrooms = np.clip(bedrooms, 1, 6)\n",
    "\n",
    " bathrooms = np.random.poisson(lam=2, size=n_houses) + 1\n",
    " bathrooms = np.clip(bathrooms, 1, 5)\n",
    "\n",
    " garage_spaces = np.random.choice([0, 1, 2, 3], size=n_houses, p=[0.1, 0.3, 0.5, 0.1])\n",
    "\n",
    " house_age = np.random.exponential(scale=15, size=n_houses) + 1\n",
    " house_age = np.clip(house_age, 1, 100)\n",
    "\n",
    " lot_size = np.random.gamma(shape=2, scale=3000, size=n_houses) + 2000\n",
    " lot_size = np.clip(lot_size, 2000, 20000)\n",
    "\n",
    " # Location features\n",
    " distance_to_downtown = np.random.exponential(scale=8, size=n_houses) + 1\n",
    " distance_to_downtown = np.clip(distance_to_downtown, 1, 30)\n",
    "\n",
    " school_rating = np.random.beta(a=5, b=2, size=n_houses) * 10 + 1\n",
    " school_rating = np.clip(school_rating, 1, 10)\n",
    "\n",
    " crime_rate = np.random.exponential(scale=3, size=n_houses) + 0.5\n",
    " crime_rate = np.clip(crime_rate, 0.5, 15)\n",
    "\n",
    " # Neighborhood amenities\n",
    " parks_nearby = np.random.poisson(lam=2, size=n_houses)\n",
    " parks_nearby = np.clip(parks_nearby, 0, 8)\n",
    "\n",
    " shopping_centers_nearby = np.random.poisson(lam=1.5, size=n_houses)\n",
    " shopping_centers_nearby = np.clip(shopping_centers_nearby, 0, 5)\n",
    "\n",
    " # Property condition and features\n",
    " renovation_score = np.random.beta(a=3, b=2, size=n_houses) * 10\n",
    " renovation_score = np.clip(renovation_score, 1, 10)\n",
    "\n",
    " has_pool = np.random.binomial(1, 0.25, n_houses)\n",
    " has_fireplace = np.random.binomial(1, 0.4, n_houses)\n",
    " has_basement = np.random.binomial(1, 0.6, n_houses)\n",
    "\n",
    " # Generate house prices with complex non-linear relationships\n",
    " base_price = (\n",
    " house_size * 120 + # Base price per sq ft\n",
    " bedrooms * 15000 + # Bedroom premium\n",
    " bathrooms * 12000 + # Bathroom premium\n",
    " garage_spaces * 8000 + # Garage value\n",
    " -house_age * 1000 + # Depreciation\n",
    " lot_size * 10 + # Lot size value\n",
    " -distance_to_downtown * 2000 + # Location premium\n",
    " school_rating * 8000 + # School quality\n",
    " -crime_rate * 3000 + # Safety factor\n",
    " parks_nearby * 2000 + # Recreation access\n",
    " shopping_centers_nearby * 3000 + # Convenience\n",
    " renovation_score * 5000 + # Condition\n",
    " has_pool * 15000 + # Pool premium\n",
    " has_fireplace * 8000 + # Fireplace value\n",
    " has_basement * 12000 # Basement value\n",
    " )\n",
    "\n",
    " # Add non-linear interactions\n",
    " # Premium for large houses with many bedrooms\n",
    " luxury_bonus = np.where((house_size > 2500) & (bedrooms >= 4), 50000, 0)\n",
    "\n",
    " # Penalty for old houses far from downtown\n",
    " location_age_penalty = np.where((house_age > 30) & (distance_to_downtown > 15), -30000, 0)\n",
    "\n",
    " # Bonus for high-rated schools with low crime\n",
    " safe_school_bonus = np.where((school_rating > 8) & (crime_rate < 2), 25000, 0)\n",
    "\n",
    " house_price = (base_price + luxury_bonus + location_age_penalty + safe_school_bonus +\n",
    " np.random.normal(0, 20000, n_houses))\n",
    " house_price = np.maximum(house_price, 100000) # Minimum price floor\n",
    "\n",
    " regression_df = pd.DataFrame({\n",
    " 'house_size': house_size,\n",
    " 'bedrooms': bedrooms,\n",
    " 'bathrooms': bathrooms,\n",
    " 'garage_spaces': garage_spaces,\n",
    " 'house_age': house_age,\n",
    " 'lot_size': lot_size,\n",
    " 'distance_to_downtown': distance_to_downtown,\n",
    " 'school_rating': school_rating,\n",
    " 'crime_rate': crime_rate,\n",
    " 'parks_nearby': parks_nearby,\n",
    " 'shopping_centers_nearby': shopping_centers_nearby,\n",
    " 'renovation_score': renovation_score,\n",
    " 'has_pool': has_pool,\n",
    " 'has_fireplace': has_fireplace,\n",
    " 'has_basement': has_basement,\n",
    " 'price': house_price\n",
    " })\n",
    "\n",
    " # 3. HIGH-DIMENSIONAL DATASET - Gene Expression Classification\n",
    " n_samples = 400\n",
    " n_genes = 100 # Many features to showcase Random Forest's robustness\n",
    "\n",
    " # Generate gene expression data\n",
    " np.random.seed(42)\n",
    "\n",
    " # Create correlated gene groups (pathways)\n",
    " pathway_sizes = [10, 8, 12, 15, 20] # Different pathway sizes\n",
    " pathway_effects = [2.0, 1.5, 1.8, 1.2, 2.5] # Effect sizes\n",
    "\n",
    " gene_expression = np.random.normal(0, 1, (n_samples, n_genes))\n",
    "\n",
    " # Create two disease classes\n",
    " disease_status = np.random.binomial(1, 0.5, n_samples)\n",
    "\n",
    " # Add pathway effects for disease samples\n",
    " pathway_start = 0\n",
    " for pathway_size, effect in zip(pathway_sizes, pathway_effects):\n",
    " pathway_end = pathway_start + pathway_size\n",
    "\n",
    " # Disease samples have different expression in this pathway\n",
    " disease_mask = disease_status == 1\n",
    " gene_expression[disease_mask, pathway_start:pathway_end] += np.random.normal(\n",
    " effect, 0.5, (disease_mask.sum(), pathway_size)\n",
    " )\n",
    "\n",
    " pathway_start = pathway_end\n",
    "\n",
    " # Add noise genes (remaining genes are just noise)\n",
    " # These should have no predictive power\n",
    "\n",
    " # Create gene names\n",
    " gene_names = [f'Gene_{i+1:03d}' for i in range(n_genes)]\n",
    "\n",
    " # Create DataFrame\n",
    " gene_df = pd.DataFrame(gene_expression, columns=gene_names)\n",
    " gene_df['disease_status'] = disease_status\n",
    "\n",
    " return classification_df, regression_df, gene_df\n",
    "\n",
    "# Generate datasets\n",
    "print(\" Generating Random Forest optimized datasets...\")\n",
    "classification_df, regression_df, gene_df = generate_random_forest_datasets()\n",
    "\n",
    "print(f\"Classification Dataset (Employee Performance): {classification_df.shape}\")\n",
    "print(f\"Regression Dataset (House Prices): {regression_df.shape}\")\n",
    "print(f\"High-dimensional Dataset (Gene Expression): {gene_df.shape}\")\n",
    "\n",
    "print(\"\\nClassification Dataset (Employee Performance):\")\n",
    "print(classification_df.head())\n",
    "performance_labels = ['Needs Improvement', 'Meets Expectations', 'Exceeds Expectations']\n",
    "perf_counts = classification_df['performance_rating'].value_counts().sort_index()\n",
    "for i, count in enumerate(perf_counts):\n",
    " print(f\"• {performance_labels[i]}: {count} ({count/len(classification_df):.1%})\")\n",
    "\n",
    "print(\"\\nRegression Dataset (House Prices):\")\n",
    "print(regression_df.head())\n",
    "print(f\"Price Range: ${regression_df['price'].min():,.0f} - ${regression_df['price'].max():,.0f}\")\n",
    "print(f\"Median Price: ${regression_df['price'].median():,.0f}\")\n",
    "\n",
    "print(\"\\nHigh-dimensional Dataset (Gene Expression):\")\n",
    "print(gene_df.head())\n",
    "print(f\"Disease Distribution: {gene_df['disease_status'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9655da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. RANDOM FOREST CLASSIFICATION ANALYSIS\n",
    "print(\" 1. RANDOM FOREST CLASSIFICATION ANALYSIS\")\n",
    "print(\"=\" * 43)\n",
    "\n",
    "# Prepare classification data\n",
    "class_features = ['age', 'years_experience', 'education_level', 'training_hours',\n",
    " 'projects_completed', 'team_size', 'work_from_home_days',\n",
    " 'meeting_attendance', 'peer_collaboration_score', 'innovation_score', 'department']\n",
    "X_class = classification_df[class_features]\n",
    "y_class = classification_df['performance_rating']\n",
    "\n",
    "# Split data\n",
    "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(\n",
    " X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_class_train.shape}\")\n",
    "print(f\"Test set: {X_class_test.shape}\")\n",
    "print(f\"Class distribution: {y_class_train.value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "# Train basic Random Forest\n",
    "rf_basic = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_basic.fit(X_class_train, y_class_train)\n",
    "\n",
    "# Predictions\n",
    "y_class_pred = rf_basic.predict(X_class_test)\n",
    "y_class_proba = rf_basic.predict_proba(X_class_test)\n",
    "\n",
    "# Performance metrics\n",
    "class_accuracy = accuracy_score(y_class_test, y_class_pred)\n",
    "oob_score = rf_basic.oob_score_ if hasattr(rf_basic, 'oob_score_') else None\n",
    "\n",
    "print(f\"\\n Basic Random Forest Performance:\")\n",
    "print(f\"• Test Accuracy: {class_accuracy:.4f}\")\n",
    "\n",
    "# Enable OOB scoring\n",
    "rf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf_oob.fit(X_class_train, y_class_train)\n",
    "print(f\"• OOB Score: {rf_oob.oob_score_:.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf_basic, X_class_train, y_class_train, cv=5)\n",
    "print(f\"• Cross-validation: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_class_test, y_class_pred, target_names=performance_labels))\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = rf_basic.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    " 'Feature': class_features,\n",
    " 'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\n Feature Importance Analysis:\")\n",
    "for _, row in importance_df.iterrows():\n",
    " print(f\"• {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "fig_importance = go.Figure()\n",
    "\n",
    "fig_importance.add_trace(\n",
    " go.Bar(\n",
    " x=importance_df['Importance'],\n",
    " y=importance_df['Feature'],\n",
    " orientation='h',\n",
    " marker_color='lightblue',\n",
    " hovertemplate=\"Feature: %{y}<br>Importance: %{x:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_importance.update_layout(\n",
    " title=\"Random Forest Feature Importance (Employee Performance)\",\n",
    " xaxis_title=\"Feature Importance\",\n",
    " yaxis_title=\"Features\",\n",
    " height=600\n",
    ")\n",
    "fig_importance.show()\n",
    "\n",
    "# Compare with single decision tree\n",
    "dt_single = DecisionTreeClassifier(random_state=42)\n",
    "dt_single.fit(X_class_train, y_class_train)\n",
    "dt_accuracy = dt_single.score(X_class_test, y_class_test)\n",
    "\n",
    "print(f\"\\n Single Decision Tree vs Random Forest:\")\n",
    "print(f\"• Single Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"• Random Forest Accuracy: {class_accuracy:.4f}\")\n",
    "print(f\"• Improvement: {(class_accuracy - dt_accuracy)*100:.1f} percentage points\")\n",
    "\n",
    "# Hyperparameter optimization\n",
    "print(f\"\\n Hyperparameter Optimization:\")\n",
    "\n",
    "# Grid search for optimal parameters\n",
    "param_grid = {\n",
    " 'n_estimators': [50, 100, 200],\n",
    " 'max_depth': [None, 10, 20],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Use a smaller grid for demonstration\n",
    "grid_search = GridSearchCV(\n",
    " RandomForestClassifier(random_state=42),\n",
    " param_grid,\n",
    " cv=3, # Reduced for speed\n",
    " scoring='accuracy',\n",
    " n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_class_train, y_class_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Test best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_accuracy = best_rf.score(X_class_test, y_class_test)\n",
    "print(f\"Best model test accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_class = confusion_matrix(y_class_test, y_class_pred)\n",
    "\n",
    "fig_cm_class = ff.create_annotated_heatmap(\n",
    " z=cm_class,\n",
    " x=performance_labels,\n",
    " y=performance_labels,\n",
    " annotation_text=cm_class,\n",
    " colorscale='Blues',\n",
    " showscale=True\n",
    ")\n",
    "\n",
    "fig_cm_class.update_layout(\n",
    " title=\"Random Forest Confusion Matrix (Employee Performance)\",\n",
    " xaxis_title=\"Predicted Performance\",\n",
    " yaxis_title=\"Actual Performance\",\n",
    " height=500\n",
    ")\n",
    "fig_cm_class.show()\n",
    "\n",
    "# Number of estimators effect\n",
    "print(f\"\\n Effect of Number of Estimators:\")\n",
    "\n",
    "n_estimators_range = [10, 25, 50, 100, 200, 300]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "oob_scores = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    " rf_temp = RandomForestClassifier(n_estimators=n_est, oob_score=True, random_state=42)\n",
    " rf_temp.fit(X_class_train, y_class_train)\n",
    "\n",
    " train_scores.append(rf_temp.score(X_class_train, y_class_train))\n",
    " test_scores.append(rf_temp.score(X_class_test, y_class_test))\n",
    " oob_scores.append(rf_temp.oob_score_)\n",
    "\n",
    "# Visualize effect of n_estimators\n",
    "fig_n_est = go.Figure()\n",
    "\n",
    "fig_n_est.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=train_scores,\n",
    " mode='lines+markers',\n",
    " name='Training Accuracy',\n",
    " line=dict(color='blue')\n",
    " )\n",
    ")\n",
    "\n",
    "fig_n_est.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=test_scores,\n",
    " mode='lines+markers',\n",
    " name='Test Accuracy',\n",
    " line=dict(color='red')\n",
    " )\n",
    ")\n",
    "\n",
    "fig_n_est.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=oob_scores,\n",
    " mode='lines+markers',\n",
    " name='OOB Score',\n",
    " line=dict(color='green')\n",
    " )\n",
    ")\n",
    "\n",
    "fig_n_est.update_layout(\n",
    " title=\"Random Forest: Effect of Number of Estimators\",\n",
    " xaxis_title=\"Number of Estimators\",\n",
    " yaxis_title=\"Accuracy\",\n",
    " height=500\n",
    ")\n",
    "fig_n_est.show()\n",
    "\n",
    "print(f\"Optimal number of estimators appears to be around 100-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. RANDOM FOREST REGRESSION ANALYSIS\n",
    "print(\" 2. RANDOM FOREST REGRESSION ANALYSIS\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "# Prepare regression data\n",
    "reg_features = ['house_size', 'bedrooms', 'bathrooms', 'garage_spaces', 'house_age',\n",
    " 'lot_size', 'distance_to_downtown', 'school_rating', 'crime_rate',\n",
    " 'parks_nearby', 'shopping_centers_nearby', 'renovation_score',\n",
    " 'has_pool', 'has_fireplace', 'has_basement']\n",
    "X_reg = regression_df[reg_features]\n",
    "y_reg = regression_df['price']\n",
    "\n",
    "# Split data\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    " X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_reg_train.shape}\")\n",
    "print(f\"Test set: {X_reg_test.shape}\")\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf_reg.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "# Predictions\n",
    "y_reg_pred = rf_reg.predict(X_reg_test)\n",
    "\n",
    "# Performance metrics\n",
    "reg_mse = mean_squared_error(y_reg_test, y_reg_pred)\n",
    "reg_r2 = r2_score(y_reg_test, y_reg_pred)\n",
    "reg_mae = mean_absolute_error(y_reg_test, y_reg_pred)\n",
    "\n",
    "print(f\"\\n Random Forest Regression Performance:\")\n",
    "print(f\"• Test R²: {reg_r2:.4f}\")\n",
    "print(f\"• Test RMSE: ${np.sqrt(reg_mse):,.0f}\")\n",
    "print(f\"• Test MAE: ${reg_mae:,.0f}\")\n",
    "print(f\"• OOB Score: {rf_reg.oob_score_:.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_reg = cross_val_score(rf_reg, X_reg_train, y_reg_train, cv=5, scoring='r2')\n",
    "print(f\"• Cross-validation R²: {cv_scores_reg.mean():.4f} ± {cv_scores_reg.std():.4f}\")\n",
    "\n",
    "# Feature importance for regression\n",
    "reg_importance = rf_reg.feature_importances_\n",
    "reg_importance_df = pd.DataFrame({\n",
    " 'Feature': reg_features,\n",
    " 'Importance': reg_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\n Feature Importance Analysis (House Price Prediction):\")\n",
    "for _, row in reg_importance_df.iterrows():\n",
    " print(f\"• {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "# Visualize regression feature importance\n",
    "fig_reg_importance = go.Figure()\n",
    "\n",
    "fig_reg_importance.add_trace(\n",
    " go.Bar(\n",
    " x=reg_importance_df['Importance'],\n",
    " y=reg_importance_df['Feature'],\n",
    " orientation='h',\n",
    " marker_color='lightgreen',\n",
    " hovertemplate=\"Feature: %{y}<br>Importance: %{x:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_reg_importance.update_layout(\n",
    " title=\"Random Forest Feature Importance (House Price Prediction)\",\n",
    " xaxis_title=\"Feature Importance\",\n",
    " yaxis_title=\"Features\",\n",
    " height=600\n",
    ")\n",
    "fig_reg_importance.show()\n",
    "\n",
    "# Actual vs Predicted plot\n",
    "fig_pred = go.Figure()\n",
    "\n",
    "fig_pred.add_trace(\n",
    " go.Scatter(\n",
    " x=y_reg_test,\n",
    " y=y_reg_pred,\n",
    " mode='markers',\n",
    " marker=dict(color='blue', opacity=0.6),\n",
    " name='Predictions',\n",
    " hovertemplate=\"Actual: $%{x:,.0f}<br>Predicted: $%{y:,.0f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "# Perfect prediction line\n",
    "min_price = min(y_reg_test.min(), y_reg_pred.min())\n",
    "max_price = max(y_reg_test.max(), y_reg_pred.max())\n",
    "\n",
    "fig_pred.add_trace(\n",
    " go.Scatter(\n",
    " x=[min_price, max_price],\n",
    " y=[min_price, max_price],\n",
    " mode='lines',\n",
    " line=dict(color='red', dash='dash'),\n",
    " name='Perfect Prediction',\n",
    " hovertemplate=\"Perfect Line<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_pred.update_layout(\n",
    " title=f\"Random Forest: Actual vs Predicted House Prices (R² = {reg_r2:.3f})\",\n",
    " xaxis_title=\"Actual Price ($)\",\n",
    " yaxis_title=\"Predicted Price ($)\",\n",
    " height=500\n",
    ")\n",
    "fig_pred.show()\n",
    "\n",
    "# Residuals analysis\n",
    "residuals = y_reg_test - y_reg_pred\n",
    "\n",
    "fig_residuals = go.Figure()\n",
    "\n",
    "fig_residuals.add_trace(\n",
    " go.Scatter(\n",
    " x=y_reg_pred,\n",
    " y=residuals,\n",
    " mode='markers',\n",
    " marker=dict(color='green', opacity=0.6),\n",
    " hovertemplate=\"Predicted: $%{x:,.0f}<br>Residual: $%{y:,.0f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_residuals.add_hline(y=0, line_dash=\"dash\", line_color=\"red\")\n",
    "\n",
    "fig_residuals.update_layout(\n",
    " title=\"Random Forest Residuals Analysis\",\n",
    " xaxis_title=\"Predicted Price ($)\",\n",
    " yaxis_title=\"Residuals ($)\",\n",
    " height=500\n",
    ")\n",
    "fig_residuals.show()\n",
    "\n",
    "# Permutation importance for more robust feature ranking\n",
    "print(f\"\\n Permutation Importance Analysis:\")\n",
    "\n",
    "perm_importance = permutation_importance(\n",
    " rf_reg, X_reg_test, y_reg_test, n_repeats=10, random_state=42\n",
    ")\n",
    "\n",
    "perm_importance_df = pd.DataFrame({\n",
    " 'Feature': reg_features,\n",
    " 'Perm_Importance_Mean': perm_importance.importances_mean,\n",
    " 'Perm_Importance_Std': perm_importance.importances_std\n",
    "}).sort_values('Perm_Importance_Mean', ascending=False)\n",
    "\n",
    "print(\"Permutation Importance (more robust than built-in importance):\")\n",
    "for _, row in perm_importance_df.head(5).iterrows():\n",
    " print(f\"• {row['Feature']}: {row['Perm_Importance_Mean']:.4f} ± {row['Perm_Importance_Std']:.4f}\")\n",
    "\n",
    "# Compare built-in vs permutation importance\n",
    "fig_importance_comparison = go.Figure()\n",
    "\n",
    "# Merge dataframes for comparison\n",
    "comparison_df = reg_importance_df.merge(\n",
    " perm_importance_df, on='Feature', suffixes=('_builtin', '_permutation')\n",
    ")\n",
    "\n",
    "fig_importance_comparison.add_trace(\n",
    " go.Scatter(\n",
    " x=comparison_df['Importance'],\n",
    " y=comparison_df['Perm_Importance_Mean'],\n",
    " mode='markers+text',\n",
    " text=comparison_df['Feature'],\n",
    " textposition='top center',\n",
    " marker=dict(size=10, color='blue'),\n",
    " name='Feature Importance Comparison',\n",
    " hovertemplate=\"Built-in: %{x:.4f}<br>Permutation: %{y:.4f}<br>Feature: %{text}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "# Add diagonal line for perfect correlation\n",
    "max_importance = max(comparison_df['Importance'].max(), comparison_df['Perm_Importance_Mean'].max())\n",
    "fig_importance_comparison.add_trace(\n",
    " go.Scatter(\n",
    " x=[0, max_importance],\n",
    " y=[0, max_importance],\n",
    " mode='lines',\n",
    " line=dict(color='red', dash='dash'),\n",
    " name='Perfect Correlation',\n",
    " showlegend=False\n",
    " )\n",
    ")\n",
    "\n",
    "fig_importance_comparison.update_layout(\n",
    " title=\"Built-in vs Permutation Feature Importance\",\n",
    " xaxis_title=\"Built-in Importance\",\n",
    " yaxis_title=\"Permutation Importance\",\n",
    " height=600\n",
    ")\n",
    "fig_importance_comparison.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279abd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. HIGH-DIMENSIONAL DATA ANALYSIS\n",
    "print(\" 3. HIGH-DIMENSIONAL DATA ANALYSIS\")\n",
    "print(\"=\" * 33)\n",
    "\n",
    "# Prepare gene expression data\n",
    "gene_features = [col for col in gene_df.columns if col != 'disease_status']\n",
    "X_gene = gene_df[gene_features]\n",
    "y_gene = gene_df['disease_status']\n",
    "\n",
    "print(f\"Gene expression dataset: {X_gene.shape}\")\n",
    "print(f\"Number of features: {len(gene_features)}\")\n",
    "print(f\"Sample size: {len(y_gene)}\")\n",
    "\n",
    "# Split data\n",
    "X_gene_train, X_gene_test, y_gene_train, y_gene_test = train_test_split(\n",
    " X_gene, y_gene, test_size=0.2, random_state=42, stratify=y_gene\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_gene_train.shape}\")\n",
    "print(f\"Test set: {X_gene_test.shape}\")\n",
    "\n",
    "# Train Random Forest on high-dimensional data\n",
    "rf_gene = RandomForestClassifier(\n",
    " n_estimators=200, # More trees for stability with many features\n",
    " max_features='sqrt', # Use sqrt(p) features per tree\n",
    " oob_score=True,\n",
    " random_state=42\n",
    ")\n",
    "\n",
    "rf_gene.fit(X_gene_train, y_gene_train)\n",
    "\n",
    "# Predictions\n",
    "y_gene_pred = rf_gene.predict(X_gene_test)\n",
    "y_gene_proba = rf_gene.predict_proba(X_gene_test)\n",
    "\n",
    "# Performance metrics\n",
    "gene_accuracy = accuracy_score(y_gene_test, y_gene_pred)\n",
    "gene_auc = roc_auc_score(y_gene_test, y_gene_proba[:, 1])\n",
    "\n",
    "print(f\"\\n High-dimensional Random Forest Performance:\")\n",
    "print(f\"• Test Accuracy: {gene_accuracy:.4f}\")\n",
    "print(f\"• ROC AUC: {gene_auc:.4f}\")\n",
    "print(f\"• OOB Score: {rf_gene.oob_score_:.4f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "gene_importance = rf_gene.feature_importances_\n",
    "gene_importance_df = pd.DataFrame({\n",
    " 'Gene': gene_features,\n",
    " 'Importance': gene_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Identify top important genes\n",
    "print(f\"\\n Top 15 Most Important Genes:\")\n",
    "for _, row in gene_importance_df.head(15).iterrows():\n",
    " gene_num = int(row['Gene'].split('_')[1])\n",
    " # Determine if this gene is from a true pathway (first 65 genes)\n",
    " is_signal = \"Signal\" if gene_num <= 65 else \"Noise\"\n",
    " print(f\"• {row['Gene']}: {row['Importance']:.4f} ({is_signal})\")\n",
    "\n",
    "# Visualize gene importance\n",
    "fig_gene_importance = go.Figure()\n",
    "\n",
    "# Color genes by signal vs noise\n",
    "colors = ['green' if int(gene.split('_')[1]) <= 65 else 'red'\n",
    " for gene in gene_importance_df.head(20)['Gene']]\n",
    "\n",
    "fig_gene_importance.add_trace(\n",
    " go.Bar(\n",
    " x=gene_importance_df.head(20)['Gene'],\n",
    " y=gene_importance_df.head(20)['Importance'],\n",
    " marker_color=colors,\n",
    " hovertemplate=\"Gene: %{x}<br>Importance: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_gene_importance.update_layout(\n",
    " title=\"Top 20 Gene Importance (Green=Signal, Red=Noise)\",\n",
    " xaxis_title=\"Genes\",\n",
    " yaxis_title=\"Feature Importance\",\n",
    " xaxis_tickangle=-45,\n",
    " height=600\n",
    ")\n",
    "fig_gene_importance.show()\n",
    "\n",
    "# Feature selection using Random Forest\n",
    "print(f\"\\n Feature Selection Analysis:\")\n",
    "\n",
    "# Use Random Forest for feature selection\n",
    "selector = SelectFromModel(rf_gene, threshold='mean')\n",
    "selector.fit(X_gene_train, y_gene_train)\n",
    "\n",
    "selected_features = selector.get_support()\n",
    "selected_gene_names = np.array(gene_features)[selected_features]\n",
    "n_selected = len(selected_gene_names)\n",
    "\n",
    "print(f\"• Selected {n_selected} out of {len(gene_features)} genes\")\n",
    "print(f\"• Selection threshold: {selector.threshold_:.4f}\")\n",
    "\n",
    "# Analyze quality of feature selection\n",
    "signal_genes_selected = sum(1 for gene in selected_gene_names if int(gene.split('_')[1]) <= 65)\n",
    "noise_genes_selected = n_selected - signal_genes_selected\n",
    "total_signal_genes = 65\n",
    "total_noise_genes = len(gene_features) - 65\n",
    "\n",
    "precision = signal_genes_selected / n_selected if n_selected > 0 else 0\n",
    "recall = signal_genes_selected / total_signal_genes\n",
    "\n",
    "print(f\"• Signal genes selected: {signal_genes_selected}/{total_signal_genes}\")\n",
    "print(f\"• Noise genes selected: {noise_genes_selected}/{total_noise_genes}\")\n",
    "print(f\"• Selection precision: {precision:.3f}\")\n",
    "print(f\"• Selection recall: {recall:.3f}\")\n",
    "\n",
    "# Train model on selected features only\n",
    "X_gene_train_selected = selector.transform(X_gene_train)\n",
    "X_gene_test_selected = selector.transform(X_gene_test)\n",
    "\n",
    "rf_gene_selected = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_gene_selected.fit(X_gene_train_selected, y_gene_train)\n",
    "\n",
    "selected_accuracy = rf_gene_selected.score(X_gene_test_selected, y_gene_test)\n",
    "\n",
    "print(f\"\\n Performance Comparison:\")\n",
    "print(f\"• All genes ({len(gene_features)}): {gene_accuracy:.4f} accuracy\")\n",
    "print(f\"• Selected genes ({n_selected}): {selected_accuracy:.4f} accuracy\")\n",
    "print(f\"• Feature reduction: {(1 - n_selected/len(gene_features))*100:.1f}%\")\n",
    "\n",
    "# ROC Curve comparison\n",
    "fpr_all, tpr_all, _ = roc_curve(y_gene_test, y_gene_proba[:, 1])\n",
    "y_gene_proba_selected = rf_gene_selected.predict_proba(X_gene_test_selected)\n",
    "fpr_selected, tpr_selected, _ = roc_curve(y_gene_test, y_gene_proba_selected[:, 1])\n",
    "\n",
    "fig_roc_comparison = go.Figure()\n",
    "\n",
    "fig_roc_comparison.add_trace(\n",
    " go.Scatter(\n",
    " x=fpr_all,\n",
    " y=tpr_all,\n",
    " mode='lines',\n",
    " name=f'All Genes (AUC = {gene_auc:.3f})',\n",
    " line=dict(color='blue', width=2)\n",
    " )\n",
    ")\n",
    "\n",
    "auc_selected = roc_auc_score(y_gene_test, y_gene_proba_selected[:, 1])\n",
    "fig_roc_comparison.add_trace(\n",
    " go.Scatter(\n",
    " x=fpr_selected,\n",
    " y=tpr_selected,\n",
    " mode='lines',\n",
    " name=f'Selected Genes (AUC = {auc_selected:.3f})',\n",
    " line=dict(color='green', width=2)\n",
    " )\n",
    ")\n",
    "\n",
    "fig_roc_comparison.add_trace(\n",
    " go.Scatter(\n",
    " x=[0, 1],\n",
    " y=[0, 1],\n",
    " mode='lines',\n",
    " name='Random Classifier',\n",
    " line=dict(color='red', dash='dash')\n",
    " )\n",
    ")\n",
    "\n",
    "fig_roc_comparison.update_layout(\n",
    " title=\"ROC Curves: All Genes vs Selected Genes\",\n",
    " xaxis_title=\"False Positive Rate\",\n",
    " yaxis_title=\"True Positive Rate\",\n",
    " height=500\n",
    ")\n",
    "fig_roc_comparison.show()\n",
    "\n",
    "# Analyze max_features parameter effect\n",
    "print(f\"\\n Effect of max_features Parameter:\")\n",
    "\n",
    "max_features_options = ['sqrt', 'log2', None, 0.1, 0.3, 0.5]\n",
    "max_features_scores = []\n",
    "\n",
    "for max_feat in max_features_options:\n",
    " rf_temp = RandomForestClassifier(\n",
    " n_estimators=100,\n",
    " max_features=max_feat,\n",
    " random_state=42\n",
    " )\n",
    " cv_score = cross_val_score(rf_temp, X_gene_train, y_gene_train, cv=3).mean()\n",
    " max_features_scores.append(cv_score)\n",
    "\n",
    " feat_name = str(max_feat) if max_feat is not None else 'all'\n",
    " print(f\"• max_features={feat_name}: {cv_score:.4f}\")\n",
    "\n",
    "# Find optimal max_features\n",
    "best_idx = np.argmax(max_features_scores)\n",
    "best_max_features = max_features_options[best_idx]\n",
    "print(f\"\\n Optimal max_features: {best_max_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ENSEMBLE BEHAVIOR ANALYSIS\n",
    "print(\" 4. ENSEMBLE BEHAVIOR ANALYSIS\")\n",
    "print(\"=\" * 31)\n",
    "\n",
    "# Analyze individual tree predictions vs ensemble\n",
    "print(\"Understanding how Random Forest combines individual tree predictions:\")\n",
    "\n",
    "# Use the employee performance dataset for this analysis\n",
    "rf_ensemble = RandomForestClassifier(n_estimators=10, random_state=42) # Small ensemble for visualization\n",
    "rf_ensemble.fit(X_class_train, y_class_train)\n",
    "\n",
    "# Get predictions from individual trees\n",
    "individual_predictions = []\n",
    "for tree in rf_ensemble.estimators_:\n",
    " tree_pred = tree.predict(X_class_test)\n",
    " individual_predictions.append(tree_pred)\n",
    "\n",
    "individual_predictions = np.array(individual_predictions)\n",
    "\n",
    "# Ensemble prediction\n",
    "ensemble_pred = rf_ensemble.predict(X_class_test)\n",
    "\n",
    "# Analyze agreement between trees\n",
    "print(f\"\\n Tree Agreement Analysis:\")\n",
    "\n",
    "# Calculate agreement for each sample\n",
    "agreements = []\n",
    "for i in range(len(X_class_test)):\n",
    " sample_predictions = individual_predictions[:, i]\n",
    " # Count how many trees agree with the final ensemble prediction\n",
    " agreement = np.sum(sample_predictions == ensemble_pred[i]) / len(rf_ensemble.estimators_)\n",
    " agreements.append(agreement)\n",
    "\n",
    "agreements = np.array(agreements)\n",
    "\n",
    "print(f\"• Average tree agreement: {agreements.mean():.3f}\")\n",
    "print(f\"• Minimum agreement: {agreements.min():.3f}\")\n",
    "print(f\"• Maximum agreement: {agreements.max():.3f}\")\n",
    "\n",
    "# Analyze disagreement cases\n",
    "low_agreement_mask = agreements < 0.6\n",
    "high_agreement_mask = agreements > 0.9\n",
    "\n",
    "print(f\"• Samples with low agreement (<60%): {low_agreement_mask.sum()}\")\n",
    "print(f\"• Samples with high agreement (>90%): {high_agreement_mask.sum()}\")\n",
    "\n",
    "# Visualize agreement distribution\n",
    "fig_agreement = go.Figure()\n",
    "\n",
    "fig_agreement.add_trace(\n",
    " go.Histogram(\n",
    " x=agreements,\n",
    " nbinsx=20,\n",
    " name='Tree Agreement',\n",
    " marker_color='lightblue',\n",
    " opacity=0.7\n",
    " )\n",
    ")\n",
    "\n",
    "fig_agreement.update_layout(\n",
    " title=\"Distribution of Tree Agreement in Random Forest\",\n",
    " xaxis_title=\"Fraction of Trees Agreeing with Ensemble\",\n",
    " yaxis_title=\"Number of Samples\",\n",
    " height=500\n",
    ")\n",
    "fig_agreement.show()\n",
    "\n",
    "# Prediction confidence analysis\n",
    "print(f\"\\n Prediction Confidence Analysis:\")\n",
    "\n",
    "# Get class probabilities\n",
    "class_probabilities = rf_ensemble.predict_proba(X_class_test)\n",
    "max_probabilities = class_probabilities.max(axis=1)\n",
    "\n",
    "# Correlate confidence with agreement\n",
    "correlation = np.corrcoef(agreements, max_probabilities)[0, 1]\n",
    "print(f\"• Correlation between tree agreement and prediction confidence: {correlation:.3f}\")\n",
    "\n",
    "# Visualize confidence vs agreement\n",
    "fig_conf_agreement = go.Figure()\n",
    "\n",
    "# Color points by correctness\n",
    "correct_predictions = (ensemble_pred == y_class_test)\n",
    "colors = ['green' if correct else 'red' for correct in correct_predictions]\n",
    "\n",
    "fig_conf_agreement.add_trace(\n",
    " go.Scatter(\n",
    " x=agreements,\n",
    " y=max_probabilities,\n",
    " mode='markers',\n",
    " marker=dict(color=colors, opacity=0.6),\n",
    " hovertemplate=\"Agreement: %{x:.3f}<br>Confidence: %{y:.3f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_conf_agreement.update_layout(\n",
    " title=\"Prediction Confidence vs Tree Agreement (Green=Correct, Red=Incorrect)\",\n",
    " xaxis_title=\"Tree Agreement\",\n",
    " yaxis_title=\"Prediction Confidence\",\n",
    " height=500\n",
    ")\n",
    "fig_conf_agreement.show()\n",
    "\n",
    "# Out-of-Bag (OOB) analysis\n",
    "print(f\"\\n Out-of-Bag (OOB) Analysis:\")\n",
    "\n",
    "# Train with different OOB sample sizes\n",
    "n_estimators_range = [10, 25, 50, 100, 200, 500]\n",
    "oob_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    " rf_oob_temp = RandomForestClassifier(\n",
    " n_estimators=n_est,\n",
    " oob_score=True,\n",
    " random_state=42\n",
    " )\n",
    " rf_oob_temp.fit(X_class_train, y_class_train)\n",
    "\n",
    " oob_scores.append(rf_oob_temp.oob_score_)\n",
    " test_scores.append(rf_oob_temp.score(X_class_test, y_class_test))\n",
    "\n",
    "print(\"OOB Score vs Test Score by Number of Estimators:\")\n",
    "for i, n_est in enumerate(n_estimators_range):\n",
    " print(f\"• n_estimators={n_est}: OOB={oob_scores[i]:.4f}, Test={test_scores[i]:.4f}\")\n",
    "\n",
    "# Visualize OOB vs Test performance\n",
    "fig_oob_test = go.Figure()\n",
    "\n",
    "fig_oob_test.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=oob_scores,\n",
    " mode='lines+markers',\n",
    " name='OOB Score',\n",
    " line=dict(color='blue')\n",
    " )\n",
    ")\n",
    "\n",
    "fig_oob_test.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=test_scores,\n",
    " mode='lines+markers',\n",
    " name='Test Score',\n",
    " line=dict(color='red')\n",
    " )\n",
    ")\n",
    "\n",
    "fig_oob_test.update_layout(\n",
    " title=\"OOB Score vs Test Score\",\n",
    " xaxis_title=\"Number of Estimators\",\n",
    " yaxis_title=\"Accuracy\",\n",
    " height=500\n",
    ")\n",
    "fig_oob_test.show()\n",
    "\n",
    "# Bootstrap sampling analysis\n",
    "print(f\"\\n Bootstrap Sampling Analysis:\")\n",
    "\n",
    "# Analyze what fraction of training data each tree sees\n",
    "n_train_samples = len(X_class_train)\n",
    "bootstrap_fractions = []\n",
    "\n",
    "# Simulate bootstrap sampling\n",
    "for _ in range(100): # 100 simulations\n",
    " bootstrap_sample = np.random.choice(n_train_samples, size=n_train_samples, replace=True)\n",
    " unique_samples = len(np.unique(bootstrap_sample))\n",
    " fraction_seen = unique_samples / n_train_samples\n",
    " bootstrap_fractions.append(fraction_seen)\n",
    "\n",
    "avg_fraction = np.mean(bootstrap_fractions)\n",
    "theoretical_fraction = 1 - (1 - 1/n_train_samples)**n_train_samples\n",
    "print(f\"• Average fraction of training data seen per tree: {avg_fraction:.3f}\")\n",
    "print(f\"• Theoretical expectation (1 - 1/e): {theoretical_fraction:.3f}\")\n",
    "print(f\"• This means ~{(1-avg_fraction)*100:.1f}% of data is out-of-bag for each tree\")\n",
    "\n",
    "# Feature subsampling analysis\n",
    "print(f\"\\n Feature Subsampling Analysis:\")\n",
    "\n",
    "n_features = len(class_features)\n",
    "print(f\"• Total features: {n_features}\")\n",
    "\n",
    "# Different max_features options\n",
    "max_features_options = {\n",
    " 'sqrt': int(np.sqrt(n_features)),\n",
    " 'log2': int(np.log2(n_features)),\n",
    " 'None': n_features,\n",
    " '0.3': int(0.3 * n_features)\n",
    "}\n",
    "\n",
    "for option, n_feat in max_features_options.items():\n",
    " fraction = n_feat / n_features\n",
    " print(f\"• max_features='{option}': {n_feat} features ({fraction:.1%} of total)\")\n",
    "\n",
    "print(f\"\\nFeature subsampling increases diversity and reduces overfitting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a0a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\n",
    "print(\" 5. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\")\n",
    "print(\"=\" * 54)\n",
    "\n",
    "# Comprehensive business analysis\n",
    "print(\" Random Forest Business Applications Analysis:\")\n",
    "\n",
    "print(f\"\\n1. MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\" • Employee Performance Classification: {class_accuracy:.1%} accuracy\")\n",
    "print(f\" • House Price Prediction: R² = {reg_r2:.3f} (${np.sqrt(reg_mse):,.0f} RMSE)\")\n",
    "print(f\" • Gene Expression Analysis: {gene_accuracy:.1%} accuracy, {gene_auc:.3f} AUC\")\n",
    "\n",
    "# Employee performance insights\n",
    "print(f\"\\n2. EMPLOYEE PERFORMANCE INSIGHTS:\")\n",
    "\n",
    "top_performance_factors = importance_df.head(5)\n",
    "print(f\" Top 5 Performance Drivers:\")\n",
    "for i, (_, row) in enumerate(top_performance_factors.iterrows(), 1):\n",
    " feature = row['Feature']\n",
    " importance = row['Importance']\n",
    " print(f\" {i}. {feature}: {importance:.3f} importance\")\n",
    "\n",
    "# Actionable recommendations based on feature importance\n",
    "print(f\"\\n HR Strategy Recommendations:\")\n",
    "top_factor = top_performance_factors.iloc[0]['Feature']\n",
    "if 'training' in top_factor.lower():\n",
    " print(f\" • Invest heavily in employee training programs\")\n",
    " print(f\" • Target 40+ training hours annually for high performers\")\n",
    "elif 'collaboration' in top_factor.lower():\n",
    " print(f\" • Implement team collaboration tools and practices\")\n",
    " print(f\" • Measure and reward collaborative behaviors\")\n",
    "elif 'innovation' in top_factor.lower():\n",
    " print(f\" • Create innovation time and recognition programs\")\n",
    " print(f\" • Encourage creative problem-solving initiatives\")\n",
    "\n",
    "# Calculate ROI of performance improvements\n",
    "total_employees = 10000\n",
    "current_high_performers = total_employees * 0.33 # Current top 33%\n",
    "target_improvement = 0.15 # 15% improvement in classification\n",
    "\n",
    "# Assuming high performers generate 30% more value\n",
    "avg_employee_value = 100000 # Annual value\n",
    "high_performer_premium = 0.30\n",
    "additional_value_per_improvement = avg_employee_value * high_performer_premium\n",
    "\n",
    "potential_new_high_performers = total_employees * target_improvement\n",
    "additional_annual_value = potential_new_high_performers * additional_value_per_improvement\n",
    "\n",
    "print(f\"\\n ROI Analysis:\")\n",
    "print(f\" • Current high performers: {current_high_performers:,.0f}\")\n",
    "print(f\" • With 15% classification improvement: +{potential_new_high_performers:,.0f} identified\")\n",
    "print(f\" • Additional annual value: ${additional_annual_value:,.0f}\")\n",
    "print(f\" • Implementation cost estimate: ${total_employees * 50:,.0f} (training/tools)\")\n",
    "print(f\" • Net ROI: {(additional_annual_value - total_employees * 50) / (total_employees * 50) * 100:.0f}%\")\n",
    "\n",
    "# Real estate insights\n",
    "print(f\"\\n3. REAL ESTATE PRICING INSIGHTS:\")\n",
    "\n",
    "top_price_factors = reg_importance_df.head(5)\n",
    "print(f\" Top 5 Price Drivers:\")\n",
    "for i, (_, row) in enumerate(top_price_factors.iterrows(), 1):\n",
    " feature = row['Feature']\n",
    " importance = row['Importance']\n",
    " print(f\" {i}. {feature}: {importance:.3f} importance\")\n",
    "\n",
    "# Property investment recommendations\n",
    "print(f\"\\n Investment Strategy Recommendations:\")\n",
    "most_important_factor = top_price_factors.iloc[0]['Feature']\n",
    "if 'size' in most_important_factor.lower():\n",
    " print(f\" • Focus on property size as primary value driver\")\n",
    " print(f\" • Target properties >2500 sq ft for premium market\")\n",
    "elif 'school' in most_important_factor.lower():\n",
    " print(f\" • Prioritize properties in high-rated school districts\")\n",
    " print(f\" • School rating >8 provides significant premium\")\n",
    "elif 'location' in most_important_factor.lower():\n",
    " print(f\" • Location proximity is crucial for valuation\")\n",
    " print(f\" • Properties within 10 miles of downtown preferred\")\n",
    "\n",
    "# Market analysis automation value\n",
    "properties_valued_monthly = 1000\n",
    "manual_appraisal_cost = 500\n",
    "automated_appraisal_cost = 50\n",
    "accuracy_threshold = 0.90 # Require 90% accuracy for automation\n",
    "\n",
    "if reg_r2 >= accuracy_threshold:\n",
    " monthly_savings = properties_valued_monthly * (manual_appraisal_cost - automated_appraisal_cost)\n",
    " print(f\"\\n Appraisal Automation ROI:\")\n",
    " print(f\" • Monthly property valuations: {properties_valued_monthly:,}\")\n",
    " print(f\" • Cost per manual appraisal: ${manual_appraisal_cost}\")\n",
    " print(f\" • Cost per automated appraisal: ${automated_appraisal_cost}\")\n",
    " print(f\" • Monthly savings: ${monthly_savings:,}\")\n",
    " print(f\" • Annual savings: ${monthly_savings * 12:,}\")\n",
    "else:\n",
    " print(f\"\\n Model needs improvement (R² = {reg_r2:.3f}) before automation deployment\")\n",
    "\n",
    "# Biomedical research insights\n",
    "print(f\"\\n4. BIOMEDICAL RESEARCH INSIGHTS:\")\n",
    "\n",
    "print(f\" Gene Discovery Results:\")\n",
    "signal_genes_in_top_20 = sum(1 for gene in gene_importance_df.head(20)['Gene']\n",
    " if int(gene.split('_')[1]) <= 65)\n",
    "print(f\" • Signal genes in top 20: {signal_genes_in_top_20}/20\")\n",
    "print(f\" • Feature selection precision: {precision:.1%}\")\n",
    "print(f\" • Feature selection recall: {recall:.1%}\")\n",
    "\n",
    "# Research cost savings\n",
    "total_genes_to_validate = 100\n",
    "cost_per_gene_validation = 5000\n",
    "selected_genes_to_validate = n_selected\n",
    "\n",
    "validation_cost_all = total_genes_to_validate * cost_per_gene_validation\n",
    "validation_cost_selected = selected_genes_to_validate * cost_per_gene_validation\n",
    "cost_savings = validation_cost_all - validation_cost_selected\n",
    "\n",
    "print(f\"\\n Research Cost Analysis:\")\n",
    "print(f\" • Cost to validate all genes: ${validation_cost_all:,}\")\n",
    "print(f\" • Cost to validate selected genes: ${validation_cost_selected:,}\")\n",
    "print(f\" • Research cost savings: ${cost_savings:,}\")\n",
    "print(f\" • Efficiency gain: {(1 - selected_genes_to_validate/total_genes_to_validate)*100:.0f}% reduction\")\n",
    "\n",
    "# Implementation strategy\n",
    "print(f\"\\n5. IMPLEMENTATION STRATEGY:\")\n",
    "\n",
    "print(f\"\\n Phase 1 - Employee Performance (Months 1-3):\")\n",
    "print(f\" • Deploy performance prediction model for 25% of workforce\")\n",
    "print(f\" • Focus on high-confidence predictions (agreement >80%)\")\n",
    "print(f\" • A/B test interventions based on model recommendations\")\n",
    "print(f\" • Expected outcome: 10-15% improvement in performance identification\")\n",
    "\n",
    "print(f\"\\n Phase 2 - Real Estate Automation (Months 2-4):\")\n",
    "print(f\" • Implement automated property valuation for pre-screening\")\n",
    "print(f\" • Human review for properties with low model confidence\")\n",
    "print(f\" • Integration with existing appraisal workflows\")\n",
    "print(f\" • Expected outcome: 50% reduction in appraisal time\")\n",
    "\n",
    "print(f\"\\n Phase 3 - Research Optimization (Months 3-6):\")\n",
    "print(f\" • Deploy feature selection pipeline for new studies\")\n",
    "print(f\" • Validate top gene candidates with wet lab experiments\")\n",
    "print(f\" • Iterative model improvement with validation results\")\n",
    "print(f\" • Expected outcome: 60% reduction in experimental costs\")\n",
    "\n",
    "print(f\"\\n6. RANDOM FOREST ADVANTAGES:\")\n",
    "print(f\" • Handles mixed data types (numerical, categorical)\")\n",
    "print(f\" • Robust to outliers and missing values\")\n",
    "print(f\" • Built-in feature importance ranking\")\n",
    "print(f\" • No need for feature scaling or preprocessing\")\n",
    "print(f\" • Excellent performance on high-dimensional data\")\n",
    "print(f\" • Built-in validation through OOB scoring\")\n",
    "print(f\" • Interpretable through feature importance\")\n",
    "\n",
    "print(f\"\\n7. LIMITATIONS AND MITIGATION:\")\n",
    "print(f\" • Can overfit with very noisy data\")\n",
    "print(f\" → Use OOB scoring and cross-validation for monitoring\")\n",
    "print(f\" • Memory intensive with large datasets\")\n",
    "print(f\" → Consider online/incremental learning approaches\")\n",
    "print(f\" • Less interpretable than single decision trees\")\n",
    "print(f\" → Supplement with SHAP values for instance-level explanations\")\n",
    "print(f\" • May struggle with linear relationships\")\n",
    "print(f\" → Ensemble with linear models when appropriate\")\n",
    "\n",
    "print(f\"\\n8. MONITORING AND MAINTENANCE:\")\n",
    "print(f\" • Monitor OOB score for model drift detection\")\n",
    "print(f\" • Track feature importance stability over time\")\n",
    "print(f\" • Retrain quarterly or when performance degrades >5%\")\n",
    "print(f\" • A/B test model updates against current production\")\n",
    "print(f\" • Regular feature engineering and selection reviews\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\" RANDOM FOREST LEARNING SUMMARY:\")\n",
    "print(f\" Mastered ensemble learning and bagging principles\")\n",
    "print(f\" Applied Random Forest to classification and regression\")\n",
    "print(f\" Analyzed feature importance and selection techniques\")\n",
    "print(f\" Understood OOB validation and ensemble behavior\")\n",
    "print(f\" Optimized hyperparameters for different problem types\")\n",
    "print(f\" Generated comprehensive business strategies and ROI analysis\")\n",
    "print(f\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}