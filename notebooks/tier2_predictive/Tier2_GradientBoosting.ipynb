{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 2: Gradient Boosting\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** 3fb3e2a2-0d73-49dc-a70e-2b038b04932e\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 2: Gradient Boosting,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** 3fb3e2a2-0d73-49dc-a70e-2b038b04932e\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1705f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries for Gradient Boosting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Scikit-learn boosting algorithms\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "# XGBoost (if available)\n",
    "try:\n",
    " import xgboost as xgb\n",
    " XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    " XGBOOST_AVAILABLE = False\n",
    " print(\"XGBoost not available - using scikit-learn implementations only\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 2: Gradient Boosting - Libraries Loaded!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Available Boosting Techniques:\")\n",
    "print(\"• AdaBoost - Adaptive weight adjustment\")\n",
    "print(\"• Gradient Boosting - Sequential error correction\")\n",
    "print(\"• Learning curve analysis - Bias-variance optimization\")\n",
    "print(\"• Hyperparameter tuning - Learning rate and regularization\")\n",
    "if XGBOOST_AVAILABLE:\n",
    " print(\"• XGBoost - Extreme gradient boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84827ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Compact Datasets for Boosting Analysis\n",
    "np.random.seed(42)\n",
    "\n",
    "def create_boosting_datasets():\n",
    " \"\"\"Create focused datasets for boosting demonstration\"\"\"\n",
    "\n",
    " # 1. CLASSIFICATION: Credit Default Prediction\n",
    " n_customers = 800\n",
    "\n",
    " # Financial features\n",
    " credit_score = np.random.normal(650, 100, n_customers)\n",
    " credit_score = np.clip(credit_score, 300, 850)\n",
    "\n",
    " income = np.random.lognormal(mean=10.5, sigma=0.6, size=n_customers)\n",
    " debt_to_income = np.random.beta(2, 5, n_customers) * 0.8\n",
    "\n",
    " # Account history\n",
    " account_age_months = np.random.exponential(24, n_customers) + 6\n",
    " account_age_months = np.clip(account_age_months, 6, 120)\n",
    "\n",
    " payment_history = np.random.beta(8, 2, n_customers) # Generally good\n",
    " num_accounts = np.random.poisson(4, n_customers) + 1\n",
    "\n",
    " # Generate realistic default probability\n",
    " default_logit = (\n",
    " -0.01 * (credit_score - 650) +\n",
    " -0.5 * np.log(income / 50000) +\n",
    " 3.0 * debt_to_income +\n",
    " -0.01 * account_age_months +\n",
    " -2.0 * payment_history +\n",
    " 0.1 * num_accounts +\n",
    " np.random.normal(0, 0.5, n_customers)\n",
    " )\n",
    "\n",
    " default_prob = 1 / (1 + np.exp(-default_logit))\n",
    " default = np.random.binomial(1, default_prob)\n",
    "\n",
    " credit_df = pd.DataFrame({\n",
    " 'credit_score': credit_score,\n",
    " 'income': income,\n",
    " 'debt_to_income': debt_to_income,\n",
    " 'account_age_months': account_age_months,\n",
    " 'payment_history': payment_history,\n",
    " 'num_accounts': num_accounts,\n",
    " 'default': default\n",
    " })\n",
    "\n",
    " # 2. REGRESSION: Sales Forecasting\n",
    " n_periods = 500\n",
    "\n",
    " # Time-based features\n",
    " trend = np.linspace(100, 200, n_periods)\n",
    " seasonality = 20 * np.sin(2 * np.pi * np.arange(n_periods) / 12)\n",
    "\n",
    " # Business features\n",
    " marketing_spend = np.random.gamma(2, 10, n_periods)\n",
    " competitor_price = np.random.normal(50, 5, n_periods)\n",
    " economic_index = np.random.normal(100, 10, n_periods)\n",
    "\n",
    " # Generate sales with complex relationships\n",
    " sales = (\n",
    " trend +\n",
    " seasonality +\n",
    " 0.5 * marketing_spend +\n",
    " -0.8 * (competitor_price - 50) +\n",
    " 0.3 * (economic_index - 100) +\n",
    " np.random.normal(0, 15, n_periods)\n",
    " )\n",
    "\n",
    " sales_df = pd.DataFrame({\n",
    " 'period': np.arange(n_periods),\n",
    " 'marketing_spend': marketing_spend,\n",
    " 'competitor_price': competitor_price,\n",
    " 'economic_index': economic_index,\n",
    " 'sales': sales\n",
    " })\n",
    "\n",
    " return credit_df, sales_df\n",
    "\n",
    "credit_df, sales_df = create_boosting_datasets()\n",
    "\n",
    "print(\" Boosting Datasets Created:\")\n",
    "print(f\"Credit Default: {credit_df.shape} - {credit_df['default'].mean():.1%} default rate\")\n",
    "print(f\"Sales Forecasting: {sales_df.shape}\")\n",
    "print(f\"Sales range: {sales_df['sales'].min():.1f} - {sales_df['sales'].max():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ADABOOST CLASSIFICATION ANALYSIS\n",
    "print(\" 1. ADABOOST CLASSIFICATION\")\n",
    "print(\"=\" * 28)\n",
    "\n",
    "# Prepare data\n",
    "credit_features = ['credit_score', 'income', 'debt_to_income', 'account_age_months', 'payment_history', 'num_accounts']\n",
    "X_credit = credit_df[credit_features]\n",
    "y_credit = credit_df['default']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_credit, y_credit, test_size=0.2, random_state=42, stratify=y_credit)\n",
    "\n",
    "# Train AdaBoost with different n_estimators\n",
    "n_estimators_range = [10, 25, 50, 100, 200]\n",
    "ada_results = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    " ada = AdaBoostClassifier(n_estimators=n_est, random_state=42)\n",
    " ada.fit(X_train, y_train)\n",
    "\n",
    " train_acc = ada.score(X_train, y_train)\n",
    " test_acc = ada.score(X_test, y_test)\n",
    "\n",
    " ada_results.append({\n",
    " 'n_estimators': n_est,\n",
    " 'train_accuracy': train_acc,\n",
    " 'test_accuracy': test_acc\n",
    " })\n",
    "\n",
    "ada_results_df = pd.DataFrame(ada_results)\n",
    "\n",
    "print(\"AdaBoost Performance by Number of Estimators:\")\n",
    "for _, row in ada_results_df.iterrows():\n",
    " print(f\"n_estimators={row['n_estimators']:3d}: Train={row['train_accuracy']:.3f}, Test={row['test_accuracy']:.3f}\")\n",
    "\n",
    "# Best AdaBoost model\n",
    "best_ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "best_ada.fit(X_train, y_train)\n",
    "ada_accuracy = best_ada.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nBest AdaBoost Test Accuracy: {ada_accuracy:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "ada_importance = best_ada.feature_importances_\n",
    "for i, importance in enumerate(ada_importance):\n",
    " print(f\"• {credit_features[i]}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. GRADIENT BOOSTING CLASSIFICATION\n",
    "print(\" 2. GRADIENT BOOSTING CLASSIFICATION\")\n",
    "print(\"=\" * 34)\n",
    "\n",
    "# Train Gradient Boosting\n",
    "gb_clf = GradientBoostingClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=0.1,\n",
    " max_depth=3,\n",
    " random_state=42\n",
    ")\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "gb_accuracy = gb_clf.score(X_test, y_test)\n",
    "print(f\"Gradient Boosting Test Accuracy: {gb_accuracy:.3f}\")\n",
    "\n",
    "# Learning rate analysis\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "lr_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    " gb_temp = GradientBoostingClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=lr,\n",
    " max_depth=3,\n",
    " random_state=42\n",
    " )\n",
    " gb_temp.fit(X_train, y_train)\n",
    " test_acc = gb_temp.score(X_test, y_test)\n",
    " lr_results.append({'learning_rate': lr, 'accuracy': test_acc})\n",
    "\n",
    "print(\"\\nLearning Rate Effect:\")\n",
    "for result in lr_results:\n",
    " print(f\"lr={result['learning_rate']:.2f}: {result['accuracy']:.3f}\")\n",
    "\n",
    "# Compare all boosting methods\n",
    "methods_comparison = {\n",
    " 'AdaBoost': ada_accuracy,\n",
    " 'Gradient Boosting': gb_accuracy\n",
    "}\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    " xgb_clf = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    " xgb_clf.fit(X_train, y_train)\n",
    " xgb_accuracy = xgb_clf.score(X_test, y_test)\n",
    " methods_comparison['XGBoost'] = xgb_accuracy\n",
    "\n",
    "print(f\"\\n Method Comparison:\")\n",
    "for method, acc in methods_comparison.items():\n",
    " print(f\"• {method}: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. GRADIENT BOOSTING REGRESSION\n",
    "print(\" 3. GRADIENT BOOSTING REGRESSION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Prepare sales data\n",
    "sales_features = ['period', 'marketing_spend', 'competitor_price', 'economic_index']\n",
    "X_sales = sales_df[sales_features]\n",
    "y_sales = sales_df['sales']\n",
    "\n",
    "X_sales_train, X_sales_test, y_sales_train, y_sales_test = train_test_split(\n",
    " X_sales, y_sales, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Gradient Boosting Regressor\n",
    "gb_reg = GradientBoostingRegressor(\n",
    " n_estimators=100,\n",
    " learning_rate=0.1,\n",
    " max_depth=3,\n",
    " random_state=42\n",
    ")\n",
    "gb_reg.fit(X_sales_train, y_sales_train)\n",
    "\n",
    "# Predictions and metrics\n",
    "y_sales_pred = gb_reg.predict(X_sales_test)\n",
    "sales_r2 = r2_score(y_sales_test, y_sales_pred)\n",
    "sales_rmse = np.sqrt(mean_squared_error(y_sales_test, y_sales_pred))\n",
    "\n",
    "print(f\"Sales Forecasting Performance:\")\n",
    "print(f\"• R²: {sales_r2:.3f}\")\n",
    "print(f\"• RMSE: {sales_rmse:.1f}\")\n",
    "\n",
    "# Feature importance for regression\n",
    "sales_importance = gb_reg.feature_importances_\n",
    "print(f\"\\nSales Feature Importance:\")\n",
    "for i, importance in enumerate(sales_importance):\n",
    " print(f\"• {sales_features[i]}: {importance:.3f}\")\n",
    "\n",
    "# Training progress analysis\n",
    "train_scores = gb_reg.train_score_\n",
    "test_scores = np.zeros_like(train_scores)\n",
    "\n",
    "for i, pred in enumerate(gb_reg.staged_predict(X_sales_test)):\n",
    " test_scores[i] = r2_score(y_sales_test, pred)\n",
    "\n",
    "print(f\"\\nTraining Progress:\")\n",
    "print(f\"• Initial test R²: {test_scores[0]:.3f}\")\n",
    "print(f\"• Final test R²: {test_scores[-1]:.3f}\")\n",
    "print(f\"• Best test R²: {test_scores.max():.3f} at iteration {test_scores.argmax()+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LEARNING CURVES AND BIAS-VARIANCE ANALYSIS\n",
    "print(\" 4. LEARNING CURVES ANALYSIS\")\n",
    "print(\"=\" * 28)\n",
    "\n",
    "# Learning curves for gradient boosting\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "train_sizes_abs, train_scores, test_scores = learning_curve(\n",
    " GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    " X_train, y_train,\n",
    " train_sizes=train_sizes,\n",
    " cv=3,\n",
    " random_state=42,\n",
    " scoring='accuracy'\n",
    ")\n",
    "\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "train_std = train_scores.std(axis=1)\n",
    "test_mean = test_scores.mean(axis=1)\n",
    "test_std = test_scores.std(axis=1)\n",
    "\n",
    "print(\"Learning Curve Analysis:\")\n",
    "print(f\"• Training set performance: {train_mean[-1]:.3f} ± {train_std[-1]:.3f}\")\n",
    "print(f\"• Validation performance: {test_mean[-1]:.3f} ± {test_std[-1]:.3f}\")\n",
    "print(f\"• Bias (underfitting): {'Low' if test_mean[-1] > 0.75 else 'High'}\")\n",
    "print(f\"• Variance (overfitting): {'High' if train_mean[-1] - test_mean[-1] > 0.1 else 'Low'}\")\n",
    "\n",
    "# Visualize learning curves\n",
    "fig_learning = go.Figure()\n",
    "\n",
    "fig_learning.add_trace(\n",
    " go.Scatter(\n",
    " x=train_sizes_abs,\n",
    " y=train_mean,\n",
    " mode='lines+markers',\n",
    " name='Training Score',\n",
    " line=dict(color='blue'),\n",
    " error_y=dict(type='data', array=train_std)\n",
    " )\n",
    ")\n",
    "\n",
    "fig_learning.add_trace(\n",
    " go.Scatter(\n",
    " x=train_sizes_abs,\n",
    " y=test_mean,\n",
    " mode='lines+markers',\n",
    " name='Validation Score',\n",
    " line=dict(color='red'),\n",
    " error_y=dict(type='data', array=test_std)\n",
    " )\n",
    ")\n",
    "\n",
    "fig_learning.update_layout(\n",
    " title=\"Gradient Boosting Learning Curves\",\n",
    " xaxis_title=\"Training Set Size\",\n",
    " yaxis_title=\"Accuracy\",\n",
    " height=400\n",
    ")\n",
    "fig_learning.show()\n",
    "\n",
    "# Overfitting analysis with validation curves\n",
    "n_estimators_detailed = range(10, 201, 20)\n",
    "train_scores_detailed = []\n",
    "test_scores_detailed = []\n",
    "\n",
    "for n_est in n_estimators_detailed:\n",
    " gb_temp = GradientBoostingClassifier(n_estimators=n_est, random_state=42)\n",
    " gb_temp.fit(X_train, y_train)\n",
    "\n",
    " train_scores_detailed.append(gb_temp.score(X_train, y_train))\n",
    " test_scores_detailed.append(gb_temp.score(X_test, y_test))\n",
    "\n",
    "# Find optimal number of estimators\n",
    "optimal_idx = np.argmax(test_scores_detailed)\n",
    "optimal_n_estimators = list(n_estimators_detailed)[optimal_idx]\n",
    "\n",
    "print(f\"\\nOverfitting Analysis:\")\n",
    "print(f\"• Optimal n_estimators: {optimal_n_estimators}\")\n",
    "print(f\"• Best validation score: {test_scores_detailed[optimal_idx]:.3f}\")\n",
    "\n",
    "# Visualize validation curves\n",
    "fig_validation = go.Figure()\n",
    "\n",
    "fig_validation.add_trace(\n",
    " go.Scatter(\n",
    " x=list(n_estimators_detailed),\n",
    " y=train_scores_detailed,\n",
    " mode='lines+markers',\n",
    " name='Training Score',\n",
    " line=dict(color='blue')\n",
    " )\n",
    ")\n",
    "\n",
    "fig_validation.add_trace(\n",
    " go.Scatter(\n",
    " x=list(n_estimators_detailed),\n",
    " y=test_scores_detailed,\n",
    " mode='lines+markers',\n",
    " name='Test Score',\n",
    " line=dict(color='red')\n",
    " )\n",
    ")\n",
    "\n",
    "fig_validation.add_vline(x=optimal_n_estimators, line_dash=\"dash\", line_color=\"green\")\n",
    "\n",
    "fig_validation.update_layout(\n",
    " title=\"Validation Curves: Effect of n_estimators\",\n",
    " xaxis_title=\"Number of Estimators\",\n",
    " yaxis_title=\"Accuracy\",\n",
    " height=400\n",
    ")\n",
    "fig_validation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. BUSINESS INSIGHTS AND RECOMMENDATIONS\n",
    "print(\" 5. BUSINESS INSIGHTS\")\n",
    "print(\"=\" * 21)\n",
    "\n",
    "print(\" GRADIENT BOOSTING BUSINESS APPLICATIONS:\")\n",
    "\n",
    "# Credit default analysis\n",
    "default_rate_reduction = 0.15 # 15% improvement in default prediction\n",
    "portfolio_value = 10_000_000 # $10M loan portfolio\n",
    "current_loss_rate = credit_df['default'].mean()\n",
    "improved_loss_rate = current_loss_rate * (1 - default_rate_reduction)\n",
    "\n",
    "current_losses = portfolio_value * current_loss_rate\n",
    "improved_losses = portfolio_value * improved_loss_rate\n",
    "annual_savings = current_losses - improved_losses\n",
    "\n",
    "print(f\"\\n Credit Risk Management ROI:\")\n",
    "print(f\"• Portfolio value: ${portfolio_value:,}\")\n",
    "print(f\"• Current default rate: {current_loss_rate:.1%}\")\n",
    "print(f\"• Improved default rate: {improved_loss_rate:.1%}\")\n",
    "print(f\"• Annual loss prevention: ${annual_savings:,.0f}\")\n",
    "\n",
    "# Sales forecasting value\n",
    "forecast_accuracy_improvement = 0.20 # 20% RMSE improvement\n",
    "current_forecast_error = sales_rmse\n",
    "improved_forecast_error = current_forecast_error * (1 - forecast_accuracy_improvement)\n",
    "\n",
    "inventory_cost_reduction = 0.10 # 10% inventory cost reduction\n",
    "annual_sales_volume = 1_000_000\n",
    "inventory_savings = annual_sales_volume * inventory_cost_reduction\n",
    "\n",
    "print(f\"\\n Sales Forecasting ROI:\")\n",
    "print(f\"• Current RMSE: {current_forecast_error:.1f}\")\n",
    "print(f\"• Improved RMSE: {improved_forecast_error:.1f}\")\n",
    "print(f\"• Annual sales volume: ${annual_sales_volume:,}\")\n",
    "print(f\"• Inventory cost savings: ${inventory_savings:,}\")\n",
    "\n",
    "print(f\"\\n GRADIENT BOOSTING ADVANTAGES:\")\n",
    "print(f\"• Sequential error correction reduces bias\")\n",
    "print(f\"• Handles complex non-linear relationships\")\n",
    "print(f\"• Built-in feature selection through importance\")\n",
    "print(f\"• Robust to outliers and missing data\")\n",
    "print(f\"• Excellent predictive performance\")\n",
    "\n",
    "print(f\"\\n KEY CONSIDERATIONS:\")\n",
    "print(f\"• Risk of overfitting with too many estimators\")\n",
    "print(f\"• Sensitive to hyperparameter tuning\")\n",
    "print(f\"• Computationally intensive for large datasets\")\n",
    "print(f\"• Sequential training (less parallelizable)\")\n",
    "\n",
    "print(f\"\\n HYPERPARAMETER GUIDELINES:\")\n",
    "print(f\"• learning_rate: 0.05-0.1 for stability\")\n",
    "print(f\"• n_estimators: 100-200 for most problems\")\n",
    "print(f\"• max_depth: 3-6 to prevent overfitting\")\n",
    "print(f\"• min_samples_split: 10-20 for regularization\")\n",
    "\n",
    "print(f\"\\n IMPLEMENTATION STRATEGY:\")\n",
    "print(f\"• Start with default parameters\")\n",
    "print(f\"• Use cross-validation for hyperparameter tuning\")\n",
    "print(f\"• Monitor training vs validation performance\")\n",
    "print(f\"• Consider early stopping for optimal complexity\")\n",
    "print(f\"• Ensemble with other algorithms for robustness\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\" GRADIENT BOOSTING LEARNING SUMMARY:\")\n",
    "print(f\" Mastered sequential learning and error correction\")\n",
    "print(f\" Compared AdaBoost vs Gradient Boosting approaches\")\n",
    "print(f\" Analyzed learning rates and overfitting patterns\")\n",
    "print(f\" Applied boosting to real-world business problems\")\n",
    "print(f\" Optimized bias-variance tradeoff through validation\")\n",
    "print(f\" Generated ROI-focused implementation strategies\")\n",
    "print(f\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}