{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 2: Support Vector Machines (SVM)\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** bd672181-b720-4a15-82ce-893083f211ae\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 2: Support Vector Machines (SVM),\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** bd672181-b720-4a15-82ce-893083f211ae\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Essential Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, mean_absolute_error\n",
    "from sklearn.datasets import make_classification, make_circles, make_moons\n",
    "\n",
    "# Additional utilities\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 2: Support Vector Machines (SVM) - Libraries Loaded Successfully!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Available SVM Techniques:\")\n",
    "print(\"• Linear SVM - Maximum margin linear classification\")\n",
    "print(\"• Kernel SVM - Non-linear classification with RBF, polynomial kernels\")\n",
    "print(\"• SVM Regression (SVR) - Support vector regression for continuous targets\")\n",
    "print(\"• Hyperparameter Optimization - C, gamma, kernel parameter tuning\")\n",
    "print(\"• Support Vector Analysis - Understanding model decision boundaries\")\n",
    "print(\"• Kernel Trick Visualization - Non-linear transformation insights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Comprehensive Datasets for SVM Analysis\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_svm_datasets():\n",
    " \"\"\"Generate datasets optimized for SVM analysis with various complexity levels\"\"\"\n",
    "\n",
    " # 1. LINEAR SEPARABLE DATASET - Credit Risk Assessment\n",
    " n_samples = 1000\n",
    "\n",
    " # Generate linearly separable credit data\n",
    " credit_score = np.random.normal(650, 120, n_samples)\n",
    " credit_score = np.clip(credit_score, 300, 850)\n",
    "\n",
    " debt_to_income = np.random.beta(2, 5, n_samples) * 80 # 0-80% DTI\n",
    " annual_income = np.random.lognormal(10.8, 0.6, n_samples)\n",
    " annual_income = np.clip(annual_income, 30000, 200000)\n",
    "\n",
    " employment_years = np.random.exponential(5, n_samples) + 0.5\n",
    " employment_years = np.clip(employment_years, 0.5, 40)\n",
    "\n",
    " # Create clear linear separation for credit approval\n",
    " # Good credit: high score, low DTI, good income\n",
    " linear_separator = (credit_score - 600) / 100 + (50 - debt_to_income) / 25 + np.log(annual_income) - 10.5\n",
    "\n",
    " # Add some noise but maintain linear separability\n",
    " noise = np.random.normal(0, 0.3, n_samples)\n",
    " credit_approved = (linear_separator + noise) > 0\n",
    "\n",
    " # Add realistic features\n",
    " previous_defaults = np.random.binomial(3, 0.1, n_samples)\n",
    " credit_utilization = np.random.beta(2, 3, n_samples) * 100\n",
    "\n",
    " # Make some correlation with approval\n",
    " previous_defaults[credit_approved] = np.random.binomial(3, 0.05, credit_approved.sum())\n",
    " previous_defaults[~credit_approved] = np.random.binomial(3, 0.2, (~credit_approved).sum())\n",
    "\n",
    " linear_df = pd.DataFrame({\n",
    " 'credit_score': credit_score,\n",
    " 'debt_to_income': debt_to_income,\n",
    " 'annual_income': annual_income,\n",
    " 'employment_years': employment_years,\n",
    " 'previous_defaults': previous_defaults,\n",
    " 'credit_utilization': credit_utilization,\n",
    " 'approved': credit_approved.astype(int)\n",
    " })\n",
    "\n",
    " # 2. NON-LINEAR DATASET - Customer Segmentation (Circular patterns)\n",
    " # Generate concentric circles for non-linear classification\n",
    " X_circles, y_circles = make_circles(n_samples=800, noise=0.1, factor=0.3, random_state=42)\n",
    "\n",
    " # Transform to business context - Customer value segments\n",
    " # Inner circle = high value, outer circle = standard value\n",
    " customer_spending = X_circles[:, 0] * 5000 + 7000 # Scale to spending range\n",
    " customer_frequency = X_circles[:, 1] * 20 + 25 # Scale to frequency range\n",
    "\n",
    " # Add business-relevant features\n",
    " customer_tenure = np.random.exponential(3, len(X_circles)) + 0.5\n",
    " support_tickets = np.random.poisson(2, len(X_circles))\n",
    "\n",
    " # High value customers (inner circle) have different patterns\n",
    " high_value_mask = y_circles == 0\n",
    " customer_tenure[high_value_mask] += np.random.exponential(2, high_value_mask.sum()) # Longer tenure\n",
    " support_tickets[high_value_mask] = np.random.poisson(1, high_value_mask.sum()) # Fewer tickets\n",
    "\n",
    " nonlinear_df = pd.DataFrame({\n",
    " 'customer_spending': customer_spending,\n",
    " 'customer_frequency': customer_frequency,\n",
    " 'customer_tenure': customer_tenure,\n",
    " 'support_tickets': support_tickets,\n",
    " 'value_segment': y_circles # 0=High Value, 1=Standard\n",
    " })\n",
    "\n",
    " # 3. REGRESSION DATASET - House Price Prediction with Complex Relationships\n",
    " n_reg_samples = 1000\n",
    "\n",
    " # Generate house features\n",
    " lot_size = np.random.gamma(2, 2000, n_reg_samples) + 1000 # sq ft\n",
    " house_age = np.random.exponential(15, n_reg_samples) + 1\n",
    " bedrooms = np.random.poisson(3, n_reg_samples) + 1\n",
    " bathrooms = np.random.poisson(2, n_reg_samples) + 1\n",
    "\n",
    " # School district rating (affects price non-linearly)\n",
    " school_rating = np.random.beta(2, 2, n_reg_samples) * 10 + 1\n",
    "\n",
    " # Crime rate (non-linear negative effect)\n",
    " crime_rate = np.random.exponential(5, n_reg_samples)\n",
    "\n",
    " # Distance to city center\n",
    " distance_to_center = np.random.gamma(2, 5, n_reg_samples) + 1\n",
    "\n",
    " # Generate prices with non-linear relationships\n",
    " base_price = (\n",
    " lot_size * 50 + # Linear lot effect\n",
    " bedrooms * 25000 + # Linear bedroom effect\n",
    " bathrooms * 20000 + # Linear bathroom effect\n",
    " np.exp(school_rating / 3) * 15000 + # Exponential school effect\n",
    " -crime_rate ** 1.5 * 3000 + # Non-linear crime penalty\n",
    " -np.log(distance_to_center + 1) * 20000 + # Log distance effect\n",
    " -house_age * 2000 # Linear age depreciation\n",
    " )\n",
    "\n",
    " # Add noise and ensure positive prices\n",
    " price_noise = np.random.normal(0, 30000, n_reg_samples)\n",
    " house_prices = np.maximum(base_price + price_noise, 100000)\n",
    "\n",
    " regression_df = pd.DataFrame({\n",
    " 'lot_size': lot_size,\n",
    " 'house_age': house_age,\n",
    " 'bedrooms': bedrooms,\n",
    " 'bathrooms': bathrooms,\n",
    " 'school_rating': school_rating,\n",
    " 'crime_rate': crime_rate,\n",
    " 'distance_to_center': distance_to_center,\n",
    " 'price': house_prices\n",
    " })\n",
    "\n",
    " return linear_df, nonlinear_df, regression_df\n",
    "\n",
    "# Generate datasets\n",
    "print(\" Generating SVM-optimized datasets...\")\n",
    "linear_df, nonlinear_df, regression_df = generate_svm_datasets()\n",
    "\n",
    "print(f\"Linear Dataset (Credit Approval): {linear_df.shape}\")\n",
    "print(f\"Non-linear Dataset (Customer Segments): {nonlinear_df.shape}\")\n",
    "print(f\"Regression Dataset (House Prices): {regression_df.shape}\")\n",
    "\n",
    "print(\"\\nLinear Classification Dataset (Credit Approval):\")\n",
    "print(linear_df.head())\n",
    "print(f\"Approval Rate: {linear_df['approved'].mean():.1%}\")\n",
    "\n",
    "print(\"\\nNon-linear Classification Dataset (Customer Segmentation):\")\n",
    "print(nonlinear_df.head())\n",
    "print(f\"High Value Customers: {(nonlinear_df['value_segment'] == 0).mean():.1%}\")\n",
    "\n",
    "print(\"\\nRegression Dataset (House Prices):\")\n",
    "print(regression_df.head())\n",
    "print(f\"Price Range: ${regression_df['price'].min():,.0f} - ${regression_df['price'].max():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ec442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LINEAR SVM ANALYSIS\n",
    "print(\" 1. LINEAR SVM ANALYSIS\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Prepare linear classification data\n",
    "linear_features = ['credit_score', 'debt_to_income', 'annual_income',\n",
    " 'employment_years', 'previous_defaults', 'credit_utilization']\n",
    "X_linear = linear_df[linear_features]\n",
    "y_linear = linear_df['approved']\n",
    "\n",
    "# Split data\n",
    "X_linear_train, X_linear_test, y_linear_train, y_linear_test = train_test_split(\n",
    " X_linear, y_linear, test_size=0.2, random_state=42, stratify=y_linear\n",
    ")\n",
    "\n",
    "# Scale features (important for SVM)\n",
    "scaler_linear = StandardScaler()\n",
    "X_linear_train_scaled = scaler_linear.fit_transform(X_linear_train)\n",
    "X_linear_test_scaled = scaler_linear.transform(X_linear_test)\n",
    "\n",
    "print(f\"Training set: {X_linear_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_linear_test_scaled.shape}\")\n",
    "print(f\"Class distribution: {y_linear_train.value_counts().to_dict()}\")\n",
    "\n",
    "# Train Linear SVM with different C values\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "linear_svm_results = {}\n",
    "\n",
    "print(f\"\\n C Parameter Optimization:\")\n",
    "\n",
    "for C in C_values:\n",
    " # Linear SVM\n",
    " svm_linear = SVC(kernel='linear', C=C, random_state=42)\n",
    " svm_linear.fit(X_linear_train_scaled, y_linear_train)\n",
    "\n",
    " # Predictions\n",
    " train_score = svm_linear.score(X_linear_train_scaled, y_linear_train)\n",
    " test_score = svm_linear.score(X_linear_test_scaled, y_linear_test)\n",
    "\n",
    " # Cross-validation\n",
    " cv_scores = cross_val_score(svm_linear, X_linear_train_scaled, y_linear_train, cv=5)\n",
    "\n",
    " # Support vector count\n",
    " n_support = len(svm_linear.support_)\n",
    "\n",
    " linear_svm_results[C] = {\n",
    " 'train_accuracy': train_score,\n",
    " 'test_accuracy': test_score,\n",
    " 'cv_mean': cv_scores.mean(),\n",
    " 'cv_std': cv_scores.std(),\n",
    " 'n_support_vectors': n_support\n",
    " }\n",
    "\n",
    " print(f\"• C={C}: Test Acc={test_score:.4f}, CV={cv_scores.mean():.4f}±{cv_scores.std():.4f}, SV={n_support}\")\n",
    "\n",
    "# Find optimal C\n",
    "results_df = pd.DataFrame(linear_svm_results).T\n",
    "optimal_C = results_df['cv_mean'].idxmax()\n",
    "print(f\"\\n Optimal C: {optimal_C}\")\n",
    "\n",
    "# Visualize C parameter effect\n",
    "fig_c_effect = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=['Accuracy vs C Parameter', 'Support Vectors vs C Parameter']\n",
    ")\n",
    "\n",
    "# Accuracy plot\n",
    "fig_c_effect.add_trace(\n",
    " go.Scatter(\n",
    " x=list(C_values),\n",
    " y=results_df['train_accuracy'],\n",
    " mode='lines+markers',\n",
    " name='Training',\n",
    " line=dict(color='blue')\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig_c_effect.add_trace(\n",
    " go.Scatter(\n",
    " x=list(C_values),\n",
    " y=results_df['cv_mean'],\n",
    " mode='lines+markers',\n",
    " name='CV Mean',\n",
    " line=dict(color='green'),\n",
    " error_y=dict(type='data', array=results_df['cv_std'])\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig_c_effect.add_trace(\n",
    " go.Scatter(\n",
    " x=list(C_values),\n",
    " y=results_df['test_accuracy'],\n",
    " mode='lines+markers',\n",
    " name='Test',\n",
    " line=dict(color='red')\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Support vectors plot\n",
    "fig_c_effect.add_trace(\n",
    " go.Scatter(\n",
    " x=list(C_values),\n",
    " y=results_df['n_support_vectors'],\n",
    " mode='lines+markers',\n",
    " name='Support Vectors',\n",
    " line=dict(color='purple'),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_c_effect.update_xaxes(type=\"log\", title_text=\"C Parameter\", row=1, col=1)\n",
    "fig_c_effect.update_xaxes(type=\"log\", title_text=\"C Parameter\", row=1, col=2)\n",
    "fig_c_effect.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "fig_c_effect.update_yaxes(title_text=\"Number of Support Vectors\", row=1, col=2)\n",
    "\n",
    "fig_c_effect.update_layout(\n",
    " title=\"Linear SVM: C Parameter Analysis\",\n",
    " height=500\n",
    ")\n",
    "fig_c_effect.show()\n",
    "\n",
    "# Train final model with optimal C\n",
    "svm_linear_final = SVC(kernel='linear', C=optimal_C, random_state=42)\n",
    "svm_linear_final.fit(X_linear_train_scaled, y_linear_train)\n",
    "\n",
    "# Final predictions and metrics\n",
    "y_linear_pred = svm_linear_final.predict(X_linear_test_scaled)\n",
    "linear_accuracy = accuracy_score(y_linear_test, y_linear_pred)\n",
    "\n",
    "print(f\"\\n Final Linear SVM Performance:\")\n",
    "print(f\"• C = {optimal_C}\")\n",
    "print(f\"• Test Accuracy: {linear_accuracy:.4f}\")\n",
    "print(f\"• Support Vectors: {len(svm_linear_final.support_)}/{len(X_linear_train_scaled)} ({len(svm_linear_final.support_)/len(X_linear_train_scaled):.1%})\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_linear_test, y_linear_pred, target_names=['Rejected', 'Approved']))\n",
    "\n",
    "# Feature importance analysis (using coefficients)\n",
    "feature_importance = np.abs(svm_linear_final.coef_[0])\n",
    "feature_importance_normalized = feature_importance / feature_importance.sum()\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    " 'Feature': linear_features,\n",
    " 'Importance': feature_importance_normalized\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Importance (Linear SVM coefficients):\")\n",
    "for _, row in importance_df.iterrows():\n",
    " print(f\"• {row['Feature']}: {row['Importance']:.3f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "fig_feat_imp = go.Figure()\n",
    "\n",
    "fig_feat_imp.add_trace(\n",
    " go.Bar(\n",
    " x=importance_df['Feature'],\n",
    " y=importance_df['Importance'],\n",
    " marker_color='lightblue',\n",
    " hovertemplate=\"Feature: %{x}<br>Importance: %{y:.3f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_feat_imp.update_layout(\n",
    " title=\"Linear SVM Feature Importance\",\n",
    " xaxis_title=\"Features\",\n",
    " yaxis_title=\"Normalized Coefficient Magnitude\",\n",
    " xaxis_tickangle=-45,\n",
    " height=500\n",
    ")\n",
    "fig_feat_imp.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_linear = confusion_matrix(y_linear_test, y_linear_pred)\n",
    "\n",
    "fig_cm_linear = ff.create_annotated_heatmap(\n",
    " z=cm_linear,\n",
    " x=['Rejected', 'Approved'],\n",
    " y=['Rejected', 'Approved'],\n",
    " annotation_text=cm_linear,\n",
    " colorscale='Blues',\n",
    " showscale=True\n",
    ")\n",
    "\n",
    "fig_cm_linear.update_layout(\n",
    " title=f\"Linear SVM Confusion Matrix (C={optimal_C})\",\n",
    " xaxis_title=\"Predicted\",\n",
    " yaxis_title=\"Actual\",\n",
    " height=400\n",
    ")\n",
    "fig_cm_linear.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e34ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. KERNEL SVM ANALYSIS (NON-LINEAR)\n",
    "print(\" 2. KERNEL SVM ANALYSIS (NON-LINEAR)\")\n",
    "print(\"=\" * 36)\n",
    "\n",
    "# Prepare non-linear classification data\n",
    "nonlinear_features = ['customer_spending', 'customer_frequency', 'customer_tenure', 'support_tickets']\n",
    "X_nonlinear = nonlinear_df[nonlinear_features]\n",
    "y_nonlinear = nonlinear_df['value_segment']\n",
    "\n",
    "# Split data\n",
    "X_nonlinear_train, X_nonlinear_test, y_nonlinear_train, y_nonlinear_test = train_test_split(\n",
    " X_nonlinear, y_nonlinear, test_size=0.2, random_state=42, stratify=y_nonlinear\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_nonlinear = StandardScaler()\n",
    "X_nonlinear_train_scaled = scaler_nonlinear.fit_transform(X_nonlinear_train)\n",
    "X_nonlinear_test_scaled = scaler_nonlinear.transform(X_nonlinear_test)\n",
    "\n",
    "print(f\"Training set: {X_nonlinear_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_nonlinear_test_scaled.shape}\")\n",
    "print(f\"Class distribution: {y_nonlinear_train.value_counts().to_dict()}\")\n",
    "\n",
    "# Test different kernels\n",
    "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "kernel_results = {}\n",
    "\n",
    "print(f\"\\n Kernel Comparison:\")\n",
    "\n",
    "for kernel in kernels:\n",
    " if kernel == 'poly':\n",
    " svm_kernel = SVC(kernel=kernel, degree=3, C=1.0, random_state=42)\n",
    " else:\n",
    " svm_kernel = SVC(kernel=kernel, C=1.0, random_state=42)\n",
    "\n",
    " # Cross-validation\n",
    " cv_scores = cross_val_score(svm_kernel, X_nonlinear_train_scaled, y_nonlinear_train, cv=5)\n",
    "\n",
    " # Fit and test\n",
    " svm_kernel.fit(X_nonlinear_train_scaled, y_nonlinear_train)\n",
    " test_score = svm_kernel.score(X_nonlinear_test_scaled, y_nonlinear_test)\n",
    " n_support = len(svm_kernel.support_)\n",
    "\n",
    " kernel_results[kernel] = {\n",
    " 'cv_mean': cv_scores.mean(),\n",
    " 'cv_std': cv_scores.std(),\n",
    " 'test_accuracy': test_score,\n",
    " 'n_support_vectors': n_support\n",
    " }\n",
    "\n",
    " print(f\"• {kernel}: Test Acc={test_score:.4f}, CV={cv_scores.mean():.4f}±{cv_scores.std():.4f}, SV={n_support}\")\n",
    "\n",
    "# Find best kernel\n",
    "kernel_df = pd.DataFrame(kernel_results).T\n",
    "best_kernel = kernel_df['cv_mean'].idxmax()\n",
    "print(f\"\\n Best kernel: {best_kernel}\")\n",
    "\n",
    "# Visualize kernel comparison\n",
    "fig_kernels = go.Figure()\n",
    "\n",
    "fig_kernels.add_trace(\n",
    " go.Bar(\n",
    " x=list(kernel_results.keys()),\n",
    " y=[result['test_accuracy'] for result in kernel_results.values()],\n",
    " name='Test Accuracy',\n",
    " marker_color='lightcoral',\n",
    " hovertemplate=\"Kernel: %{x}<br>Accuracy: %{y:.4f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_kernels.update_layout(\n",
    " title=\"Kernel Comparison (Non-linear Dataset)\",\n",
    " xaxis_title=\"Kernel Type\",\n",
    " yaxis_title=\"Test Accuracy\",\n",
    " height=500\n",
    ")\n",
    "fig_kernels.show()\n",
    "\n",
    "# Hyperparameter optimization for RBF kernel\n",
    "print(f\"\\n RBF Kernel Hyperparameter Optimization:\")\n",
    "\n",
    "# Grid search for C and gamma\n",
    "param_grid = {\n",
    " 'C': [0.1, 1, 10, 100],\n",
    " 'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    " svm_rbf,\n",
    " param_grid,\n",
    " cv=5,\n",
    " scoring='accuracy',\n",
    " n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_nonlinear_train_scaled, y_nonlinear_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Test best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_nonlinear_pred = best_svm.predict(X_nonlinear_test_scaled)\n",
    "nonlinear_accuracy = accuracy_score(y_nonlinear_test, y_nonlinear_pred)\n",
    "\n",
    "print(f\"Test accuracy: {nonlinear_accuracy:.4f}\")\n",
    "print(f\"Support vectors: {len(best_svm.support_)}/{len(X_nonlinear_train_scaled)} ({len(best_svm.support_)/len(X_nonlinear_train_scaled):.1%})\")\n",
    "\n",
    "# Visualize hyperparameter grid search results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Create heatmap for C vs gamma\n",
    "C_values = param_grid['C']\n",
    "gamma_values = [g for g in param_grid['gamma'] if isinstance(g, (int, float))]\n",
    "\n",
    "# Filter results for numeric gamma values only\n",
    "numeric_results = results[results['param_gamma'].isin(gamma_values)]\n",
    "\n",
    "if len(numeric_results) > 0:\n",
    " heatmap_data = numeric_results.pivot_table(\n",
    " values='mean_test_score',\n",
    " index='param_C',\n",
    " columns='param_gamma',\n",
    " aggfunc='mean'\n",
    " )\n",
    "\n",
    " fig_heatmap = go.Figure(data=go.Heatmap(\n",
    " z=heatmap_data.values,\n",
    " x=[str(g) for g in heatmap_data.columns],\n",
    " y=[str(c) for c in heatmap_data.index],\n",
    " colorscale='Viridis',\n",
    " hovertemplate=\"C: %{y}<br>Gamma: %{x}<br>CV Score: %{z:.4f}<extra></extra>\"\n",
    " ))\n",
    "\n",
    " fig_heatmap.update_layout(\n",
    " title=\"RBF SVM Hyperparameter Grid Search\",\n",
    " xaxis_title=\"Gamma\",\n",
    " yaxis_title=\"C\",\n",
    " height=500\n",
    " )\n",
    " fig_heatmap.show()\n",
    "\n",
    "# Classification report for best model\n",
    "print(f\"\\nClassification Report (Best RBF SVM):\")\n",
    "print(classification_report(y_nonlinear_test, y_nonlinear_pred, target_names=['High Value', 'Standard']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_nonlinear = confusion_matrix(y_nonlinear_test, y_nonlinear_pred)\n",
    "\n",
    "fig_cm_nonlinear = ff.create_annotated_heatmap(\n",
    " z=cm_nonlinear,\n",
    " x=['High Value', 'Standard'],\n",
    " y=['High Value', 'Standard'],\n",
    " annotation_text=cm_nonlinear,\n",
    " colorscale='Blues',\n",
    " showscale=True\n",
    ")\n",
    "\n",
    "fig_cm_nonlinear.update_layout(\n",
    " title=f\"RBF SVM Confusion Matrix\",\n",
    " xaxis_title=\"Predicted\",\n",
    " yaxis_title=\"Actual\",\n",
    " height=400\n",
    ")\n",
    "fig_cm_nonlinear.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e9403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SUPPORT VECTOR REGRESSION (SVR)\n",
    "print(\" 3. SUPPORT VECTOR REGRESSION (SVR)\")\n",
    "print(\"=\" * 33)\n",
    "\n",
    "# Prepare regression data\n",
    "regression_features = ['lot_size', 'house_age', 'bedrooms', 'bathrooms',\n",
    " 'school_rating', 'crime_rate', 'distance_to_center']\n",
    "X_reg = regression_df[regression_features]\n",
    "y_reg = regression_df['price']\n",
    "\n",
    "# Split data\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    " X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features and target\n",
    "scaler_reg = StandardScaler()\n",
    "X_reg_train_scaled = scaler_reg.fit_transform(X_reg_train)\n",
    "X_reg_test_scaled = scaler_reg.transform(X_reg_test)\n",
    "\n",
    "# Scale target for better SVR performance\n",
    "y_scaler = StandardScaler()\n",
    "y_reg_train_scaled = y_scaler.fit_transform(y_reg_train.values.reshape(-1, 1)).ravel()\n",
    "y_reg_test_scaled = y_scaler.transform(y_reg_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "print(f\"Training set: {X_reg_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_reg_test_scaled.shape}\")\n",
    "\n",
    "# Test different SVR kernels\n",
    "svr_kernels = ['linear', 'rbf', 'poly']\n",
    "svr_results = {}\n",
    "\n",
    "print(f\"\\n SVR Kernel Comparison:\")\n",
    "\n",
    "for kernel in svr_kernels:\n",
    " if kernel == 'poly':\n",
    " svr_model = SVR(kernel=kernel, degree=3, C=1.0)\n",
    " else:\n",
    " svr_model = SVR(kernel=kernel, C=1.0)\n",
    "\n",
    " # Fit model\n",
    " svr_model.fit(X_reg_train_scaled, y_reg_train_scaled)\n",
    "\n",
    " # Predictions (transform back to original scale)\n",
    " y_pred_scaled = svr_model.predict(X_reg_test_scaled)\n",
    " y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    " # Metrics\n",
    " mse = mean_squared_error(y_reg_test, y_pred)\n",
    " r2 = r2_score(y_reg_test, y_pred)\n",
    " mae = mean_absolute_error(y_reg_test, y_pred)\n",
    " n_support = len(svr_model.support_)\n",
    "\n",
    " svr_results[kernel] = {\n",
    " 'MSE': mse,\n",
    " 'R2': r2,\n",
    " 'MAE': mae,\n",
    " 'RMSE': np.sqrt(mse),\n",
    " 'n_support_vectors': n_support\n",
    " }\n",
    "\n",
    " print(f\"• {kernel}: R²={r2:.4f}, RMSE=${np.sqrt(mse):,.0f}, SV={n_support}\")\n",
    "\n",
    "# Find best SVR kernel\n",
    "svr_df = pd.DataFrame(svr_results).T\n",
    "best_svr_kernel = svr_df['R2'].idxmax()\n",
    "print(f\"\\n Best SVR kernel: {best_svr_kernel}\")\n",
    "\n",
    "# Visualize SVR kernel comparison\n",
    "fig_svr_kernels = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=['R² Score by Kernel', 'RMSE by Kernel']\n",
    ")\n",
    "\n",
    "fig_svr_kernels.add_trace(\n",
    " go.Bar(\n",
    " x=list(svr_results.keys()),\n",
    " y=[result['R2'] for result in svr_results.values()],\n",
    " name='R² Score',\n",
    " marker_color='lightgreen'\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig_svr_kernels.add_trace(\n",
    " go.Bar(\n",
    " x=list(svr_results.keys()),\n",
    " y=[result['RMSE'] for result in svr_results.values()],\n",
    " name='RMSE',\n",
    " marker_color='lightcoral',\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_svr_kernels.update_layout(\n",
    " title=\"SVR Kernel Comparison\",\n",
    " height=500\n",
    ")\n",
    "\n",
    "fig_svr_kernels.update_yaxes(title_text=\"R² Score\", row=1, col=1)\n",
    "fig_svr_kernels.update_yaxes(title_text=\"RMSE ($)\", row=1, col=2)\n",
    "\n",
    "fig_svr_kernels.show()\n",
    "\n",
    "# Hyperparameter optimization for best kernel\n",
    "print(f\"\\n SVR Hyperparameter Optimization ({best_svr_kernel} kernel):\")\n",
    "\n",
    "# Grid search for SVR\n",
    "svr_param_grid = {\n",
    " 'C': [0.1, 1, 10, 100],\n",
    " 'epsilon': [0.01, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "if best_svr_kernel == 'rbf':\n",
    " svr_param_grid['gamma'] = ['scale', 'auto', 0.001, 0.01, 0.1]\n",
    "\n",
    "svr_best = SVR(kernel=best_svr_kernel)\n",
    "svr_grid_search = GridSearchCV(\n",
    " svr_best,\n",
    " svr_param_grid,\n",
    " cv=5,\n",
    " scoring='r2',\n",
    " n_jobs=-1\n",
    ")\n",
    "\n",
    "svr_grid_search.fit(X_reg_train_scaled, y_reg_train_scaled)\n",
    "\n",
    "print(f\"Best parameters: {svr_grid_search.best_params_}\")\n",
    "print(f\"Best CV R²: {svr_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Final SVR model evaluation\n",
    "best_svr = svr_grid_search.best_estimator_\n",
    "y_reg_pred_scaled = best_svr.predict(X_reg_test_scaled)\n",
    "y_reg_pred = y_scaler.inverse_transform(y_reg_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Final metrics\n",
    "final_mse = mean_squared_error(y_reg_test, y_reg_pred)\n",
    "final_r2 = r2_score(y_reg_test, y_reg_pred)\n",
    "final_mae = mean_absolute_error(y_reg_test, y_reg_pred)\n",
    "\n",
    "print(f\"\\n Final SVR Performance:\")\n",
    "print(f\"• Kernel: {best_svr_kernel}\")\n",
    "print(f\"• Test R²: {final_r2:.4f}\")\n",
    "print(f\"• Test RMSE: ${np.sqrt(final_mse):,.0f}\")\n",
    "print(f\"• Test MAE: ${final_mae:,.0f}\")\n",
    "print(f\"• Support Vectors: {len(best_svr.support_)}/{len(X_reg_train_scaled)} ({len(best_svr.support_)/len(X_reg_train_scaled):.1%})\")\n",
    "\n",
    "# Actual vs Predicted plot\n",
    "fig_svr_pred = go.Figure()\n",
    "\n",
    "fig_svr_pred.add_trace(\n",
    " go.Scatter(\n",
    " x=y_reg_test,\n",
    " y=y_reg_pred,\n",
    " mode='markers',\n",
    " marker=dict(color='blue', opacity=0.6),\n",
    " name='Predictions',\n",
    " hovertemplate=\"Actual: $%{x:,.0f}<br>Predicted: $%{y:,.0f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "# Perfect prediction line\n",
    "min_price = min(y_reg_test.min(), y_reg_pred.min())\n",
    "max_price = max(y_reg_test.max(), y_reg_pred.max())\n",
    "\n",
    "fig_svr_pred.add_trace(\n",
    " go.Scatter(\n",
    " x=[min_price, max_price],\n",
    " y=[min_price, max_price],\n",
    " mode='lines',\n",
    " line=dict(color='red', dash='dash'),\n",
    " name='Perfect Prediction',\n",
    " hovertemplate=\"Perfect Line<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_svr_pred.update_layout(\n",
    " title=f\"SVR: Actual vs Predicted Prices ({best_svr_kernel} kernel)\",\n",
    " xaxis_title=\"Actual Price ($)\",\n",
    " yaxis_title=\"Predicted Price ($)\",\n",
    " height=500\n",
    ")\n",
    "fig_svr_pred.show()\n",
    "\n",
    "# Residuals analysis\n",
    "residuals = y_reg_test - y_reg_pred\n",
    "\n",
    "fig_residuals = go.Figure()\n",
    "\n",
    "fig_residuals.add_trace(\n",
    " go.Scatter(\n",
    " x=y_reg_pred,\n",
    " y=residuals,\n",
    " mode='markers',\n",
    " marker=dict(color='green', opacity=0.6),\n",
    " hovertemplate=\"Predicted: $%{x:,.0f}<br>Residual: $%{y:,.0f}<extra></extra>\"\n",
    " )\n",
    ")\n",
    "\n",
    "fig_residuals.add_hline(y=0, line_dash=\"dash\", line_color=\"red\")\n",
    "\n",
    "fig_residuals.update_layout(\n",
    " title=\"SVR Residuals Analysis\",\n",
    " xaxis_title=\"Predicted Price ($)\",\n",
    " yaxis_title=\"Residuals ($)\",\n",
    " height=500\n",
    ")\n",
    "fig_residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LEARNING CURVES AND MODEL COMPLEXITY\n",
    "print(\" 4. LEARNING CURVES AND MODEL COMPLEXITY\")\n",
    "print(\"=\" * 41)\n",
    "\n",
    "# Learning curves for different models\n",
    "def plot_learning_curves(estimator, X, y, title, cv=5):\n",
    " \"\"\"Plot learning curves for an estimator\"\"\"\n",
    "\n",
    " train_sizes = np.linspace(0.1, 1.0, 10)\n",
    " train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    " estimator, X, y, train_sizes=train_sizes, cv=cv, n_jobs=-1, random_state=42\n",
    " )\n",
    "\n",
    " train_mean = train_scores.mean(axis=1)\n",
    " train_std = train_scores.std(axis=1)\n",
    " val_mean = val_scores.mean(axis=1)\n",
    " val_std = val_scores.std(axis=1)\n",
    "\n",
    " return train_sizes_abs, train_mean, train_std, val_mean, val_std\n",
    "\n",
    "# Generate learning curves for different SVM configurations\n",
    "print(\" Generating Learning Curves...\")\n",
    "\n",
    "# Linear SVM learning curve\n",
    "linear_svm_lc = SVC(kernel='linear', C=optimal_C, random_state=42)\n",
    "train_sizes, lin_train_mean, lin_train_std, lin_val_mean, lin_val_std = plot_learning_curves(\n",
    " linear_svm_lc, X_linear_train_scaled, y_linear_train, \"Linear SVM\"\n",
    ")\n",
    "\n",
    "# RBF SVM learning curve\n",
    "rbf_svm_lc = SVC(kernel='rbf', C=grid_search.best_params_['C'],\n",
    " gamma=grid_search.best_params_['gamma'], random_state=42)\n",
    "_, rbf_train_mean, rbf_train_std, rbf_val_mean, rbf_val_std = plot_learning_curves(\n",
    " rbf_svm_lc, X_nonlinear_train_scaled, y_nonlinear_train, \"RBF SVM\"\n",
    ")\n",
    "\n",
    "# Plot learning curves\n",
    "fig_lc = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=['Linear SVM Learning Curve', 'RBF SVM Learning Curve']\n",
    ")\n",
    "\n",
    "# Linear SVM\n",
    "fig_lc.add_trace(\n",
    " go.Scatter(\n",
    " x=train_sizes,\n",
    " y=lin_train_mean,\n",
    " mode='lines+markers',\n",
    " name='Training',\n",
    " line=dict(color='blue'),\n",
    " error_y=dict(type='data', array=lin_train_std)\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig_lc.add_trace(\n",
    " go.Scatter(\n",
    " x=train_sizes,\n",
    " y=lin_val_mean,\n",
    " mode='lines+markers',\n",
    " name='Validation',\n",
    " line=dict(color='red'),\n",
    " error_y=dict(type='data', array=lin_val_std),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# RBF SVM\n",
    "fig_lc.add_trace(\n",
    " go.Scatter(\n",
    " x=train_sizes,\n",
    " y=rbf_train_mean,\n",
    " mode='lines+markers',\n",
    " name='Training',\n",
    " line=dict(color='blue'),\n",
    " error_y=dict(type='data', array=rbf_train_std),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_lc.add_trace(\n",
    " go.Scatter(\n",
    " x=train_sizes,\n",
    " y=rbf_val_mean,\n",
    " mode='lines+markers',\n",
    " name='Validation',\n",
    " line=dict(color='red'),\n",
    " error_y=dict(type='data', array=rbf_val_std),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_lc.update_layout(\n",
    " title=\"SVM Learning Curves\",\n",
    " height=500\n",
    ")\n",
    "\n",
    "fig_lc.update_xaxes(title_text=\"Training Set Size\", row=1, col=1)\n",
    "fig_lc.update_xaxes(title_text=\"Training Set Size\", row=1, col=2)\n",
    "fig_lc.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "fig_lc.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "\n",
    "fig_lc.show()\n",
    "\n",
    "# Validation curves for C parameter\n",
    "print(f\"\\n Validation Curves for C Parameter:\")\n",
    "\n",
    "# C parameter validation curve for Linear SVM\n",
    "C_range = np.logspace(-3, 2, 10)\n",
    "train_scores_c, val_scores_c = validation_curve(\n",
    " SVC(kernel='linear', random_state=42),\n",
    " X_linear_train_scaled, y_linear_train,\n",
    " param_name='C', param_range=C_range, cv=5, n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mean_c = train_scores_c.mean(axis=1)\n",
    "train_std_c = train_scores_c.std(axis=1)\n",
    "val_mean_c = val_scores_c.mean(axis=1)\n",
    "val_std_c = val_scores_c.std(axis=1)\n",
    "\n",
    "# Gamma parameter validation curve for RBF SVM\n",
    "gamma_range = np.logspace(-4, 1, 10)\n",
    "train_scores_g, val_scores_g = validation_curve(\n",
    " SVC(kernel='rbf', C=1.0, random_state=42),\n",
    " X_nonlinear_train_scaled, y_nonlinear_train,\n",
    " param_name='gamma', param_range=gamma_range, cv=5, n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mean_g = train_scores_g.mean(axis=1)\n",
    "train_std_g = train_scores_g.std(axis=1)\n",
    "val_mean_g = val_scores_g.mean(axis=1)\n",
    "val_std_g = val_scores_g.std(axis=1)\n",
    "\n",
    "# Plot validation curves\n",
    "fig_vc = make_subplots(\n",
    " rows=1, cols=2,\n",
    " subplot_titles=['C Parameter Validation Curve', 'Gamma Parameter Validation Curve']\n",
    ")\n",
    "\n",
    "# C parameter\n",
    "fig_vc.add_trace(\n",
    " go.Scatter(\n",
    " x=C_range,\n",
    " y=train_mean_c,\n",
    " mode='lines+markers',\n",
    " name='Training',\n",
    " line=dict(color='blue'),\n",
    " error_y=dict(type='data', array=train_std_c)\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig_vc.add_trace(\n",
    " go.Scatter(\n",
    " x=C_range,\n",
    " y=val_mean_c,\n",
    " mode='lines+markers',\n",
    " name='Validation',\n",
    " line=dict(color='red'),\n",
    " error_y=dict(type='data', array=val_std_c),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Gamma parameter\n",
    "fig_vc.add_trace(\n",
    " go.Scatter(\n",
    " x=gamma_range,\n",
    " y=train_mean_g,\n",
    " mode='lines+markers',\n",
    " name='Training',\n",
    " line=dict(color='blue'),\n",
    " error_y=dict(type='data', array=train_std_g),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_vc.add_trace(\n",
    " go.Scatter(\n",
    " x=gamma_range,\n",
    " y=val_mean_g,\n",
    " mode='lines+markers',\n",
    " name='Validation',\n",
    " line=dict(color='red'),\n",
    " error_y=dict(type='data', array=val_std_g),\n",
    " showlegend=False\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig_vc.update_xaxes(type=\"log\", title_text=\"C Parameter\", row=1, col=1)\n",
    "fig_vc.update_xaxes(type=\"log\", title_text=\"Gamma Parameter\", row=1, col=2)\n",
    "fig_vc.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "fig_vc.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "\n",
    "fig_vc.update_layout(\n",
    " title=\"SVM Validation Curves\",\n",
    " height=500\n",
    ")\n",
    "fig_vc.show()\n",
    "\n",
    "# Model complexity analysis\n",
    "print(f\"\\n Model Complexity Analysis:\")\n",
    "\n",
    "# Support vector analysis\n",
    "print(f\"Support Vector Statistics:\")\n",
    "print(f\"• Linear SVM: {len(svm_linear_final.support_)} support vectors ({len(svm_linear_final.support_)/len(X_linear_train_scaled):.1%} of training data)\")\n",
    "print(f\"• RBF SVM: {len(best_svm.support_)} support vectors ({len(best_svm.support_)/len(X_nonlinear_train_scaled):.1%} of training data)\")\n",
    "print(f\"• SVR: {len(best_svr.support_)} support vectors ({len(best_svr.support_)/len(X_reg_train_scaled):.1%} of training data)\")\n",
    "\n",
    "# Margin analysis for Linear SVM\n",
    "margin = 2 / np.sqrt(np.sum(svm_linear_final.coef_ ** 2))\n",
    "print(f\"\\nLinear SVM Margin Analysis:\")\n",
    "print(f\"• Decision boundary margin: {margin:.4f}\")\n",
    "print(f\"• This represents the distance between support vectors and decision boundary\")\n",
    "\n",
    "# Training time comparison\n",
    "import time\n",
    "\n",
    "training_times = {}\n",
    "\n",
    "# Linear SVM timing\n",
    "start_time = time.time()\n",
    "svm_linear_timing = SVC(kernel='linear', C=optimal_C, random_state=42)\n",
    "svm_linear_timing.fit(X_linear_train_scaled, y_linear_train)\n",
    "training_times['Linear SVM'] = time.time() - start_time\n",
    "\n",
    "# RBF SVM timing\n",
    "start_time = time.time()\n",
    "svm_rbf_timing = SVC(kernel='rbf', C=1.0, random_state=42)\n",
    "svm_rbf_timing.fit(X_nonlinear_train_scaled, y_nonlinear_train)\n",
    "training_times['RBF SVM'] = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining Time Comparison:\")\n",
    "for model, time_taken in training_times.items():\n",
    " print(f\"• {model}: {time_taken:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\n",
    "print(\" 5. BUSINESS INSIGHTS AND STRATEGIC RECOMMENDATIONS\")\n",
    "print(\"=\" * 54)\n",
    "\n",
    "# Decision boundary analysis and interpretability\n",
    "print(\" SVM Decision Boundary Analysis:\")\n",
    "\n",
    "print(f\"\\n1. MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\" • Linear SVM (Credit Approval): {linear_accuracy:.1%} accuracy\")\n",
    "print(f\" • RBF SVM (Customer Segmentation): {nonlinear_accuracy:.1%} accuracy\")\n",
    "print(f\" • SVR (House Price Prediction): R² = {final_r2:.3f}\")\n",
    "\n",
    "# Support vector insights\n",
    "print(f\"\\n2. SUPPORT VECTOR INSIGHTS:\")\n",
    "linear_sv_pct = len(svm_linear_final.support_) / len(X_linear_train_scaled) * 100\n",
    "rbf_sv_pct = len(best_svm.support_) / len(X_nonlinear_train_scaled) * 100\n",
    "svr_sv_pct = len(best_svr.support_) / len(X_reg_train_scaled) * 100\n",
    "\n",
    "print(f\" • Linear SVM uses {linear_sv_pct:.1f}% of training data as support vectors\")\n",
    "print(f\" • RBF SVM uses {rbf_sv_pct:.1f}% of training data as support vectors\")\n",
    "print(f\" • SVR uses {svr_sv_pct:.1f}% of training data as support vectors\")\n",
    "\n",
    "if linear_sv_pct < 50:\n",
    " print(\" • Linear model: Good separation with clear margin\")\n",
    "else:\n",
    " print(\" • Linear model: Complex decision boundary, may benefit from feature engineering\")\n",
    "\n",
    "if rbf_sv_pct < 30:\n",
    " print(\" • RBF model: Efficient non-linear separation\")\n",
    "elif rbf_sv_pct > 70:\n",
    " print(\" • RBF model: High complexity, consider regularization\")\n",
    "else:\n",
    " print(\" • RBF model: Balanced complexity for non-linear patterns\")\n",
    "\n",
    "# Feature importance insights\n",
    "print(f\"\\n3. FEATURE IMPORTANCE INSIGHTS (Linear SVM):\")\n",
    "top_3_features = importance_df.head(3)\n",
    "for i, (_, row) in enumerate(top_3_features.iterrows(), 1):\n",
    " print(f\" • #{i} {row['Feature']}: {row['Importance']:.3f} importance\")\n",
    "\n",
    "most_important = top_3_features.iloc[0]['Feature']\n",
    "least_important = importance_df.tail(1).iloc[0]['Feature']\n",
    "\n",
    "print(f\" • Focus data quality efforts on: {most_important}\")\n",
    "print(f\" • Consider removing: {least_important} (lowest impact)\")\n",
    "\n",
    "# Hyperparameter insights\n",
    "print(f\"\\n4. HYPERPARAMETER INSIGHTS:\")\n",
    "print(f\" • Optimal Linear SVM C: {optimal_C}\")\n",
    "if optimal_C < 1:\n",
    " print(\" - Low C suggests high regularization needed\")\n",
    " print(\" - Data may have noise or overlapping classes\")\n",
    "elif optimal_C > 10:\n",
    " print(\" - High C suggests low regularization needed\")\n",
    " print(\" - Data is well-separated\")\n",
    "else:\n",
    " print(\" - Moderate C suggests balanced regularization\")\n",
    "\n",
    "print(f\" • Optimal RBF SVM parameters: C={grid_search.best_params_['C']}, gamma={grid_search.best_params_['gamma']}\")\n",
    "\n",
    "rbf_c = grid_search.best_params_['C']\n",
    "rbf_gamma = grid_search.best_params_['gamma']\n",
    "\n",
    "if isinstance(rbf_gamma, str):\n",
    " print(f\" - Using automatic gamma scaling\")\n",
    "elif rbf_gamma < 0.1:\n",
    " print(f\" - Low gamma: smooth decision boundary\")\n",
    "else:\n",
    " print(f\" - High gamma: complex decision boundary\")\n",
    "\n",
    "# Business application strategies\n",
    "print(f\"\\n5. BUSINESS APPLICATION STRATEGIES:\")\n",
    "\n",
    "print(f\"\\n Credit Approval System (Linear SVM):\")\n",
    "print(f\" • Deploy for automated credit decisions\")\n",
    "print(f\" • {linear_accuracy:.1%} accuracy reduces manual review by ~{linear_accuracy*100-50:.0f}%\")\n",
    "print(f\" • Most important factor: {most_important}\")\n",
    "print(f\" • Support vectors represent edge cases for manual review\")\n",
    "print(f\" • Recommended: A/B test against current decision rules\")\n",
    "\n",
    "print(f\"\\n Customer Segmentation (RBF SVM):\")\n",
    "print(f\" • {nonlinear_accuracy:.1%} accuracy for customer targeting\")\n",
    "print(f\" • Non-linear patterns suggest complex customer behaviors\")\n",
    "print(f\" • Use for personalized marketing campaigns\")\n",
    "print(f\" • Support vectors identify boundary customers for special attention\")\n",
    "\n",
    "print(f\"\\n House Price Prediction (SVR):\")\n",
    "print(f\" • R² = {final_r2:.3f} explains {final_r2*100:.1f}% of price variance\")\n",
    "print(f\" • Average prediction error: ${final_mae:,.0f}\")\n",
    "print(f\" • Use for automated property valuation\")\n",
    "print(f\" • Support vectors represent unique/complex properties\")\n",
    "\n",
    "# ROI and cost-benefit analysis\n",
    "print(f\"\\n6. ROI AND COST-BENEFIT ANALYSIS:\")\n",
    "\n",
    "# Credit approval ROI\n",
    "credit_volume = 10000 # Annual applications\n",
    "current_approval_rate = 0.7\n",
    "manual_review_cost = 50 # per application\n",
    "automated_cost = 5 # per application\n",
    "\n",
    "manual_cost = credit_volume * manual_review_cost\n",
    "automated_cost_total = credit_volume * automated_cost\n",
    "accuracy_benefit = linear_accuracy - 0.7 # vs random/current system\n",
    "\n",
    "print(f\"\\n Credit Approval System:\")\n",
    "print(f\" • Manual review cost: ${manual_cost:,}/year\")\n",
    "print(f\" • Automated system cost: ${automated_cost_total:,}/year\")\n",
    "print(f\" • Cost savings: ${manual_cost - automated_cost_total:,}/year\")\n",
    "print(f\" • Accuracy improvement: +{accuracy_benefit:.1%}\")\n",
    "print(f\" • Break-even: ~{automated_cost_total/(manual_review_cost-automated_cost):,.0f} applications\")\n",
    "\n",
    "# Customer segmentation ROI\n",
    "customer_base = 50000\n",
    "campaign_cost_per_customer = 10\n",
    "conversion_rate_improvement = 0.15 # 15% improvement\n",
    "revenue_per_conversion = 100\n",
    "\n",
    "segmentation_revenue = customer_base * campaign_cost_per_customer * conversion_rate_improvement * revenue_per_conversion / campaign_cost_per_customer\n",
    "\n",
    "print(f\"\\n Customer Segmentation:\")\n",
    "print(f\" • Improved targeting on {customer_base:,} customers\")\n",
    "print(f\" • Expected conversion improvement: +{conversion_rate_improvement:.1%}\")\n",
    "print(f\" • Additional annual revenue: ${segmentation_revenue:,.0f}\")\n",
    "print(f\" • ROI: {segmentation_revenue/(customer_base*2):,.0f}x (assuming $2/customer implementation cost)\")\n",
    "\n",
    "# Implementation recommendations\n",
    "print(f\"\\n7. IMPLEMENTATION RECOMMENDATIONS:\")\n",
    "\n",
    "print(f\"\\n Technical Implementation:\")\n",
    "print(f\" • Use scikit-learn Pipeline for preprocessing consistency\")\n",
    "print(f\" • Implement model versioning and A/B testing framework\")\n",
    "print(f\" • Monitor support vector count for model drift detection\")\n",
    "print(f\" • Set up automated retraining when performance degrades\")\n",
    "print(f\" • Consider approximate methods for large-scale deployment\")\n",
    "\n",
    "print(f\"\\n Monitoring and Maintenance:\")\n",
    "print(f\" • Track prediction confidence and flag low-confidence cases\")\n",
    "print(f\" • Monitor support vector characteristics over time\")\n",
    "print(f\" • Retrain when support vector percentage changes significantly\")\n",
    "print(f\" • Validate model assumptions quarterly\")\n",
    "\n",
    "print(f\"\\n Risk Management:\")\n",
    "print(f\" • Implement prediction explanation for regulatory compliance\")\n",
    "print(f\" • Set confidence thresholds for automated decisions\")\n",
    "print(f\" • Maintain human oversight for edge cases\")\n",
    "print(f\" • Regular bias audits on decision boundaries\")\n",
    "\n",
    "print(f\"\\n8. NEXT STEPS AND ADVANCED TECHNIQUES:\")\n",
    "print(f\" • Experiment with ensemble methods combining multiple kernels\")\n",
    "print(f\" • Investigate feature engineering for better linear separability\")\n",
    "print(f\" • Consider online/incremental SVM for streaming data\")\n",
    "print(f\" • Explore kernel customization for domain-specific problems\")\n",
    "print(f\" • Implement SHAP values for better model interpretability\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\" SVM LEARNING SUMMARY:\")\n",
    "print(f\" Mastered linear and non-linear SVM classification\")\n",
    "print(f\" Optimized hyperparameters using grid search and cross-validation\")\n",
    "print(f\" Applied SVR for regression problems with kernel tricks\")\n",
    "print(f\" Analyzed support vectors and decision boundary characteristics\")\n",
    "print(f\" Understood model complexity trade-offs and performance curves\")\n",
    "print(f\" Generated comprehensive business insights and ROI analysis\")\n",
    "print(f\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}