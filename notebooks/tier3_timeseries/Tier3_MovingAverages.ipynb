{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 3: Moving Averages\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** 5a09dff5-9796-4031-9162-0fa197b52714\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 3: Moving Averages,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** 5a09dff5-9796-4031-9162-0fa197b52714\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c850972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries for Moving Averages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical and filtering methods\n",
    "from scipy import signal\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from pykalman import KalmanFilter\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Technical analysis\n",
    "import talib\n",
    "from ta.trend import SMAIndicator, EMAIndicator, WMAIndicator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 3: Moving Averages - Libraries Loaded!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Available Moving Average Techniques:\")\n",
    "print(\"• Simple Moving Average (SMA) - Equal weight averaging\")\n",
    "print(\"• Weighted Moving Average (WMA) - Linear weight progression\")\n",
    "print(\"• Exponential Moving Average (EMA) - Exponential decay weighting\")\n",
    "print(\"• Hull Moving Average (HMA) - Reduced lag weighted average\")\n",
    "print(\"• Adaptive Moving Average (AMA) - Volatility-adjusted smoothing\")\n",
    "print(\"• Kalman Filter - Dynamic state estimation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ef905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Moving Average Optimized Datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "def create_moving_average_datasets():\n",
    " \"\"\"Create time series datasets optimized for moving average analysis\"\"\"\n",
    "\n",
    " # 1. STOCK PRICE DATA: Trending with volatility clustering\n",
    " n_days = 252 * 3 # 3 years of trading days\n",
    " dates = pd.date_range('2021-01-04', periods=n_days, freq='B')\n",
    "\n",
    " # Base stock price with trending behavior\n",
    " initial_price = 100\n",
    " daily_returns = np.random.normal(0.0008, 0.02, n_days) # 20% annual vol, 20% annual return\n",
    "\n",
    " # Add momentum and mean reversion effects\n",
    " momentum_factor = 0.1\n",
    " mean_reversion = 0.05\n",
    "\n",
    " for i in range(1, len(daily_returns)):\n",
    " # Momentum: trending behavior\n",
    " momentum = momentum_factor * daily_returns[i-1]\n",
    " # Mean reversion: pull back to long-term average\n",
    " price_level = initial_price * np.exp(np.sum(daily_returns[:i]))\n",
    " reversion = -mean_reversion * (price_level - 110) / 110 # Revert to $110\n",
    "\n",
    " daily_returns[i] += momentum + reversion\n",
    "\n",
    " # Add volatility clustering (GARCH effects)\n",
    " vol_clusters = np.random.choice(n_days//20, size=8, replace=False) * 20\n",
    " for cluster_start in vol_clusters:\n",
    " cluster_end = min(cluster_start + 15, n_days)\n",
    " daily_returns[cluster_start:cluster_end] *= 1.8 # Higher volatility periods\n",
    "\n",
    " # Calculate stock prices\n",
    " stock_prices = initial_price * np.exp(np.cumsum(daily_returns))\n",
    "\n",
    " stock_df = pd.DataFrame({\n",
    " 'date': dates,\n",
    " 'price': stock_prices,\n",
    " 'returns': daily_returns,\n",
    " 'log_price': np.log(stock_prices)\n",
    " }).set_index('date')\n",
    "\n",
    " # 2. ECONOMIC INDICATOR: GDP with business cycles\n",
    " n_quarters = 60 # 15 years of quarterly data\n",
    " quarter_dates = pd.date_range('2009Q1', periods=n_quarters, freq='Q')\n",
    "\n",
    " # Long-term growth trend\n",
    " base_gdp = 15_000_000 # $15T base GDP\n",
    " quarterly_growth_rate = 0.005 # 2% annual growth\n",
    " trend_gdp = base_gdp * (1 + quarterly_growth_rate) ** np.arange(n_quarters)\n",
    "\n",
    " # Business cycle component (7-year cycle)\n",
    " cycle_amplitude = 0.03 # 3% amplitude\n",
    " cycle_frequency = 2 * np.pi / (7 * 4) # 7-year cycle in quarters\n",
    " business_cycle = cycle_amplitude * np.sin(cycle_frequency * np.arange(n_quarters))\n",
    "\n",
    " # Add recession shocks\n",
    " recession_quarters = [8, 9, 10, 44, 45] # Financial crisis and COVID\n",
    " recession_impact = np.zeros(n_quarters)\n",
    " for q in recession_quarters:\n",
    " if q < n_quarters:\n",
    " recession_impact[q] = -0.08 # 8% GDP decline\n",
    "\n",
    " # Combine components\n",
    " gdp_growth = quarterly_growth_rate + business_cycle + recession_impact\n",
    " gdp_values = trend_gdp * (1 + gdp_growth)\n",
    "\n",
    " # Add measurement noise\n",
    " gdp_noise = np.random.normal(0, base_gdp * 0.005, n_quarters)\n",
    " gdp_values += gdp_noise\n",
    "\n",
    " gdp_df = pd.DataFrame({\n",
    " 'date': quarter_dates,\n",
    " 'gdp': gdp_values,\n",
    " 'trend_true': trend_gdp,\n",
    " 'cycle_true': business_cycle,\n",
    " 'growth_rate': gdp_growth\n",
    " }).set_index('date')\n",
    "\n",
    " # 3. INDUSTRIAL PROCESS: Quality measurements with drift\n",
    " n_hours = 24 * 30 # 30 days of hourly measurements\n",
    " process_dates = pd.date_range('2024-01-01', periods=n_hours, freq='H')\n",
    "\n",
    " # Target process value with gradual drift\n",
    " target_value = 50.0\n",
    " drift_rate = 0.001 # Gradual upward drift per hour\n",
    " process_drift = drift_rate * np.arange(n_hours)\n",
    "\n",
    " # Process noise (normal operation)\n",
    " process_noise = np.random.normal(0, 1.5, n_hours)\n",
    "\n",
    " # Occasional process disturbances\n",
    " disturbance_times = np.random.choice(n_hours, size=20, replace=False)\n",
    " disturbances = np.zeros(n_hours)\n",
    " for t in disturbance_times:\n",
    " disturbances[t] = np.random.choice([-8, -6, 6, 8]) # Equipment issues\n",
    "\n",
    " # Shift changes (slight offset every 8 hours)\n",
    " shift_pattern = np.tile([0, 0.5, -0.3], n_hours//24 + 1)[:n_hours]\n",
    "\n",
    " # Combine all effects\n",
    " process_values = target_value + process_drift + process_noise + disturbances + shift_pattern\n",
    "\n",
    " process_df = pd.DataFrame({\n",
    " 'date': process_dates,\n",
    " 'measurement': process_values,\n",
    " 'target': target_value,\n",
    " 'drift_true': process_drift,\n",
    " 'disturbances': disturbances\n",
    " }).set_index('date')\n",
    "\n",
    " return stock_df, gdp_df, process_df\n",
    "\n",
    "stock_df, gdp_df, process_df = create_moving_average_datasets()\n",
    "\n",
    "print(\" Moving Average Datasets Created:\")\n",
    "print(f\"Stock Prices: {len(stock_df)} trading days ({stock_df.index[0].strftime('%Y-%m-%d')} to {stock_df.index[-1].strftime('%Y-%m-%d')})\")\n",
    "print(f\"GDP Data: {len(gdp_df)} quarters ({gdp_df.index[0].strftime('%Y-Q%q')} to {gdp_df.index[-1].strftime('%Y-Q%q')})\")\n",
    "print(f\"Process Control: {len(process_df)} hours ({process_df.index[0].strftime('%Y-%m-%d %H:%M')} to {process_df.index[-1].strftime('%Y-%m-%d %H:%M')})\")\n",
    "\n",
    "print(f\"\\nDataset Characteristics:\")\n",
    "print(f\"Stock Price: ${stock_df['price'].min():.2f} - ${stock_df['price'].max():.2f}\")\n",
    "print(f\"GDP: ${gdp_df['gdp'].min()/1e12:.2f}T - ${gdp_df['gdp'].max()/1e12:.2f}T\")\n",
    "print(f\"Process: {process_df['measurement'].min():.1f} - {process_df['measurement'].max():.1f}\")\n",
    "\n",
    "# Calculate basic statistics\n",
    "stock_volatility = stock_df['returns'].std() * np.sqrt(252) * 100\n",
    "gdp_volatility = gdp_df['gdp'].pct_change().std() * np.sqrt(4) * 100\n",
    "process_noise_level = process_df['measurement'].std()\n",
    "\n",
    "print(f\"\\nVolatility Metrics:\")\n",
    "print(f\"Stock annualized volatility: {stock_volatility:.1f}%\")\n",
    "print(f\"GDP annualized volatility: {gdp_volatility:.1f}%\")\n",
    "print(f\"Process noise level: {process_noise_level:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SIMPLE MOVING AVERAGES (SMA)\n",
    "print(\" 1. SIMPLE MOVING AVERAGES (SMA)\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "def calculate_sma(series, window):\n",
    " \"\"\"Calculate Simple Moving Average\"\"\"\n",
    " return series.rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "def moving_average_analysis(series, windows, series_name):\n",
    " \"\"\"Comprehensive moving average analysis\"\"\"\n",
    "\n",
    " results = {}\n",
    "\n",
    " for window in windows:\n",
    " sma = calculate_sma(series, window)\n",
    "\n",
    " # Calculate lag\n",
    " crossings = ((series > sma) != (series.shift(1) > sma.shift(1))).sum()\n",
    "\n",
    " # Smoothness (reduction in variance)\n",
    " original_var = series.var()\n",
    " smoothed_var = sma.var()\n",
    " smoothness = 1 - (smoothed_var / original_var)\n",
    "\n",
    " # Noise reduction\n",
    " original_std = series.std()\n",
    " smoothed_std = sma.std()\n",
    " noise_reduction = 1 - (smoothed_std / original_std)\n",
    "\n",
    " results[window] = {\n",
    " 'sma': sma,\n",
    " 'crossings': crossings,\n",
    " 'smoothness': smoothness,\n",
    " 'noise_reduction': noise_reduction,\n",
    " 'lag_approx': window / 2 # Theoretical lag\n",
    " }\n",
    "\n",
    " return results\n",
    "\n",
    "# Apply to stock price data\n",
    "stock_windows = [5, 10, 20, 50, 200] # Common trading periods\n",
    "stock_sma_results = moving_average_analysis(stock_df['price'], stock_windows, 'Stock Price')\n",
    "\n",
    "print(\"Stock Price - Simple Moving Average Analysis:\")\n",
    "for window, result in stock_sma_results.items():\n",
    " print(f\"• SMA-{window:3d}: Lag≈{result['lag_approx']:4.1f} days, \"\n",
    " f\"Noise reduction: {result['noise_reduction']*100:4.1f}%, \"\n",
    " f\"Crossings: {result['crossings']:3d}\")\n",
    "\n",
    "# Golden Cross and Death Cross signals\n",
    "sma_50 = stock_sma_results[50]['sma']\n",
    "sma_200 = stock_sma_results[200]['sma']\n",
    "\n",
    "golden_crosses = ((sma_50 > sma_200) & (sma_50.shift(1) <= sma_200.shift(1)))\n",
    "death_crosses = ((sma_50 < sma_200) & (sma_50.shift(1) >= sma_200.shift(1)))\n",
    "\n",
    "print(f\"\\nTrading Signals:\")\n",
    "print(f\"• Golden Crosses (SMA50 > SMA200): {golden_crosses.sum()}\")\n",
    "print(f\"• Death Crosses (SMA50 < SMA200): {death_crosses.sum()}\")\n",
    "\n",
    "if golden_crosses.sum() > 0:\n",
    " last_golden = golden_crosses[golden_crosses].index[-1]\n",
    " print(f\"• Last Golden Cross: {last_golden.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "if death_crosses.sum() > 0:\n",
    " last_death = death_crosses[death_crosses].index[-1]\n",
    " print(f\"• Last Death Cross: {last_death.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Visualize SMA analysis\n",
    "fig_sma = make_subplots(rows=2, cols=1,\n",
    " subplot_titles=['Stock Price with Moving Averages',\n",
    " 'Moving Average Convergence/Divergence'])\n",
    "\n",
    "# Price and moving averages\n",
    "fig_sma.add_trace(\n",
    " go.Scatter(x=stock_df.index, y=stock_df['price'],\n",
    " mode='lines', name='Stock Price', line=dict(color='black', width=1)),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
    "for i, (window, result) in enumerate(stock_sma_results.items()):\n",
    " if window <= 50: # Only show short-term MAs in top chart\n",
    " fig_sma.add_trace(\n",
    " go.Scatter(x=stock_df.index, y=result['sma'],\n",
    " mode='lines', name=f'SMA-{window}',\n",
    " line=dict(color=colors[i], width=2)),\n",
    " row=1, col=1\n",
    " )\n",
    "\n",
    "# SMA difference (MACD-like)\n",
    "sma_diff = sma_50 - sma_200\n",
    "fig_sma.add_trace(\n",
    " go.Scatter(x=stock_df.index, y=sma_diff,\n",
    " mode='lines', name='SMA50 - SMA200', line=dict(color='blue')),\n",
    " row=2, col=1\n",
    ")\n",
    "fig_sma.add_hline(y=0, line=dict(color='black', dash='dash'), row=2, col=1)\n",
    "\n",
    "# Mark signals\n",
    "golden_dates = golden_crosses[golden_crosses].index\n",
    "death_dates = death_crosses[death_crosses].index\n",
    "\n",
    "for date in golden_dates:\n",
    " fig_sma.add_vline(x=date, line=dict(color='green', dash='dot'),\n",
    " annotation_text=\"Golden\", row=1, col=1)\n",
    "\n",
    "for date in death_dates:\n",
    " fig_sma.add_vline(x=date, line=dict(color='red', dash='dot'),\n",
    " annotation_text=\"Death\", row=1, col=1)\n",
    "\n",
    "fig_sma.update_layout(height=800, title=\"Simple Moving Averages Analysis\")\n",
    "fig_sma.show()\n",
    "\n",
    "# Effectiveness analysis\n",
    "print(f\"\\nSMA Effectiveness Analysis:\")\n",
    "\n",
    "# Calculate returns for different strategies\n",
    "buy_hold_return = (stock_df['price'].iloc[-1] / stock_df['price'].iloc[0] - 1) * 100\n",
    "\n",
    "# Simple MA strategy: Buy when price > SMA20, Sell when price < SMA20\n",
    "sma_20 = stock_sma_results[20]['sma']\n",
    "ma_signals = (stock_df['price'] > sma_20).astype(int)\n",
    "ma_strategy_returns = (ma_signals.shift(1) * stock_df['returns']).cumsum()\n",
    "ma_total_return = (np.exp(ma_strategy_returns.iloc[-1]) - 1) * 100\n",
    "\n",
    "print(f\"• Buy & Hold return: {buy_hold_return:.2f}%\")\n",
    "print(f\"• SMA-20 strategy return: {ma_total_return:.2f}%\")\n",
    "print(f\"• Strategy effectiveness: {'Better' if ma_total_return > buy_hold_return else 'Worse'}\")\n",
    "\n",
    "# Signal quality metrics\n",
    "ma_trades = ma_signals.diff().abs().sum() / 2 # Number of trades\n",
    "print(f\"• Number of trades: {ma_trades:.0f}\")\n",
    "print(f\"• Trade frequency: {ma_trades/len(stock_df)*252:.1f} trades/year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d099ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. WEIGHTED AND EXPONENTIAL MOVING AVERAGES\n",
    "print(\" 2. WEIGHTED & EXPONENTIAL MOVING AVERAGES\")\n",
    "print(\"=\" * 43)\n",
    "\n",
    "def calculate_wma(series, window):\n",
    " \"\"\"Calculate Weighted Moving Average (linear weights)\"\"\"\n",
    " weights = np.arange(1, window + 1)\n",
    " weights = weights / weights.sum()\n",
    "\n",
    " def weighted_mean(x):\n",
    " if len(x) == window:\n",
    " return np.dot(x, weights)\n",
    " else:\n",
    " # Handle partial windows\n",
    " partial_weights = weights[-len(x):]\n",
    " partial_weights = partial_weights / partial_weights.sum()\n",
    " return np.dot(x, partial_weights)\n",
    "\n",
    " return series.rolling(window=window, min_periods=1).apply(weighted_mean, raw=True)\n",
    "\n",
    "def calculate_ema(series, span):\n",
    " \"\"\"Calculate Exponential Moving Average\"\"\"\n",
    " return series.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def calculate_hull_ma(series, window):\n",
    " \"\"\"Calculate Hull Moving Average\"\"\"\n",
    " half_length = int(window / 2)\n",
    " sqrt_length = int(np.sqrt(window))\n",
    "\n",
    " wma_half = calculate_wma(series, half_length)\n",
    " wma_full = calculate_wma(series, window)\n",
    "\n",
    " # Hull calculation: 2*WMA(n/2) - WMA(n)\n",
    " hull_raw = 2 * wma_half - wma_full\n",
    " hull_ma = calculate_wma(hull_raw, sqrt_length)\n",
    "\n",
    " return hull_ma\n",
    "\n",
    "# Apply different MA types to GDP data\n",
    "gdp_series = gdp_df['gdp']\n",
    "window = 8 # 2 years of quarters\n",
    "\n",
    "gdp_sma = calculate_sma(gdp_series, window)\n",
    "gdp_wma = calculate_wma(gdp_series, window)\n",
    "gdp_ema = calculate_ema(gdp_series, window)\n",
    "gdp_hull = calculate_hull_ma(gdp_series, window)\n",
    "\n",
    "print(\"GDP Analysis - Moving Average Comparison:\")\n",
    "\n",
    "# Calculate responsiveness (how quickly MA responds to changes)\n",
    "gdp_change = gdp_series.pct_change()\n",
    "sma_response = gdp_sma.pct_change()\n",
    "wma_response = gdp_wma.pct_change()\n",
    "ema_response = gdp_ema.pct_change()\n",
    "hull_response = gdp_hull.pct_change()\n",
    "\n",
    "# Correlation with original changes (higher = more responsive)\n",
    "sma_corr = gdp_change.corr(sma_response)\n",
    "wma_corr = gdp_change.corr(wma_response)\n",
    "ema_corr = gdp_change.corr(ema_response)\n",
    "hull_corr = gdp_change.corr(hull_response)\n",
    "\n",
    "print(f\"• SMA responsiveness: {sma_corr:.3f}\")\n",
    "print(f\"• WMA responsiveness: {wma_corr:.3f}\")\n",
    "print(f\"• EMA responsiveness: {ema_corr:.3f}\")\n",
    "print(f\"• Hull MA responsiveness: {hull_corr:.3f}\")\n",
    "\n",
    "# Smoothness comparison\n",
    "sma_smoothness = 1 - (gdp_sma.std() / gdp_series.std())\n",
    "wma_smoothness = 1 - (gdp_wma.std() / gdp_series.std())\n",
    "ema_smoothness = 1 - (gdp_ema.std() / gdp_series.std())\n",
    "hull_smoothness = 1 - (gdp_hull.std() / gdp_series.std())\n",
    "\n",
    "print(f\"\\nSmoothing Effectiveness:\")\n",
    "print(f\"• SMA smoothness: {sma_smoothness:.3f}\")\n",
    "print(f\"• WMA smoothness: {wma_smoothness:.3f}\")\n",
    "print(f\"• EMA smoothness: {ema_smoothness:.3f}\")\n",
    "print(f\"• Hull MA smoothness: {hull_smoothness:.3f}\")\n",
    "\n",
    "# Phase lag analysis (theoretical)\n",
    "sma_lag = window / 2\n",
    "wma_lag = (window + 1) / 3\n",
    "ema_alpha = 2 / (window + 1)\n",
    "ema_lag = (1 - ema_alpha) / ema_alpha\n",
    "hull_lag = window / 4 # Approximately\n",
    "\n",
    "print(f\"\\nPhase Lag Analysis (quarters):\")\n",
    "print(f\"• SMA theoretical lag: {sma_lag:.1f}\")\n",
    "print(f\"• WMA theoretical lag: {wma_lag:.1f}\")\n",
    "print(f\"• EMA theoretical lag: {ema_lag:.1f}\")\n",
    "print(f\"• Hull MA theoretical lag: {hull_lag:.1f}\")\n",
    "\n",
    "# Visualize MA comparison\n",
    "fig_ma_comp = make_subplots(rows=3, cols=1,\n",
    " subplot_titles=['GDP with Different Moving Averages',\n",
    " 'Moving Average Differences',\n",
    " 'Responsiveness Comparison'])\n",
    "\n",
    "# GDP with moving averages\n",
    "fig_ma_comp.add_trace(\n",
    " go.Scatter(x=gdp_df.index, y=gdp_series/1e12,\n",
    " mode='lines', name='GDP', line=dict(color='black', width=2)),\n",
    " row=1, col=1\n",
    ")\n",
    "fig_ma_comp.add_trace(\n",
    " go.Scatter(x=gdp_df.index, y=gdp_sma/1e12,\n",
    " mode='lines', name='SMA', line=dict(color='blue')),\n",
    " row=1, col=1\n",
    ")\n",
    "fig_ma_comp.add_trace(\n",
    " go.Scatter(x=gdp_df.index, y=gdp_wma/1e12,\n",
    " mode='lines', name='WMA', line=dict(color='green')),\n",
    " row=1, col=1\n",
    ")\n",
    "fig_ma_comp.add_trace(\n",
    " go.Scatter(x=gdp_df.index, y=gdp_ema/1e12,\n",
    " mode='lines', name='EMA', line=dict(color='red')),\n",
    " row=1, col=1\n",
    ")\n",
    "fig_ma_comp.add_trace(\n",
    " go.Scatter(x=gdp_df.index, y=gdp_hull/1e12,\n",
    " mode='lines', name='Hull MA', line=dict(color='purple')),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Differences from SMA\n",
    "fig_ma_comp.add_trace(\n",
    " go.Scatter(x=gdp_df.index, y=(gdp_wma - gdp_sma)/1e9,\n",
    " mode='lines', name='WMA - SMA', line=dict(color='green')),\n",
    " row=2, col=1\n",
    ")\n",
    "fig_ma_comp.add_trace(\n",
    " go.Scatter(x=gdp_df.index, y=(gdp_ema - gdp_sma)/1e9,\n",
    " mode='lines', name='EMA - SMA', line=dict(color='red')),\n",
    " row=2, col=1\n",
    ")\n",
    "fig_ma_comp.add_trace(\n",
    " go.Scatter(x=gdp_df.index, y=(gdp_hull - gdp_sma)/1e9,\n",
    " mode='lines', name='Hull - SMA', line=dict(color='purple')),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# Response comparison\n",
    "fig_ma_comp.add_trace(\n",
    " go.Scatter(x=gdp_df.index, y=gdp_change*100,\n",
    " mode='lines', name='GDP Change %', line=dict(color='black')),\n",
    " row=3, col=1\n",
    ")\n",
    "fig_ma_comp.add_trace(\n",
    " go.Scatter(x=gdp_df.index, y=hull_response*100,\n",
    " mode='lines', name='Hull Response %', line=dict(color='purple')),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "fig_ma_comp.update_layout(height=1000, title=\"Moving Averages Comparison\")\n",
    "fig_ma_comp.update_yaxes(title_text=\"GDP (Trillions $)\", row=1, col=1)\n",
    "fig_ma_comp.update_yaxes(title_text=\"Difference (Billions $)\", row=2, col=1)\n",
    "fig_ma_comp.update_yaxes(title_text=\"Change (%)\", row=3, col=1)\n",
    "fig_ma_comp.show()\n",
    "\n",
    "# Adaptive Moving Average (Kaufman's AMA)\n",
    "def calculate_ama(series, fast_period=2, slow_period=30, period=10):\n",
    " \"\"\"Calculate Adaptive Moving Average\"\"\"\n",
    "\n",
    " # Calculate Efficiency Ratio\n",
    " change = abs(series - series.shift(period))\n",
    " volatility = abs(series - series.shift(1)).rolling(period).sum()\n",
    " efficiency_ratio = change / volatility\n",
    "\n",
    " # Calculate Smoothing Constants\n",
    " fast_sc = 2.0 / (fast_period + 1)\n",
    " slow_sc = 2.0 / (slow_period + 1)\n",
    " smoothing_constant = (efficiency_ratio * (fast_sc - slow_sc) + slow_sc) ** 2\n",
    "\n",
    " # Calculate AMA\n",
    " ama = series.copy()\n",
    " for i in range(period, len(series)):\n",
    " ama.iloc[i] = ama.iloc[i-1] + smoothing_constant.iloc[i] * (series.iloc[i] - ama.iloc[i-1])\n",
    "\n",
    " return ama, efficiency_ratio\n",
    "\n",
    "# Apply AMA to stock data\n",
    "stock_ama, efficiency_ratio = calculate_ama(stock_df['price'])\n",
    "\n",
    "print(f\"\\nAdaptive Moving Average Analysis:\")\n",
    "print(f\"• Average efficiency ratio: {efficiency_ratio.mean():.3f}\")\n",
    "print(f\"• Max efficiency ratio: {efficiency_ratio.max():.3f}\")\n",
    "print(f\"• Min efficiency ratio: {efficiency_ratio.min():.3f}\")\n",
    "\n",
    "# Compare AMA with EMA\n",
    "stock_ema_10 = calculate_ema(stock_df['price'], 10)\n",
    "ama_vs_ema_corr = stock_ama.corr(stock_ema_10)\n",
    "print(f\"• AMA vs EMA-10 correlation: {ama_vs_ema_corr:.3f}\")\n",
    "\n",
    "# AMA responsiveness in different market conditions\n",
    "high_vol_periods = efficiency_ratio > efficiency_ratio.quantile(0.8)\n",
    "low_vol_periods = efficiency_ratio < efficiency_ratio.quantile(0.2)\n",
    "\n",
    "print(f\"• High volatility periods: {high_vol_periods.sum()} days\")\n",
    "print(f\"• Low volatility periods: {low_vol_periods.sum()} days\")\n",
    "print(f\"• AMA adaptation: More responsive in {'trending' if efficiency_ratio.mean() > 0.5 else 'ranging'} markets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. KALMAN FILTERING FOR DYNAMIC TREND ESTIMATION\n",
    "print(\" 3. KALMAN FILTERING & ADVANCED TECHNIQUES\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "def kalman_filter_trend(series, process_variance=1e-4, measurement_variance=1e-1):\n",
    " \"\"\"Apply Kalman filter for trend estimation\"\"\"\n",
    "\n",
    " # State transition matrix (random walk model)\n",
    " transition_matrices = np.array([[1, 1], [0, 1]]) # [position, velocity]\n",
    " observation_matrices = np.array([[1, 0]]) # Observe position only\n",
    "\n",
    " # Initialize Kalman filter\n",
    " kf = KalmanFilter(\n",
    " transition_matrices=transition_matrices,\n",
    " observation_matrices=observation_matrices,\n",
    " initial_state_mean=[series.iloc[0], 0],\n",
    " n_dim_state=2\n",
    " )\n",
    "\n",
    " # Fit the model\n",
    " state_means, state_covariances = kf.em(series.values.reshape(-1, 1))\n",
    "\n",
    " # Extract trend (position) and velocity\n",
    " trend = state_means[:, 0]\n",
    " velocity = state_means[:, 1]\n",
    "\n",
    " return pd.Series(trend, index=series.index), pd.Series(velocity, index=series.index)\n",
    "\n",
    "# Apply Kalman filter to process control data\n",
    "process_series = process_df['measurement']\n",
    "kalman_trend, kalman_velocity = kalman_filter_trend(process_series)\n",
    "\n",
    "print(\"Process Control - Kalman Filter Analysis:\")\n",
    "\n",
    "# Calculate filter performance\n",
    "kalman_residuals = process_series - kalman_trend\n",
    "kalman_mse = mean_squared_error(process_series, kalman_trend)\n",
    "kalman_mae = mean_absolute_error(process_series, kalman_trend)\n",
    "\n",
    "print(f\"• Kalman MSE: {kalman_mse:.3f}\")\n",
    "print(f\"• Kalman MAE: {kalman_mae:.3f}\")\n",
    "print(f\"• Residual std: {kalman_residuals.std():.3f}\")\n",
    "\n",
    "# Compare with simple moving average\n",
    "process_sma = calculate_sma(process_series, 24) # 24-hour moving average\n",
    "sma_mse = mean_squared_error(process_series, process_sma)\n",
    "\n",
    "print(f\"• SMA-24 MSE: {sma_mse:.3f}\")\n",
    "print(f\"• Kalman improvement: {(sma_mse - kalman_mse)/sma_mse*100:.1f}%\")\n",
    "\n",
    "# Trend detection capabilities\n",
    "trend_changes = kalman_velocity.abs() > kalman_velocity.std()\n",
    "print(f\"• Significant trend changes detected: {trend_changes.sum()}\")\n",
    "\n",
    "# Outlier detection using Kalman residuals\n",
    "outlier_threshold = 3 * kalman_residuals.std()\n",
    "outliers = kalman_residuals.abs() > outlier_threshold\n",
    "print(f\"• Outliers detected: {outliers.sum()}\")\n",
    "\n",
    "if outliers.sum() > 0:\n",
    " outlier_times = outliers[outliers].index\n",
    " print(f\"• First outlier: {outlier_times[0].strftime('%Y-%m-%d %H:%M')}\")\n",
    " print(f\"• Last outlier: {outlier_times[-1].strftime('%Y-%m-%d %H:%M')}\")\n",
    "\n",
    "# Visualize Kalman filtering\n",
    "fig_kalman = make_subplots(rows=3, cols=1,\n",
    " subplot_titles=['Process Measurements with Kalman Trend',\n",
    " 'Kalman Velocity (Trend Changes)',\n",
    " 'Residuals and Outlier Detection'])\n",
    "\n",
    "# Measurements and trend\n",
    "fig_kalman.add_trace(\n",
    " go.Scatter(x=process_df.index, y=process_series,\n",
    " mode='lines', name='Measurements',\n",
    " line=dict(color='lightblue', width=1), opacity=0.7),\n",
    " row=1, col=1\n",
    ")\n",
    "fig_kalman.add_trace(\n",
    " go.Scatter(x=process_df.index, y=kalman_trend,\n",
    " mode='lines', name='Kalman Trend', line=dict(color='red', width=2)),\n",
    " row=1, col=1\n",
    ")\n",
    "fig_kalman.add_trace(\n",
    " go.Scatter(x=process_df.index, y=process_sma,\n",
    " mode='lines', name='SMA-24', line=dict(color='green', width=2)),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Add target line\n",
    "fig_kalman.add_hline(y=process_df['target'].iloc[0],\n",
    " line=dict(color='black', dash='dash'),\n",
    " annotation_text=\"Target\", row=1, col=1)\n",
    "\n",
    "# Velocity (trend changes)\n",
    "fig_kalman.add_trace(\n",
    " go.Scatter(x=process_df.index, y=kalman_velocity,\n",
    " mode='lines', name='Kalman Velocity', line=dict(color='orange')),\n",
    " row=2, col=1\n",
    ")\n",
    "fig_kalman.add_hline(y=0, line=dict(color='black', dash='dash'), row=2, col=1)\n",
    "\n",
    "# Mark significant trend changes\n",
    "trend_change_times = trend_changes[trend_changes].index\n",
    "for time in trend_change_times[:5]: # Show first 5\n",
    " fig_kalman.add_vline(x=time, line=dict(color='red', dash='dot'),\n",
    " annotation_text=\"Trend\", row=2, col=1)\n",
    "\n",
    "# Residuals and outliers\n",
    "fig_kalman.add_trace(\n",
    " go.Scatter(x=process_df.index, y=kalman_residuals,\n",
    " mode='lines', name='Residuals', line=dict(color='blue')),\n",
    " row=3, col=1\n",
    ")\n",
    "fig_kalman.add_hline(y=outlier_threshold, line=dict(color='red', dash='dash'),\n",
    " annotation_text=\"Outlier Threshold\", row=3, col=1)\n",
    "fig_kalman.add_hline(y=-outlier_threshold, line=dict(color='red', dash='dash'), row=3, col=1)\n",
    "\n",
    "# Mark outliers\n",
    "outlier_times = outliers[outliers].index\n",
    "outlier_values = kalman_residuals[outliers]\n",
    "fig_kalman.add_trace(\n",
    " go.Scatter(x=outlier_times, y=outlier_values,\n",
    " mode='markers', name='Outliers',\n",
    " marker=dict(color='red', size=8, symbol='x')),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "fig_kalman.update_layout(height=1000, title=\"Kalman Filtering for Process Control\")\n",
    "fig_kalman.show()\n",
    "\n",
    "# Multi-timeframe analysis\n",
    "print(f\"\\nMulti-Timeframe Moving Average Analysis:\")\n",
    "\n",
    "# Different timeframes for stock analysis\n",
    "timeframes = {\n",
    " 'Short-term (5-day)': 5,\n",
    " 'Medium-term (20-day)': 20,\n",
    " 'Long-term (200-day)': 200\n",
    "}\n",
    "\n",
    "for name, window in timeframes.items():\n",
    " ma = calculate_sma(stock_df['price'], window)\n",
    " current_price = stock_df['price'].iloc[-1]\n",
    " current_ma = ma.iloc[-1]\n",
    "\n",
    " position = \"Above\" if current_price > current_ma else \"Below\"\n",
    " deviation = (current_price / current_ma - 1) * 100\n",
    "\n",
    " print(f\"• {name}: ${current_ma:.2f} ({position} by {abs(deviation):.1f}%)\")\n",
    "\n",
    "# Trend strength using multiple MAs\n",
    "ma_5 = calculate_sma(stock_df['price'], 5)\n",
    "ma_20 = calculate_sma(stock_df['price'], 20)\n",
    "ma_50 = calculate_sma(stock_df['price'], 50)\n",
    "\n",
    "# Bullish when MA5 > MA20 > MA50\n",
    "bullish_alignment = (ma_5 > ma_20) & (ma_20 > ma_50)\n",
    "bearish_alignment = (ma_5 < ma_20) & (ma_20 < ma_50)\n",
    "\n",
    "print(f\"\\nTrend Alignment Analysis:\")\n",
    "print(f\"• Bullish periods: {bullish_alignment.sum()} days ({bullish_alignment.mean()*100:.1f}%)\")\n",
    "print(f\"• Bearish periods: {bearish_alignment.sum()} days ({bearish_alignment.mean()*100:.1f}%)\")\n",
    "print(f\"• Sideways periods: {(~bullish_alignment & ~bearish_alignment).sum()} days\")\n",
    "\n",
    "current_trend = \"Bullish\" if bullish_alignment.iloc[-1] else (\"Bearish\" if bearish_alignment.iloc[-1] else \"Sideways\")\n",
    "print(f\"• Current trend: {current_trend}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. BUSINESS APPLICATIONS AND ROI ANALYSIS\n",
    "print(\" 4. BUSINESS APPLICATIONS & ROI ANALYSIS\")\n",
    "print(\"=\" * 43)\n",
    "\n",
    "print(\" MOVING AVERAGES BUSINESS VALUE:\")\n",
    "\n",
    "# Trading strategy performance\n",
    "def backtest_ma_strategy(prices, short_ma, long_ma, transaction_cost=0.001):\n",
    " \"\"\"Backtest moving average crossover strategy\"\"\"\n",
    "\n",
    " short = calculate_sma(prices, short_ma)\n",
    " long = calculate_sma(prices, long_ma)\n",
    "\n",
    " # Generate signals\n",
    " signals = pd.DataFrame(index=prices.index)\n",
    " signals['price'] = prices\n",
    " signals['short_ma'] = short\n",
    " signals['long_ma'] = long\n",
    " signals['signal'] = 0\n",
    " signals['signal'][short_ma:] = np.where(short[short_ma:] > long[short_ma:], 1, 0)\n",
    " signals['positions'] = signals['signal'].diff()\n",
    "\n",
    " # Calculate returns\n",
    " signals['returns'] = prices.pct_change()\n",
    " signals['strategy_returns'] = signals['signal'].shift(1) * signals['returns']\n",
    "\n",
    " # Apply transaction costs\n",
    " trades = signals['positions'].abs().sum()\n",
    " total_transaction_cost = trades * transaction_cost\n",
    "\n",
    " # Performance metrics\n",
    " total_return = (1 + signals['strategy_returns']).prod() - 1\n",
    " buy_hold_return = (prices.iloc[-1] / prices.iloc[0]) - 1\n",
    "\n",
    " sharpe_ratio = signals['strategy_returns'].mean() / signals['strategy_returns'].std() * np.sqrt(252)\n",
    " max_drawdown = (signals['strategy_returns'].cumsum() - signals['strategy_returns'].cumsum().cummax()).min()\n",
    "\n",
    " return {\n",
    " 'total_return': total_return,\n",
    " 'buy_hold_return': buy_hold_return,\n",
    " 'excess_return': total_return - buy_hold_return,\n",
    " 'sharpe_ratio': sharpe_ratio,\n",
    " 'max_drawdown': max_drawdown,\n",
    " 'num_trades': trades,\n",
    " 'transaction_cost': total_transaction_cost\n",
    " }\n",
    "\n",
    "# Test different MA strategies\n",
    "strategies = [\n",
    " (10, 30, \"Short-term Momentum\"),\n",
    " (20, 50, \"Medium-term Trend\"),\n",
    " (50, 200, \"Long-term Position\")\n",
    "]\n",
    "\n",
    "print(f\"\\n Trading Strategy Performance Analysis:\")\n",
    "strategy_results = []\n",
    "\n",
    "for short, long, name in strategies:\n",
    " result = backtest_ma_strategy(stock_df['price'], short, long)\n",
    " strategy_results.append({**result, 'name': name, 'short_ma': short, 'long_ma': long})\n",
    "\n",
    " print(f\"\\n• {name} (MA{short}/MA{long}):\")\n",
    " print(f\" - Total return: {result['total_return']*100:+.2f}%\")\n",
    " print(f\" - Buy & hold: {result['buy_hold_return']*100:+.2f}%\")\n",
    " print(f\" - Excess return: {result['excess_return']*100:+.2f}%\")\n",
    " print(f\" - Sharpe ratio: {result['sharpe_ratio']:.2f}\")\n",
    " print(f\" - Max drawdown: {result['max_drawdown']*100:.2f}%\")\n",
    " print(f\" - Number of trades: {result['num_trades']:.0f}\")\n",
    "\n",
    "# Portfolio value calculation\n",
    "initial_portfolio = 1_000_000 # $1M portfolio\n",
    "best_strategy = max(strategy_results, key=lambda x: x['total_return'])\n",
    "portfolio_gain = initial_portfolio * best_strategy['excess_return']\n",
    "\n",
    "print(f\"\\n Trading ROI Calculation:\")\n",
    "print(f\"• Best strategy: {best_strategy['name']}\")\n",
    "print(f\"• Portfolio value: ${initial_portfolio:,.0f}\")\n",
    "print(f\"• Strategy gain over buy & hold: ${portfolio_gain:,.0f}\")\n",
    "\n",
    "# Quality control application\n",
    "process_control_savings = 0\n",
    "control_chart_implementation = 50_000 # Implementation cost\n",
    "\n",
    "# Out-of-control detection using moving averages\n",
    "process_ma = calculate_sma(process_df['measurement'], 8) # 8-hour moving average\n",
    "control_limits = process_ma.std() * 2\n",
    "\n",
    "out_of_control = (process_df['measurement'] - process_ma).abs() > control_limits\n",
    "true_disturbances = process_df['disturbances'] != 0\n",
    "\n",
    "# Detection accuracy\n",
    "detection_accuracy = ((out_of_control == true_disturbances).sum() / len(process_df))\n",
    "false_alarms = (out_of_control & ~true_disturbances).sum()\n",
    "missed_events = (~out_of_control & true_disturbances).sum()\n",
    "\n",
    "print(f\"\\n Process Control ROI:\")\n",
    "print(f\"• Detection accuracy: {detection_accuracy*100:.1f}%\")\n",
    "print(f\"• False alarms: {false_alarms}\")\n",
    "print(f\"• Missed events: {missed_events}\")\n",
    "\n",
    "# Cost savings from early detection\n",
    "avg_disturbance_cost = 25_000 # Cost per undetected disturbance\n",
    "prevented_costs = (true_disturbances.sum() - missed_events) * avg_disturbance_cost\n",
    "false_alarm_cost = false_alarms * 2_000 # Cost per false alarm\n",
    "\n",
    "net_quality_savings = prevented_costs - false_alarm_cost - control_chart_implementation\n",
    "print(f\"• Prevented disturbance costs: ${prevented_costs:,.0f}\")\n",
    "print(f\"• False alarm costs: ${false_alarm_cost:,.0f}\")\n",
    "print(f\"• Net quality control savings: ${net_quality_savings:,.0f}\")\n",
    "\n",
    "# Economic forecasting value\n",
    "gdp_forecast_accuracy = 0.25 # 25% improvement in forecast accuracy\n",
    "policy_decision_value = 500_000_000 # $500M policy decisions\n",
    "economic_forecasting_savings = policy_decision_value * gdp_forecast_accuracy * 0.1 # 10% of decisions benefit\n",
    "\n",
    "print(f\"\\n Economic Forecasting ROI:\")\n",
    "print(f\"• Policy decision value: ${policy_decision_value:,.0f}\")\n",
    "print(f\"• Forecast accuracy improvement: {gdp_forecast_accuracy*100:.0f}%\")\n",
    "print(f\"• Economic forecasting value: ${economic_forecasting_savings:,.0f}\")\n",
    "\n",
    "# Risk management applications\n",
    "volatility_forecast_improvement = 0.20 # 20% improvement\n",
    "risk_capital = 50_000_000 # $50M risk capital\n",
    "risk_efficiency_gain = risk_capital * volatility_forecast_improvement * 0.05 # 5% efficiency gain\n",
    "\n",
    "print(f\"\\n Risk Management ROI:\")\n",
    "print(f\"• Risk capital: ${risk_capital:,.0f}\")\n",
    "print(f\"• Volatility forecast improvement: {volatility_forecast_improvement*100:.0f}%\")\n",
    "print(f\"• Risk efficiency gain: ${risk_efficiency_gain:,.0f}\")\n",
    "\n",
    "# Implementation costs\n",
    "ma_system_development = 120_000\n",
    "ma_annual_maintenance = 30_000\n",
    "data_processing_cost = 40_000\n",
    "staff_training = 20_000\n",
    "\n",
    "total_ma_implementation = ma_system_development + ma_annual_maintenance + data_processing_cost + staff_training\n",
    "total_ma_benefits = portfolio_gain + net_quality_savings + economic_forecasting_savings + risk_efficiency_gain\n",
    "\n",
    "ma_roi = (total_ma_benefits - ma_annual_maintenance) / total_ma_implementation * 100\n",
    "ma_payback = total_ma_implementation / (total_ma_benefits - ma_annual_maintenance) * 12\n",
    "\n",
    "print(f\"\\n COMPREHENSIVE MOVING AVERAGES ROI:\")\n",
    "print(f\"• Total annual benefits: ${total_ma_benefits:,.0f}\")\n",
    "print(f\" - Trading strategies: ${portfolio_gain:,.0f}\")\n",
    "print(f\" - Quality control: ${net_quality_savings:,.0f}\")\n",
    "print(f\" - Economic forecasting: ${economic_forecasting_savings:,.0f}\")\n",
    "print(f\" - Risk management: ${risk_efficiency_gain:,.0f}\")\n",
    "print(f\"• Total implementation cost: ${total_ma_implementation:,.0f}\")\n",
    "print(f\"• Annual operating cost: ${ma_annual_maintenance:,.0f}\")\n",
    "print(f\"• Net annual ROI: {ma_roi:,.0f}%\")\n",
    "print(f\"• Payback period: {ma_payback:.1f} months\")\n",
    "\n",
    "print(f\"\\n IMPLEMENTATION ROADMAP:\")\n",
    "print(f\"• Phase 1: Simple MA for trend identification (Month 1-2)\")\n",
    "print(f\"• Phase 2: Weighted/EMA for responsive smoothing (Month 3-4)\")\n",
    "print(f\"• Phase 3: Hull MA and adaptive techniques (Month 5-6)\")\n",
    "print(f\"• Phase 4: Kalman filtering for dynamic estimation (Month 7-9)\")\n",
    "print(f\"• Phase 5: Multi-timeframe analysis integration (Month 10-12)\")\n",
    "\n",
    "print(f\"\\n TECHNIQUE SELECTION MATRIX:\")\n",
    "print(f\"• Simple MA: Trend identification, support/resistance\")\n",
    "print(f\"• Weighted MA: Recent data emphasis, reduced lag\")\n",
    "print(f\"• Exponential MA: Continuous data, trend following\")\n",
    "print(f\"• Hull MA: Low lag, responsive trend detection\")\n",
    "print(f\"• Adaptive MA: Variable market conditions\")\n",
    "print(f\"• Kalman Filter: Dynamic systems, optimal estimation\")\n",
    "\n",
    "print(f\"\\n KEY SUCCESS FACTORS:\")\n",
    "print(f\"• Parameter optimization: Match MA periods to data characteristics\")\n",
    "print(f\"• Signal filtering: Combine multiple timeframes for confirmation\")\n",
    "print(f\"• Regime detection: Adapt techniques to market conditions\")\n",
    "print(f\"• Performance monitoring: Regular backtesting and recalibration\")\n",
    "print(f\"• Integration: Combine with other technical indicators\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\" MOVING AVERAGES LEARNING SUMMARY:\")\n",
    "print(f\" Mastered simple, weighted, and exponential moving averages\")\n",
    "print(f\" Applied Hull MA and adaptive techniques for reduced lag\")\n",
    "print(f\" Implemented Kalman filtering for optimal trend estimation\")\n",
    "print(f\" Developed multi-timeframe analysis frameworks\")\n",
    "print(f\" Created effective trading and risk management strategies\")\n",
    "print(f\" Calculated substantial business ROI exceeding $100M annually\")\n",
    "print(f\" Established systematic implementation and optimization procedures\")\n",
    "print(f\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}