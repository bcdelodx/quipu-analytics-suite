{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 3: Time Series Analysis & Forecasting\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** d285be7b-652a-451c-8633-4a3473e9272a\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 3: Time Series Analysis & Forecasting,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** d285be7b-652a-451c-8633-4a3473e9272a\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 3: Time Series Analysis & Forecasting\")\n",
    "print(\"=\" * 47)\n",
    "print(\" CROSS-REFERENCES:\")\n",
    "print(\"• Prerequisites: Tier1_Descriptive.ipynb, Tier1_Pivot.ipynb\")\n",
    "print(\"• Builds On: Tier3_TimeSeriesDecomposition.ipynb, Tier3_ARIMA.ipynb\")\n",
    "print(\"• Complements: Tier3_ExponentialSmoothing.ipynb, Tier3_MovingAverages.ipynb\")\n",
    "print(\"• Advanced: Tier3_FourierAnalysis.ipynb, Tier3_WaveletAnalysis.ipynb\")\n",
    "print(\"=\" * 47)\n",
    "print(\"Time Series Techniques:\")\n",
    "print(\"• Seasonal decomposition and trend analysis\")\n",
    "print(\"• Multiple forecasting methods comparison\")\n",
    "print(\"• Stationarity testing and differencing\")\n",
    "print(\"• Autocorrelation and partial autocorrelation analysis\")\n",
    "print(\"• Forecast accuracy evaluation and confidence intervals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdfd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive synthetic time series datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_time_series_datasets():\n",
    " \"\"\"Generate multiple realistic time series for analysis.\"\"\"\n",
    "\n",
    " # Dataset 1: Monthly Sales Data (with trend and seasonality)\n",
    " dates_monthly = pd.date_range('2020-01-01', periods=48, freq='M')\n",
    " n_months = len(dates_monthly)\n",
    "\n",
    " # Base trend component\n",
    " trend = np.linspace(100000, 150000, n_months)\n",
    "\n",
    " # Seasonal component (stronger in Q4, weaker in Q1)\n",
    " seasonal = 10000 * np.sin(2 * np.pi * np.arange(n_months) / 12) + \\\n",
    " 5000 * np.sin(4 * np.pi * np.arange(n_months) / 12)\n",
    "\n",
    " # Add holiday spikes and random noise\n",
    " holiday_spikes = np.zeros(n_months)\n",
    " holiday_months = [11, 23, 35, 47] # December months\n",
    " for month in holiday_months:\n",
    " if month < n_months:\n",
    " holiday_spikes[month] = np.random.normal(15000, 3000)\n",
    "\n",
    " noise = np.random.normal(0, 5000, n_months)\n",
    "\n",
    " sales_data = trend + seasonal + holiday_spikes + noise\n",
    " sales_data = np.maximum(sales_data, 50000) # Minimum sales floor\n",
    "\n",
    " # Dataset 2: Daily Website Traffic (with weekly patterns)\n",
    " dates_daily = pd.date_range('2023-01-01', periods=365, freq='D')\n",
    " n_days = len(dates_daily)\n",
    "\n",
    " # Weekly seasonality (lower on weekends)\n",
    " day_of_week = np.array([d.weekday() for d in dates_daily])\n",
    " weekly_pattern = np.where(day_of_week < 5, 1.0, 0.6) # Weekdays vs weekends\n",
    "\n",
    " # Base traffic with growth trend\n",
    " base_traffic = 10000 + np.linspace(0, 5000, n_days)\n",
    "\n",
    " # Seasonal component (higher in winter months)\n",
    " yearly_seasonal = 2000 * np.sin(2 * np.pi * np.arange(n_days) / 365.25 + np.pi)\n",
    "\n",
    " # Random events and noise\n",
    " random_events = np.random.poisson(0.05, n_days) * np.random.normal(5000, 1000, n_days)\n",
    " daily_noise = np.random.normal(0, 1000, n_days)\n",
    "\n",
    " traffic_data = base_traffic * weekly_pattern + yearly_seasonal + random_events + daily_noise\n",
    " traffic_data = np.maximum(traffic_data, 1000)\n",
    "\n",
    " # Dataset 3: Hourly Temperature Data (with daily and seasonal cycles)\n",
    " dates_hourly = pd.date_range('2023-01-01', periods=24*90, freq='H') # 90 days\n",
    " n_hours = len(dates_hourly)\n",
    "\n",
    " # Daily temperature cycle\n",
    " hour_of_day = np.array([d.hour for d in dates_hourly])\n",
    " daily_cycle = 10 * np.sin(2 * np.pi * (hour_of_day - 6) / 24) # Peak at 2 PM\n",
    "\n",
    " # Seasonal trend (winter to spring)\n",
    " day_of_year = np.array([d.dayofyear for d in dates_hourly])\n",
    " seasonal_trend = 15 * np.sin(2 * np.pi * day_of_year / 365.25 - np.pi/2) + 20\n",
    "\n",
    " # Weather noise\n",
    " weather_noise = np.random.normal(0, 3, n_hours)\n",
    "\n",
    " temperature_data = seasonal_trend + daily_cycle + weather_noise\n",
    "\n",
    " # Create DataFrames\n",
    " sales_df = pd.DataFrame({\n",
    " 'date': dates_monthly,\n",
    " 'sales': sales_data,\n",
    " 'month': dates_monthly.month,\n",
    " 'quarter': dates_monthly.quarter,\n",
    " 'year': dates_monthly.year\n",
    " })\n",
    "\n",
    " traffic_df = pd.DataFrame({\n",
    " 'date': dates_daily,\n",
    " 'traffic': traffic_data,\n",
    " 'day_of_week': day_of_week,\n",
    " 'is_weekend': day_of_week >= 5,\n",
    " 'month': dates_daily.month\n",
    " })\n",
    "\n",
    " temperature_df = pd.DataFrame({\n",
    " 'datetime': dates_hourly,\n",
    " 'temperature': temperature_data,\n",
    " 'hour': hour_of_day,\n",
    " 'day': dates_hourly.day\n",
    " })\n",
    "\n",
    " return sales_df, traffic_df, temperature_df\n",
    "\n",
    "# Generate datasets\n",
    "sales_df, traffic_df, temperature_df = generate_time_series_datasets()\n",
    "\n",
    "print(\" Time Series Datasets Created:\")\n",
    "print(f\"\\n1. Monthly Sales Data:\")\n",
    "print(f\" • Period: {sales_df['date'].min()} to {sales_df['date'].max()}\")\n",
    "print(f\" • Data points: {len(sales_df)}\")\n",
    "print(f\" • Sales range: ${sales_df['sales'].min():,.0f} - ${sales_df['sales'].max():,.0f}\")\n",
    "\n",
    "print(f\"\\n2. Daily Website Traffic:\")\n",
    "print(f\" • Period: {traffic_df['date'].min()} to {traffic_df['date'].max()}\")\n",
    "print(f\" • Data points: {len(traffic_df)}\")\n",
    "print(f\" • Traffic range: {traffic_df['traffic'].min():,.0f} - {traffic_df['traffic'].max():,.0f} visits\")\n",
    "\n",
    "print(f\"\\n3. Hourly Temperature Data:\")\n",
    "print(f\" • Period: {temperature_df['datetime'].min()} to {temperature_df['datetime'].max()}\")\n",
    "print(f\" • Data points: {len(temperature_df)}\")\n",
    "print(f\" • Temperature range: {temperature_df['temperature'].min():.1f}°C - {temperature_df['temperature'].max():.1f}°C\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample Sales Data:\")\n",
    "print(sales_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c7bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TIME SERIES DECOMPOSITION AND ANALYSIS\n",
    "print(\" 1. TIME SERIES DECOMPOSITION AND ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Decompose the monthly sales time series\n",
    "sales_ts = sales_df.set_index('date')['sales']\n",
    "\n",
    "# Perform seasonal decomposition\n",
    "decomposition = seasonal_decompose(sales_ts, model='additive', period=12)\n",
    "\n",
    "print(\"Sales Time Series Decomposition:\")\n",
    "print(f\"• Original series: {len(sales_ts)} monthly observations\")\n",
    "print(f\"• Decomposition model: Additive\")\n",
    "print(f\"• Seasonal period: 12 months\")\n",
    "\n",
    "# Calculate component statistics\n",
    "trend_component = decomposition.trend.dropna()\n",
    "seasonal_component = decomposition.seasonal\n",
    "residual_component = decomposition.resid.dropna()\n",
    "\n",
    "print(f\"\\nComponent Analysis:\")\n",
    "print(f\"• Trend variation: {trend_component.std():.0f} (${trend_component.min():,.0f} to ${trend_component.max():,.0f})\")\n",
    "print(f\"• Seasonal amplitude: ±{seasonal_component.abs().max():.0f}\")\n",
    "print(f\"• Residual std: {residual_component.std():.0f}\")\n",
    "\n",
    "# Stationarity test\n",
    "def test_stationarity(timeseries, title=\"Time Series\"):\n",
    " \"\"\"Perform Augmented Dickey-Fuller test for stationarity.\"\"\"\n",
    " result = adfuller(timeseries.dropna())\n",
    "\n",
    " print(f\"\\n{title} - Stationarity Test (Augmented Dickey-Fuller):\")\n",
    " print(f\"• ADF Statistic: {result[0]:.6f}\")\n",
    " print(f\"• p-value: {result[1]:.6f}\")\n",
    " print(f\"• Critical Values:\")\n",
    " for key, value in result[4].items():\n",
    " print(f\" - {key}: {value:.3f}\")\n",
    "\n",
    " if result[1] <= 0.05:\n",
    " print(\" Series is stationary (reject null hypothesis)\")\n",
    " else:\n",
    " print(\" Series is non-stationary (fail to reject null hypothesis)\")\n",
    "\n",
    " return result[1] <= 0.05\n",
    "\n",
    "# Test stationarity of original and differenced series\n",
    "is_stationary_original = test_stationarity(sales_ts, \"Original Sales Series\")\n",
    "\n",
    "# First difference\n",
    "sales_diff = sales_ts.diff().dropna()\n",
    "is_stationary_diff = test_stationarity(sales_diff, \"First Differenced Series\")\n",
    "\n",
    "# Autocorrelation analysis\n",
    "sales_autocorr = acf(sales_ts.dropna(), nlags=24, fft=False)\n",
    "sales_partial_autocorr = pacf(sales_ts.dropna(), nlags=24)\n",
    "\n",
    "print(f\"\\nAutocorrelation Analysis:\")\n",
    "print(f\"• Lag 1 autocorrelation: {sales_autocorr[1]:.3f}\")\n",
    "print(f\"• Lag 12 autocorrelation: {sales_autocorr[12]:.3f}\")\n",
    "print(f\"• Significant lags (>0.5): {np.sum(np.abs(sales_autocorr) > 0.5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b08acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MULTIPLE FORECASTING METHODS COMPARISON\n",
    "print(\" 2. MULTIPLE FORECASTING METHODS COMPARISON\")\n",
    "print(\"=\" * 47)\n",
    "\n",
    "# Split data for train/test\n",
    "train_size = int(len(sales_ts) * 0.8)\n",
    "train_data = sales_ts[:train_size]\n",
    "test_data = sales_ts[train_size:]\n",
    "\n",
    "print(f\"Data Split:\")\n",
    "print(f\"• Training period: {train_data.index[0]} to {train_data.index[-1]} ({len(train_data)} months)\")\n",
    "print(f\"• Testing period: {test_data.index[0]} to {test_data.index[-1]} ({len(test_data)} months)\")\n",
    "\n",
    "# Method 1: Simple Moving Average\n",
    "def moving_average_forecast(data, window=3, horizon=None):\n",
    " \"\"\"Simple moving average forecast.\"\"\"\n",
    " if horizon is None:\n",
    " horizon = len(test_data)\n",
    "\n",
    " forecasts = []\n",
    " last_values = data.tail(window).values\n",
    "\n",
    " for _ in range(horizon):\n",
    " forecast = np.mean(last_values)\n",
    " forecasts.append(forecast)\n",
    " # Update rolling window\n",
    " last_values = np.append(last_values[1:], forecast)\n",
    "\n",
    " return np.array(forecasts)\n",
    "\n",
    "# Method 2: Exponential Smoothing (Holt-Winters)\n",
    "exp_smoothing_model = ExponentialSmoothing(\n",
    " train_data,\n",
    " trend='add',\n",
    " seasonal='add',\n",
    " seasonal_periods=12\n",
    ").fit()\n",
    "\n",
    "exp_smoothing_forecast = exp_smoothing_model.forecast(len(test_data))\n",
    "\n",
    "# Method 3: ARIMA Model\n",
    "# Find optimal ARIMA parameters (simplified grid search)\n",
    "def find_best_arima(data, max_p=3, max_d=2, max_q=3):\n",
    " \"\"\"Find best ARIMA parameters using AIC.\"\"\"\n",
    " best_aic = np.inf\n",
    " best_params = None\n",
    "\n",
    " for p in range(max_p + 1):\n",
    " for d in range(max_d + 1):\n",
    " for q in range(max_q + 1):\n",
    " try:\n",
    " model = ARIMA(data, order=(p, d, q))\n",
    " fitted_model = model.fit()\n",
    " if fitted_model.aic < best_aic:\n",
    " best_aic = fitted_model.aic\n",
    " best_params = (p, d, q)\n",
    " except:\n",
    " continue\n",
    "\n",
    " return best_params, best_aic\n",
    "\n",
    "# Find best ARIMA parameters\n",
    "best_arima_params, best_aic = find_best_arima(train_data)\n",
    "print(f\"\\nBest ARIMA parameters: {best_arima_params} (AIC: {best_aic:.2f})\")\n",
    "\n",
    "# Fit best ARIMA model\n",
    "arima_model = ARIMA(train_data, order=best_arima_params).fit()\n",
    "arima_forecast = arima_model.forecast(len(test_data))\n",
    "\n",
    "# Method 4: Naive forecasts (baseline)\n",
    "naive_forecast = np.full(len(test_data), train_data.iloc[-1])\n",
    "seasonal_naive_forecast = np.tile(train_data.iloc[-12:].values, (len(test_data) // 12 + 1))[:len(test_data)]\n",
    "\n",
    "# Generate forecasts\n",
    "ma_forecast = moving_average_forecast(train_data, window=3)\n",
    "\n",
    "# Compile all forecasts\n",
    "forecasts = {\n",
    " 'Moving Average (3)': ma_forecast,\n",
    " 'Exponential Smoothing': exp_smoothing_forecast.values,\n",
    " 'ARIMA': arima_forecast.values,\n",
    " 'Naive': naive_forecast,\n",
    " 'Seasonal Naive': seasonal_naive_forecast\n",
    "}\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "def calculate_accuracy_metrics(actual, predicted):\n",
    " \"\"\"Calculate comprehensive accuracy metrics.\"\"\"\n",
    " mae = mean_absolute_error(actual, predicted)\n",
    " mse = mean_squared_error(actual, predicted)\n",
    " rmse = np.sqrt(mse)\n",
    " mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "\n",
    " return {\n",
    " 'MAE': mae,\n",
    " 'MSE': mse,\n",
    " 'RMSE': rmse,\n",
    " 'MAPE': mape\n",
    " }\n",
    "\n",
    "# Evaluate all methods\n",
    "accuracy_results = {}\n",
    "for method_name, forecast_values in forecasts.items():\n",
    " metrics = calculate_accuracy_metrics(test_data.values, forecast_values)\n",
    " accuracy_results[method_name] = metrics\n",
    "\n",
    " print(f\"\\n{method_name}:\")\n",
    " print(f\" • MAE: {metrics['MAE']:,.0f}\")\n",
    " print(f\" • RMSE: {metrics['RMSE']:,.0f}\")\n",
    " print(f\" • MAPE: {metrics['MAPE']:.1f}%\")\n",
    "\n",
    "# Find best performing method\n",
    "best_method = min(accuracy_results.keys(), key=lambda x: accuracy_results[x]['MAPE'])\n",
    "best_mape = accuracy_results[best_method]['MAPE']\n",
    "print(f\"\\n Best Performing Method: {best_method} (MAPE: {best_mape:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182bb746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. INTERACTIVE TIME SERIES VISUALIZATIONS\n",
    "print(\" 3. INTERACTIVE TIME SERIES VISUALIZATIONS\")\n",
    "print(\"=\" * 47)\n",
    "\n",
    "# Create comprehensive time series dashboard\n",
    "fig = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=[\n",
    " 'Time Series Decomposition (Sales Data)',\n",
    " 'Autocorrelation and Partial Autocorrelation',\n",
    " 'Forecast Comparison (Multiple Methods)',\n",
    " 'Daily Traffic Patterns',\n",
    " 'Hourly Temperature Cycles',\n",
    " 'Forecast Accuracy Comparison'\n",
    " ],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# 1. Time Series Decomposition\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=sales_ts.index,\n",
    " y=sales_ts.values,\n",
    " mode='lines',\n",
    " name='Original',\n",
    " line=dict(color='blue', width=2)\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=trend_component.index,\n",
    " y=trend_component.values,\n",
    " mode='lines',\n",
    " name='Trend',\n",
    " line=dict(color='red', width=2)\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=seasonal_component.index,\n",
    " y=seasonal_component.values,\n",
    " mode='lines',\n",
    " name='Seasonal',\n",
    " line=dict(color='green', width=2)\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Autocorrelation plots\n",
    "lags = np.arange(len(sales_autocorr))\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=lags,\n",
    " y=sales_autocorr,\n",
    " name='ACF',\n",
    " marker_color='lightblue'\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=lags,\n",
    " y=sales_partial_autocorr,\n",
    " name='PACF',\n",
    " marker_color='lightcoral'\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Forecast Comparison\n",
    "# Plot historical data\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=train_data.index,\n",
    " y=train_data.values,\n",
    " mode='lines',\n",
    " name='Training Data',\n",
    " line=dict(color='black', width=2)\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=test_data.index,\n",
    " y=test_data.values,\n",
    " mode='lines',\n",
    " name='Actual Test',\n",
    " line=dict(color='blue', width=3)\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# Plot forecasts\n",
    "colors = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "for i, (method, forecast_vals) in enumerate(forecasts.items()):\n",
    " fig.add_trace(\n",
    " go.Scatter(\n",
    " x=test_data.index,\n",
    " y=forecast_vals,\n",
    " mode='lines',\n",
    " name=f'{method}',\n",
    " line=dict(color=colors[i % len(colors)], width=2, dash='dash')\n",
    " ),\n",
    " row=2, col=1\n",
    " )\n",
    "\n",
    "# 4. Daily Traffic Patterns\n",
    "# Aggregate by day of week\n",
    "traffic_by_day = traffic_df.groupby('day_of_week')['traffic'].mean()\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=day_names,\n",
    " y=traffic_by_day.values,\n",
    " name='Avg Traffic by Day',\n",
    " marker_color='lightgreen'\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "# 5. Hourly Temperature Cycles\n",
    "# Sample a few days for clarity\n",
    "sample_days = temperature_df[temperature_df['datetime'].dt.day <= 5]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=sample_days['datetime'],\n",
    " y=sample_days['temperature'],\n",
    " mode='lines+markers',\n",
    " name='Temperature',\n",
    " line=dict(color='red', width=2),\n",
    " marker=dict(size=4)\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Forecast Accuracy Comparison\n",
    "methods = list(accuracy_results.keys())\n",
    "mape_values = [accuracy_results[method]['MAPE'] for method in methods]\n",
    "rmse_values = [accuracy_results[method]['RMSE'] for method in methods]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=methods,\n",
    " y=mape_values,\n",
    " name='MAPE (%)',\n",
    " marker_color='lightcoral',\n",
    " yaxis='y6'\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    " height=1200,\n",
    " title=\"Time Series Analysis & Forecasting Dashboard\",\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Lag\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Day of Week\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"DateTime\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"Forecast Method\", row=3, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Sales ($)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Correlation\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Sales ($)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Average Traffic\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Temperature (°C)\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"MAPE (%)\", row=3, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Business insights and applications\n",
    "print(f\"\\n TIME SERIES BUSINESS INSIGHTS:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Sales forecasting insights\n",
    "current_sales = train_data.iloc[-1]\n",
    "best_forecast_next = forecasts[best_method][0]\n",
    "forecast_growth = (best_forecast_next - current_sales) / current_sales\n",
    "\n",
    "print(f\"\\n1. Sales Forecasting Analysis:\")\n",
    "print(f\" • Current monthly sales: ${current_sales:,.0f}\")\n",
    "print(f\" • Next month forecast ({best_method}): ${best_forecast_next:,.0f}\")\n",
    "print(f\" • Projected growth: {forecast_growth*100:+.1f}%\")\n",
    "print(f\" • Seasonal peak month: {sales_df.groupby('month')['sales'].mean().idxmax()}\")\n",
    "print(f\" • Seasonal low month: {sales_df.groupby('month')['sales'].mean().idxmin()}\")\n",
    "\n",
    "# Annual projections\n",
    "annual_forecast = forecasts[best_method]\n",
    "projected_annual_sales = np.sum(annual_forecast) * (12 / len(annual_forecast))\n",
    "current_annual_run_rate = current_sales * 12\n",
    "\n",
    "print(f\" • Current annual run rate: ${current_annual_run_rate:,.0f}\")\n",
    "print(f\" • Projected annual sales: ${projected_annual_sales:,.0f}\")\n",
    "\n",
    "# Traffic patterns insights\n",
    "weekend_traffic = traffic_df[traffic_df['is_weekend']]['traffic'].mean()\n",
    "weekday_traffic = traffic_df[~traffic_df['is_weekend']]['traffic'].mean()\n",
    "traffic_weekend_ratio = weekend_traffic / weekday_traffic\n",
    "\n",
    "print(f\"\\n2. Website Traffic Patterns:\")\n",
    "print(f\" • Average weekday traffic: {weekday_traffic:,.0f} visits\")\n",
    "print(f\" • Average weekend traffic: {weekend_traffic:,.0f} visits\")\n",
    "print(f\" • Weekend/Weekday ratio: {traffic_weekend_ratio:.2f}\")\n",
    "print(f\" • Peak traffic day: {day_names[traffic_by_day.idxmax()]}\")\n",
    "print(f\" • Lowest traffic day: {day_names[traffic_by_day.idxmin()]}\")\n",
    "\n",
    "# ROI of forecasting accuracy\n",
    "forecast_improvement = (accuracy_results['Naive']['MAPE'] - accuracy_results[best_method]['MAPE']) / accuracy_results['Naive']['MAPE']\n",
    "inventory_cost_savings = projected_annual_sales * 0.25 * 0.05 * forecast_improvement # 25% inventory cost, 5% improvement\n",
    "planning_efficiency_gain = 150_000 * forecast_improvement # Annual planning cost improvement\n",
    "\n",
    "print(f\"\\n FORECASTING ROI ANALYSIS:\")\n",
    "print(\"=\" * 27)\n",
    "print(f\"• Forecast accuracy improvement: {forecast_improvement*100:.1f}%\")\n",
    "print(f\"• Inventory cost savings: ${inventory_cost_savings:,.0f}\")\n",
    "print(f\"• Planning efficiency gain: ${planning_efficiency_gain:,.0f}\")\n",
    "print(f\"• Total annual benefits: ${inventory_cost_savings + planning_efficiency_gain:,.0f}\")\n",
    "\n",
    "implementation_cost = 125_000\n",
    "annual_maintenance = 25_000\n",
    "total_benefits = inventory_cost_savings + planning_efficiency_gain\n",
    "net_benefits = total_benefits - annual_maintenance\n",
    "roi = (net_benefits - implementation_cost) / implementation_cost\n",
    "\n",
    "print(f\"• Implementation cost: ${implementation_cost:,.0f}\")\n",
    "print(f\"• Annual maintenance: ${annual_maintenance:,.0f}\")\n",
    "print(f\"• Net annual benefits: ${net_benefits:,.0f}\")\n",
    "print(f\"• ROI: {roi*100:.0f}%\")\n",
    "\n",
    "print(f\"\\n Cross-Reference Learning Path:\")\n",
    "print(f\"• Foundation: Tier3_TimeSeriesDecomposition.ipynb (seasonal analysis)\")\n",
    "print(f\"• Advanced: Tier3_ARIMA.ipynb (autoregressive modeling)\")\n",
    "print(f\"• Comparison: Tier3_ExponentialSmoothing.ipynb (smoothing methods)\")\n",
    "print(f\"• Specialized: Tier3_FourierAnalysis.ipynb (frequency domain)\")\n",
    "print(f\"• Complete Guide: CROSS_REFERENCE_GUIDE.md\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}