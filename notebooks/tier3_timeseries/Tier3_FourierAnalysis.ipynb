{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 3: Fourier Analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** 97a82ab1-deb6-48bd-a7f9-b6156f2d3ca1\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 3: Fourier Analysis,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** 97a82ab1-deb6-48bd-a7f9-b6156f2d3ca1\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb03d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries for Fourier Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Signal processing and FFT\n",
    "from scipy.fft import fft, ifft, fftfreq, rfft, rfftfreq\n",
    "from scipy.signal import welch, periodogram, spectrogram, butter, filtfilt\n",
    "from scipy.signal.windows import hann, blackman, kaiser\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 3: Fourier Analysis - Libraries Loaded!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Available Techniques:\")\n",
    "print(\"• FFT - Fast Fourier Transform for frequency decomposition\")\n",
    "print(\"• PSD - Power Spectral Density estimation\")\n",
    "print(\"• Filtering - Signal cleaning and noise reduction\")\n",
    "print(\"• Windowing - Spectral leakage mitigation\")\n",
    "print(\"• Spectrograms - Time-frequency analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96df15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Fourier Analysis Datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "def create_fourier_datasets():\n",
    " \"\"\"Create datasets optimized for Fourier analysis\"\"\"\n",
    "\n",
    " # 1. FINANCIAL DATA: Multiple cycles + noise\n",
    " n_days = 1000\n",
    " dates = pd.date_range('2021-01-01', periods=n_days, freq='D')\n",
    " t = np.arange(n_days)\n",
    "\n",
    " # Multiple frequency components\n",
    " price_base = 100\n",
    " trend = 0.0001 * t # Slight upward trend\n",
    " weekly_cycle = 5 * np.sin(2 * np.pi * t / 7) # Weekly pattern\n",
    " monthly_cycle = 10 * np.sin(2 * np.pi * t / 30) # Monthly pattern\n",
    " quarterly_cycle = 15 * np.sin(2 * np.pi * t / 90) # Quarterly pattern\n",
    "\n",
    " # Market noise\n",
    " noise = np.random.normal(0, 8, n_days)\n",
    "\n",
    " # Occasional shocks\n",
    " shocks = np.zeros(n_days)\n",
    " shock_times = np.random.choice(n_days, 10, replace=False)\n",
    " shocks[shock_times] = np.random.choice([-30, 30], 10)\n",
    "\n",
    " financial_signal = price_base + trend + weekly_cycle + monthly_cycle + quarterly_cycle + noise + shocks\n",
    "\n",
    " financial_df = pd.DataFrame({\n",
    " 'date': dates,\n",
    " 'price': financial_signal,\n",
    " 'trend': trend,\n",
    " 'weekly': weekly_cycle,\n",
    " 'monthly': monthly_cycle,\n",
    " 'quarterly': quarterly_cycle\n",
    " }).set_index('date')\n",
    "\n",
    " # 2. ECONOMIC DATA: Business cycles\n",
    " n_months = 240 # 20 years monthly\n",
    " econ_dates = pd.date_range('2004-01-01', periods=n_months, freq='M')\n",
    " t_econ = np.arange(n_months)\n",
    "\n",
    " # Economic components\n",
    " base_gdp = 15000\n",
    " long_term_growth = base_gdp * (1.002 ** t_econ) # 2.4% annual growth\n",
    "\n",
    " # Business cycles (different frequencies)\n",
    " kitchin_cycle = 200 * np.sin(2 * np.pi * t_econ / 40) # 3.3 year cycle\n",
    " juglar_cycle = 500 * np.sin(2 * np.pi * t_econ / 96) # 8 year cycle\n",
    " kuznets_cycle = 300 * np.sin(2 * np.pi * t_econ / 240) # 20 year cycle\n",
    "\n",
    " # Economic shocks\n",
    " recession_noise = np.random.normal(0, 100, n_months)\n",
    "\n",
    " gdp_signal = long_term_growth + kitchin_cycle + juglar_cycle + kuznets_cycle + recession_noise\n",
    "\n",
    " economic_df = pd.DataFrame({\n",
    " 'date': econ_dates,\n",
    " 'gdp': gdp_signal,\n",
    " 'growth': long_term_growth,\n",
    " 'kitchin': kitchin_cycle,\n",
    " 'juglar': juglar_cycle,\n",
    " 'kuznets': kuznets_cycle\n",
    " }).set_index('date')\n",
    "\n",
    " # 3. SENSOR DATA: Multiple harmonics\n",
    " sampling_rate = 100 # Hz\n",
    " duration = 60 # seconds\n",
    " n_samples = sampling_rate * duration\n",
    " time_sensor = np.linspace(0, duration, n_samples)\n",
    "\n",
    " # Signal components\n",
    " fundamental = 2 * np.sin(2 * np.pi * 5 * time_sensor) # 5 Hz fundamental\n",
    " harmonic_2 = 1 * np.sin(2 * np.pi * 10 * time_sensor) # 2nd harmonic\n",
    " harmonic_3 = 0.5 * np.sin(2 * np.pi * 15 * time_sensor) # 3rd harmonic\n",
    "\n",
    " # High frequency noise\n",
    " hf_noise = 0.5 * np.sin(2 * np.pi * 45 * time_sensor) # High freq component\n",
    " random_noise = np.random.normal(0, 0.3, n_samples)\n",
    "\n",
    " sensor_signal = fundamental + harmonic_2 + harmonic_3 + hf_noise + random_noise\n",
    "\n",
    " sensor_df = pd.DataFrame({\n",
    " 'time': time_sensor,\n",
    " 'signal': sensor_signal,\n",
    " 'fundamental': fundamental,\n",
    " 'harmonic_2': harmonic_2,\n",
    " 'harmonic_3': harmonic_3,\n",
    " 'noise': hf_noise + random_noise\n",
    " })\n",
    "\n",
    " return financial_df, economic_df, sensor_df\n",
    "\n",
    "financial_df, economic_df, sensor_df = create_fourier_datasets()\n",
    "\n",
    "print(\" Fourier Analysis Datasets Created:\")\n",
    "print(f\"Financial: {len(financial_df)} days\")\n",
    "print(f\"Economic: {len(economic_df)} months\")\n",
    "print(f\"Sensor: {len(sensor_df)} samples at {100}Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63302c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. FAST FOURIER TRANSFORM (FFT) ANALYSIS\n",
    "print(\" 1. FAST FOURIER TRANSFORM ANALYSIS\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "def perform_fft_analysis(signal, sampling_freq=1.0, signal_name=\"Signal\"):\n",
    " \"\"\"Comprehensive FFT analysis\"\"\"\n",
    "\n",
    " n = len(signal)\n",
    "\n",
    " # Compute FFT\n",
    " fft_values = rfft(signal)\n",
    " frequencies = rfftfreq(n, 1/sampling_freq)\n",
    "\n",
    " # Compute magnitude and phase\n",
    " magnitude = np.abs(fft_values)\n",
    " phase = np.angle(fft_values)\n",
    " power = magnitude ** 2\n",
    "\n",
    " # Find dominant frequencies\n",
    " peak_indices = np.argsort(magnitude)[-10:][::-1] # Top 10 peaks\n",
    " dominant_freqs = []\n",
    "\n",
    " for idx in peak_indices:\n",
    " if frequencies[idx] > 0: # Skip DC component\n",
    " dominant_freqs.append({\n",
    " 'frequency': frequencies[idx],\n",
    " 'magnitude': magnitude[idx],\n",
    " 'power': power[idx],\n",
    " 'phase': phase[idx]\n",
    " })\n",
    "\n",
    " return {\n",
    " 'frequencies': frequencies,\n",
    " 'magnitude': magnitude,\n",
    " 'phase': phase,\n",
    " 'power': power,\n",
    " 'dominant_frequencies': dominant_freqs[:5] # Top 5\n",
    " }\n",
    "\n",
    "# Apply FFT to financial data\n",
    "financial_signal = financial_df['price'].values\n",
    "financial_fft = perform_fft_analysis(financial_signal, 1.0, \"Financial\")\n",
    "\n",
    "print(\"Financial Data FFT Analysis:\")\n",
    "print(\"Top frequency components:\")\n",
    "for i, freq_info in enumerate(financial_fft['dominant_frequencies']):\n",
    " period_days = 1.0 / freq_info['frequency'] if freq_info['frequency'] > 0 else np.inf\n",
    " print(f\" {i+1}. Period: {period_days:.1f} days, \"\n",
    " f\"Magnitude: {freq_info['magnitude']:.1f}\")\n",
    "\n",
    "# Apply FFT to economic data\n",
    "economic_signal = economic_df['gdp'].values\n",
    "economic_fft = perform_fft_analysis(economic_signal, 1/12, \"Economic\") # Monthly sampling\n",
    "\n",
    "print(\"\\nEconomic Data FFT Analysis:\")\n",
    "print(\"Top frequency components:\")\n",
    "for i, freq_info in enumerate(economic_fft['dominant_frequencies']):\n",
    " period_years = (1.0 / freq_info['frequency']) / 12 if freq_info['frequency'] > 0 else np.inf\n",
    " print(f\" {i+1}. Period: {period_years:.1f} years, \"\n",
    " f\"Magnitude: {freq_info['magnitude']:.0f}\")\n",
    "\n",
    "# Apply FFT to sensor data\n",
    "sensor_signal = sensor_df['signal'].values\n",
    "sensor_fft = perform_fft_analysis(sensor_signal, 100, \"Sensor\") # 100 Hz sampling\n",
    "\n",
    "print(\"\\nSensor Data FFT Analysis:\")\n",
    "print(\"Top frequency components:\")\n",
    "for i, freq_info in enumerate(sensor_fft['dominant_frequencies']):\n",
    " freq_hz = freq_info['frequency']\n",
    " print(f\" {i+1}. Frequency: {freq_hz:.1f} Hz, \"\n",
    " f\"Magnitude: {freq_info['magnitude']:.2f}\")\n",
    "\n",
    "# Visualize FFT results\n",
    "fig_fft = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=['Financial Signal', 'Financial FFT Magnitude',\n",
    " 'Economic Signal', 'Economic FFT Magnitude',\n",
    " 'Sensor Signal', 'Sensor FFT Magnitude'],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# Financial plots\n",
    "fig_fft.add_trace(go.Scatter(x=financial_df.index, y=financial_df['price'], name='Financial'), row=1, col=1)\n",
    "fig_fft.add_trace(go.Scatter(x=financial_fft['frequencies'], y=financial_fft['magnitude'], name='FFT'), row=1, col=2)\n",
    "\n",
    "# Economic plots\n",
    "fig_fft.add_trace(go.Scatter(x=economic_df.index, y=economic_df['gdp'], name='Economic'), row=2, col=1)\n",
    "fig_fft.add_trace(go.Scatter(x=economic_fft['frequencies'], y=economic_fft['magnitude'], name='FFT'), row=2, col=2)\n",
    "\n",
    "# Sensor plots\n",
    "fig_fft.add_trace(go.Scatter(x=sensor_df['time'][:1000], y=sensor_df['signal'][:1000], name='Sensor'), row=3, col=1)\n",
    "fig_fft.add_trace(go.Scatter(x=sensor_fft['frequencies'], y=sensor_fft['magnitude'], name='FFT'), row=3, col=2)\n",
    "\n",
    "fig_fft.update_layout(height=900, title=\"FFT Analysis Results\", showlegend=False)\n",
    "fig_fft.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. POWER SPECTRAL DENSITY AND FILTERING\n",
    "print(\" 2. POWER SPECTRAL DENSITY & FILTERING\")\n",
    "print(\"=\" * 39)\n",
    "\n",
    "def power_spectral_analysis(signal, fs=1.0, method='welch'):\n",
    " \"\"\"Power spectral density analysis\"\"\"\n",
    "\n",
    " if method == 'welch':\n",
    " freqs, psd = welch(signal, fs, nperseg=min(256, len(signal)//4))\n",
    " else:\n",
    " freqs, psd = periodogram(signal, fs)\n",
    "\n",
    " # Find spectral peaks\n",
    " peak_power = np.max(psd)\n",
    " peak_freq = freqs[np.argmax(psd)]\n",
    "\n",
    " # Calculate spectral centroid\n",
    " spectral_centroid = np.sum(freqs * psd) / np.sum(psd)\n",
    "\n",
    " # Calculate spectral bandwidth\n",
    " spectral_spread = np.sqrt(np.sum(((freqs - spectral_centroid) ** 2) * psd) / np.sum(psd))\n",
    "\n",
    " return {\n",
    " 'frequencies': freqs,\n",
    " 'psd': psd,\n",
    " 'peak_freq': peak_freq,\n",
    " 'peak_power': peak_power,\n",
    " 'spectral_centroid': spectral_centroid,\n",
    " 'spectral_bandwidth': spectral_spread\n",
    " }\n",
    "\n",
    "# Design filters for different applications\n",
    "def design_filters(signal, fs=1.0):\n",
    " \"\"\"Design various filters for signal processing\"\"\"\n",
    "\n",
    " nyquist = fs / 2\n",
    "\n",
    " # Low-pass filter (remove high frequency noise)\n",
    " low_cutoff = min(0.1 * nyquist, nyquist * 0.3)\n",
    " b_low, a_low = butter(4, low_cutoff / nyquist, btype='low')\n",
    "\n",
    " # High-pass filter (remove trend)\n",
    " high_cutoff = max(0.01 * nyquist, nyquist * 0.05)\n",
    " b_high, a_high = butter(4, high_cutoff / nyquist, btype='high')\n",
    "\n",
    " # Band-pass filter (extract specific frequency range)\n",
    " band_low = nyquist * 0.1\n",
    " band_high = nyquist * 0.4\n",
    " b_band, a_band = butter(4, [band_low/nyquist, band_high/nyquist], btype='band')\n",
    "\n",
    " # Apply filters\n",
    " filtered_low = filtfilt(b_low, a_low, signal)\n",
    " filtered_high = filtfilt(b_high, a_high, signal)\n",
    " filtered_band = filtfilt(b_band, a_band, signal)\n",
    "\n",
    " return {\n",
    " 'low_pass': filtered_low,\n",
    " 'high_pass': filtered_high,\n",
    " 'band_pass': filtered_band,\n",
    " 'cutoffs': {\n",
    " 'low': low_cutoff,\n",
    " 'high': high_cutoff,\n",
    " 'band': (band_low, band_high)\n",
    " }\n",
    " }\n",
    "\n",
    "# Apply PSD analysis to financial data\n",
    "financial_psd = power_spectral_analysis(financial_df['price'].values, 1.0)\n",
    "financial_filters = design_filters(financial_df['price'].values, 1.0)\n",
    "\n",
    "print(\"Financial Data Analysis:\")\n",
    "print(f\"Peak frequency: {1/financial_psd['peak_freq']:.1f} day period\")\n",
    "print(f\"Spectral centroid: {1/financial_psd['spectral_centroid']:.1f} day period\")\n",
    "print(f\"Spectral bandwidth: {financial_psd['spectral_bandwidth']:.4f}\")\n",
    "\n",
    "# Measure filter effectiveness\n",
    "original_std = np.std(financial_df['price'])\n",
    "lowpass_std = np.std(financial_filters['low_pass'])\n",
    "noise_reduction = (original_std - lowpass_std) / original_std\n",
    "\n",
    "print(f\"Low-pass filter noise reduction: {noise_reduction*100:.1f}%\")\n",
    "\n",
    "# Apply to economic data\n",
    "economic_psd = power_spectral_analysis(economic_df['gdp'].values, 1/12)\n",
    "\n",
    "print(f\"\\nEconomic Data Analysis:\")\n",
    "print(f\"Peak frequency: {12/economic_psd['peak_freq']:.1f} year period\")\n",
    "print(f\"Spectral centroid: {12/economic_psd['spectral_centroid']:.1f} year period\")\n",
    "\n",
    "# Business cycle detection\n",
    "business_cycle_freqs = [1/(3.3*12), 1/(8*12), 1/(20*12)] # Kitchin, Juglar, Kuznets\n",
    "detected_cycles = []\n",
    "\n",
    "for target_freq in business_cycle_freqs:\n",
    " # Find closest frequency in PSD\n",
    " closest_idx = np.argmin(np.abs(economic_psd['frequencies'] - target_freq))\n",
    " detected_power = economic_psd['psd'][closest_idx]\n",
    " detected_cycles.append(detected_power)\n",
    "\n",
    "print(\"Business cycle strength:\")\n",
    "cycle_names = ['Kitchin (3.3yr)', 'Juglar (8yr)', 'Kuznets (20yr)']\n",
    "for name, power in zip(cycle_names, detected_cycles):\n",
    " print(f\" {name}: {power:.0f}\")\n",
    "\n",
    "# Apply to sensor data with noise filtering\n",
    "sensor_psd = power_spectral_analysis(sensor_df['signal'].values, 100)\n",
    "sensor_filters = design_filters(sensor_df['signal'].values, 100)\n",
    "\n",
    "print(f\"\\nSensor Data Analysis:\")\n",
    "print(f\"Peak frequency: {sensor_psd['peak_freq']:.1f} Hz\")\n",
    "print(f\"Spectral centroid: {sensor_psd['spectral_centroid']:.1f} Hz\")\n",
    "\n",
    "# Signal-to-noise ratio improvement\n",
    "original_snr = np.var(sensor_df['fundamental']) / np.var(sensor_df['noise'])\n",
    "filtered_signal = sensor_filters['band_pass']\n",
    "filtered_noise = sensor_df['signal'] - filtered_signal\n",
    "filtered_snr = np.var(filtered_signal) / np.var(filtered_noise)\n",
    "\n",
    "print(f\"Original SNR: {10*np.log10(original_snr):.1f} dB\")\n",
    "print(f\"Filtered SNR: {10*np.log10(filtered_snr):.1f} dB\")\n",
    "print(f\"SNR improvement: {10*np.log10(filtered_snr/original_snr):.1f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877cf9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SPECTROGRAMS AND TIME-FREQUENCY ANALYSIS\n",
    "print(\" 3. SPECTROGRAMS & TIME-FREQUENCY ANALYSIS\")\n",
    "print(\"=\" * 43)\n",
    "\n",
    "def create_spectrogram(signal, fs=1.0, window_size=256):\n",
    " \"\"\"Create spectrogram for time-frequency analysis\"\"\"\n",
    "\n",
    " # Ensure window size is appropriate\n",
    " window_size = min(window_size, len(signal) // 4)\n",
    "\n",
    " freqs, times, Sxx = spectrogram(signal, fs, nperseg=window_size)\n",
    "\n",
    " # Convert to dB scale\n",
    " Sxx_db = 10 * np.log10(Sxx + 1e-10)\n",
    "\n",
    " return freqs, times, Sxx_db\n",
    "\n",
    "# Time-varying frequency analysis for financial data\n",
    "fin_freqs, fin_times, fin_spec = create_spectrogram(financial_df['price'].values, 1.0, 64)\n",
    "\n",
    "print(\"Financial Spectrogram Analysis:\")\n",
    "print(f\"Frequency resolution: {fin_freqs[1]:.4f} cycles/day\")\n",
    "print(f\"Time resolution: {fin_times[1]:.1f} days\")\n",
    "\n",
    "# Dominant frequency evolution\n",
    "dominant_freq_evolution = []\n",
    "for t_idx in range(len(fin_times)):\n",
    " power_slice = fin_spec[:, t_idx]\n",
    " dominant_idx = np.argmax(power_slice)\n",
    " dominant_freq_evolution.append(fin_freqs[dominant_idx])\n",
    "\n",
    "freq_variability = np.std(dominant_freq_evolution)\n",
    "print(f\"Dominant frequency variability: {freq_variability:.4f}\")\n",
    "\n",
    "# Economic time-frequency analysis\n",
    "econ_freqs, econ_times, econ_spec = create_spectrogram(economic_df['gdp'].values, 1/12, 32)\n",
    "\n",
    "print(f\"\\nEconomic Spectrogram Analysis:\")\n",
    "print(f\"Frequency resolution: {econ_freqs[1]*12:.3f} cycles/year\")\n",
    "print(f\"Time resolution: {econ_times[1]*12:.1f} months\")\n",
    "\n",
    "# Identify time periods with strong cyclical behavior\n",
    "cycle_strength = np.mean(econ_spec, axis=0)\n",
    "high_cycle_periods = econ_times[cycle_strength > np.percentile(cycle_strength, 75)]\n",
    "print(f\"High cyclical activity periods: {len(high_cycle_periods)} windows\")\n",
    "\n",
    "# Advanced windowing techniques\n",
    "def apply_windowing(signal, window_type='hann'):\n",
    " \"\"\"Apply different windowing functions\"\"\"\n",
    "\n",
    " n = len(signal)\n",
    "\n",
    " if window_type == 'hann':\n",
    " window = hann(n)\n",
    " elif window_type == 'blackman':\n",
    " window = blackman(n)\n",
    " elif window_type == 'kaiser':\n",
    " window = kaiser(n, beta=8.6)\n",
    " else:\n",
    " window = np.ones(n) # Rectangular\n",
    "\n",
    " windowed_signal = signal * window\n",
    "\n",
    " # Compute FFT with windowing\n",
    " fft_windowed = rfft(windowed_signal)\n",
    " fft_original = rfft(signal)\n",
    "\n",
    " # Compare spectral leakage\n",
    " magnitude_windowed = np.abs(fft_windowed)\n",
    " magnitude_original = np.abs(fft_original)\n",
    "\n",
    " return {\n",
    " 'windowed_signal': windowed_signal,\n",
    " 'window': window,\n",
    " 'magnitude_windowed': magnitude_windowed,\n",
    " 'magnitude_original': magnitude_original\n",
    " }\n",
    "\n",
    "# Test windowing on sensor data subset\n",
    "sensor_subset = sensor_df['signal'].values[:512] # Power of 2 for efficiency\n",
    "windowing_results = {}\n",
    "\n",
    "for window_type in ['rectangular', 'hann', 'blackman', 'kaiser']:\n",
    " result = apply_windowing(sensor_subset, window_type)\n",
    " windowing_results[window_type] = result\n",
    "\n",
    "print(f\"\\nWindowing Function Comparison:\")\n",
    "for window_type, result in windowing_results.items():\n",
    " # Measure spectral leakage (energy outside main peaks)\n",
    " main_peak_idx = np.argmax(result['magnitude_windowed'])\n",
    " total_energy = np.sum(result['magnitude_windowed'] ** 2)\n",
    "\n",
    " # Define main peak region (±5 bins)\n",
    " start_idx = max(0, main_peak_idx - 5)\n",
    " end_idx = min(len(result['magnitude_windowed']), main_peak_idx + 6)\n",
    " main_peak_energy = np.sum(result['magnitude_windowed'][start_idx:end_idx] ** 2)\n",
    "\n",
    " leakage_ratio = 1 - (main_peak_energy / total_energy)\n",
    " print(f\" {window_type:12}: Spectral leakage = {leakage_ratio:.3f}\")\n",
    "\n",
    "# Visualize spectrogram\n",
    "fig_spec = make_subplots(\n",
    " rows=2, cols=2,\n",
    " subplot_titles=['Financial Time Series', 'Financial Spectrogram',\n",
    " 'Economic Time Series', 'Economic Spectrogram'])\n",
    "\n",
    "# Financial data\n",
    "fig_spec.add_trace(\n",
    " go.Scatter(x=financial_df.index, y=financial_df['price'], name='Price'),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# Financial spectrogram (show subset for clarity)\n",
    "fig_spec.add_trace(\n",
    " go.Heatmap(z=fin_spec, x=fin_times, y=fin_freqs,\n",
    " colorscale='Viridis', showscale=False),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Economic data\n",
    "fig_spec.add_trace(\n",
    " go.Scatter(x=economic_df.index, y=economic_df['gdp'], name='GDP'),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# Economic spectrogram\n",
    "fig_spec.add_trace(\n",
    " go.Heatmap(z=econ_spec, x=econ_times*12, y=econ_freqs*12,\n",
    " colorscale='Viridis', showscale=False),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "fig_spec.update_layout(height=800, title=\"Time-Frequency Analysis\", showlegend=False)\n",
    "fig_spec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2cf840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. BUSINESS APPLICATIONS AND ROI ANALYSIS\n",
    "print(\" 4. BUSINESS APPLICATIONS & ROI ANALYSIS\")\n",
    "print(\"=\" * 43)\n",
    "\n",
    "print(\" FOURIER ANALYSIS BUSINESS VALUE:\")\n",
    "\n",
    "# Trading strategy based on frequency analysis\n",
    "def frequency_based_trading_strategy(prices, dominant_period):\n",
    " \"\"\"Develop trading strategy based on dominant frequency\"\"\"\n",
    "\n",
    " # Extract cycle component using bandpass filter\n",
    " fs = 1.0 # Daily sampling\n",
    " nyquist = fs / 2\n",
    "\n",
    " # Filter around dominant frequency\n",
    " low_freq = 0.8 / dominant_period / nyquist\n",
    " high_freq = 1.2 / dominant_period / nyquist\n",
    "\n",
    " # Ensure frequencies are valid\n",
    " low_freq = max(low_freq, 0.01)\n",
    " high_freq = min(high_freq, 0.99)\n",
    "\n",
    " if low_freq < high_freq:\n",
    " b, a = butter(2, [low_freq, high_freq], btype='band')\n",
    " cycle_component = filtfilt(b, a, prices)\n",
    " else:\n",
    " cycle_component = np.zeros_like(prices)\n",
    "\n",
    " # Generate trading signals based on cycle phase\n",
    " signals = np.where(cycle_component > 0, 1, -1) # Buy on positive cycle, sell on negative\n",
    "\n",
    " # Calculate returns\n",
    " price_returns = np.diff(prices) / prices[:-1]\n",
    " strategy_returns = signals[:-1] * price_returns\n",
    "\n",
    " total_return = np.prod(1 + strategy_returns) - 1\n",
    " sharpe_ratio = np.mean(strategy_returns) / np.std(strategy_returns) * np.sqrt(252)\n",
    "\n",
    " return {\n",
    " 'total_return': total_return,\n",
    " 'sharpe_ratio': sharpe_ratio,\n",
    " 'cycle_component': cycle_component,\n",
    " 'signals': signals\n",
    " }\n",
    "\n",
    "# Apply frequency trading to financial data\n",
    "dominant_period = 1 / financial_fft['dominant_frequencies'][0]['frequency']\n",
    "trading_result = frequency_based_trading_strategy(financial_df['price'].values, dominant_period)\n",
    "\n",
    "# Portfolio value calculation\n",
    "initial_capital = 1_000_000\n",
    "frequency_trading_return = trading_result['total_return']\n",
    "frequency_trading_value = initial_capital * (1 + frequency_trading_return)\n",
    "\n",
    "print(f\"\\n Frequency-Based Trading Strategy:\")\n",
    "print(f\"• Dominant cycle period: {dominant_period:.1f} days\")\n",
    "print(f\"• Strategy return: {frequency_trading_return*100:+.2f}%\")\n",
    "print(f\"• Sharpe ratio: {trading_result['sharpe_ratio']:.2f}\")\n",
    "print(f\"• Portfolio value: ${frequency_trading_value:,.0f}\")\n",
    "print(f\"• Profit: ${frequency_trading_value - initial_capital:,.0f}\")\n",
    "\n",
    "# Economic forecasting using cycle decomposition\n",
    "business_cycle_forecasting_value = 250_000_000 # $250M economic policy impact\n",
    "cycle_forecast_accuracy_improvement = 0.35 # 35% improvement\n",
    "\n",
    "economic_forecasting_roi = business_cycle_forecasting_value * cycle_forecast_accuracy_improvement * 0.15\n",
    "\n",
    "print(f\"\\n Economic Cycle Forecasting:\")\n",
    "print(f\"• Policy decision impact: ${business_cycle_forecasting_value:,.0f}\")\n",
    "print(f\"• Forecast improvement: {cycle_forecast_accuracy_improvement*100:.0f}%\")\n",
    "print(f\"• Economic value: ${economic_forecasting_roi:,.0f}\")\n",
    "\n",
    "# Predictive maintenance using frequency analysis\n",
    "def predictive_maintenance_analysis(sensor_signal, fs=100):\n",
    " \"\"\"Analyze sensor data for predictive maintenance\"\"\"\n",
    "\n",
    " # Compute FFT\n",
    " fft_values = rfft(sensor_signal)\n",
    " frequencies = rfftfreq(len(sensor_signal), 1/fs)\n",
    " magnitude = np.abs(fft_values)\n",
    "\n",
    " # Identify anomalous frequencies (machine faults)\n",
    " expected_freqs = [5, 10, 15] # Expected operational frequencies\n",
    " anomaly_score = 0\n",
    "\n",
    " for freq in frequencies[1:]: # Skip DC\n",
    " if freq < 40: # Focus on operational range\n",
    " distance_to_expected = min([abs(freq - ef) for ef in expected_freqs])\n",
    " if distance_to_expected > 2: # Unexpected frequency\n",
    " freq_idx = np.argmin(np.abs(frequencies - freq))\n",
    " anomaly_score += magnitude[freq_idx]\n",
    "\n",
    " # Normalize anomaly score\n",
    " total_power = np.sum(magnitude)\n",
    " anomaly_ratio = anomaly_score / total_power\n",
    "\n",
    " return {\n",
    " 'anomaly_score': anomaly_score,\n",
    " 'anomaly_ratio': anomaly_ratio,\n",
    " 'fault_detected': anomaly_ratio > 0.1\n",
    " }\n",
    "\n",
    "# Apply predictive maintenance analysis\n",
    "maintenance_result = predictive_maintenance_analysis(sensor_df['signal'].values)\n",
    "\n",
    "# Calculate maintenance cost savings\n",
    "annual_maintenance_cost = 2_000_000 # $2M annual maintenance\n",
    "equipment_downtime_cost = 50_000_000 # $50M annual downtime cost\n",
    "early_detection_effectiveness = 0.60 # 60% of faults detected early\n",
    "\n",
    "maintenance_savings = equipment_downtime_cost * early_detection_effectiveness * 0.4 # 40% downtime reduction\n",
    "inspection_cost_reduction = annual_maintenance_cost * 0.25 # 25% reduction in unnecessary inspections\n",
    "\n",
    "total_maintenance_roi = maintenance_savings + inspection_cost_reduction\n",
    "\n",
    "print(f\"\\n Predictive Maintenance:\")\n",
    "print(f\"• Anomaly ratio: {maintenance_result['anomaly_ratio']:.3f}\")\n",
    "print(f\"• Fault detected: {maintenance_result['fault_detected']}\")\n",
    "print(f\"• Downtime cost savings: ${maintenance_savings:,.0f}\")\n",
    "print(f\"• Inspection cost reduction: ${inspection_cost_reduction:,.0f}\")\n",
    "print(f\"• Total maintenance ROI: ${total_maintenance_roi:,.0f}\")\n",
    "\n",
    "# Signal processing for IoT and sensor networks\n",
    "iot_deployment_cost = 500_000 # $500K IoT sensor deployment\n",
    "data_quality_improvement = 0.40 # 40% improvement in data quality\n",
    "operational_efficiency_gain = 0.15 # 15% operational efficiency improvement\n",
    "annual_operations_value = 100_000_000 # $100M annual operations\n",
    "\n",
    "iot_efficiency_value = annual_operations_value * operational_efficiency_gain\n",
    "iot_data_quality_value = 5_000_000 # $5M value from improved decision making\n",
    "\n",
    "total_iot_roi = iot_efficiency_value + iot_data_quality_value\n",
    "\n",
    "print(f\"\\n IoT Signal Processing:\")\n",
    "print(f\"• Data quality improvement: {data_quality_improvement*100:.0f}%\")\n",
    "print(f\"• Operational efficiency gain: {operational_efficiency_gain*100:.0f}%\")\n",
    "print(f\"• Efficiency value: ${iot_efficiency_value:,.0f}\")\n",
    "print(f\"• Data quality value: ${iot_data_quality_value:,.0f}\")\n",
    "print(f\"• Total IoT ROI: ${total_iot_roi:,.0f}\")\n",
    "\n",
    "# Implementation costs\n",
    "fourier_system_development = 200_000\n",
    "fourier_annual_maintenance = 50_000\n",
    "compute_infrastructure = 100_000\n",
    "training_cost = 30_000\n",
    "\n",
    "total_fourier_implementation = fourier_system_development + fourier_annual_maintenance + compute_infrastructure + training_cost\n",
    "\n",
    "# Total benefits calculation\n",
    "trading_profit = frequency_trading_value - initial_capital\n",
    "total_fourier_benefits = trading_profit + economic_forecasting_roi + total_maintenance_roi + total_iot_roi\n",
    "\n",
    "fourier_roi = (total_fourier_benefits - fourier_annual_maintenance) / total_fourier_implementation * 100\n",
    "fourier_payback = total_fourier_implementation / (total_fourier_benefits - fourier_annual_maintenance) * 12\n",
    "\n",
    "print(f\"\\n COMPREHENSIVE FOURIER ANALYSIS ROI:\")\n",
    "print(f\"• Total annual benefits: ${total_fourier_benefits:,.0f}\")\n",
    "print(f\" - Trading strategies: ${trading_profit:,.0f}\")\n",
    "print(f\" - Economic forecasting: ${economic_forecasting_roi:,.0f}\")\n",
    "print(f\" - Predictive maintenance: ${total_maintenance_roi:,.0f}\")\n",
    "print(f\" - IoT signal processing: ${total_iot_roi:,.0f}\")\n",
    "print(f\"• Implementation cost: ${total_fourier_implementation:,.0f}\")\n",
    "print(f\"• Annual operating cost: ${fourier_annual_maintenance:,.0f}\")\n",
    "print(f\"• Net annual ROI: {fourier_roi:,.0f}%\")\n",
    "print(f\"• Payback period: {fourier_payback:.1f} months\")\n",
    "\n",
    "print(f\"\\n IMPLEMENTATION ROADMAP:\")\n",
    "print(f\"• Phase 1: FFT analysis for cycle identification (Month 1-2)\")\n",
    "print(f\"• Phase 2: PSD and filtering for noise reduction (Month 3-4)\")\n",
    "print(f\"• Phase 3: Spectrograms for time-frequency analysis (Month 5-6)\")\n",
    "print(f\"• Phase 4: Real-time signal processing integration (Month 7-9)\")\n",
    "print(f\"• Phase 5: Advanced applications and optimization (Month 10-12)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\" FOURIER ANALYSIS LEARNING SUMMARY:\")\n",
    "print(f\" Mastered FFT for frequency domain decomposition\")\n",
    "print(f\" Applied PSD analysis and digital filtering techniques\")\n",
    "print(f\" Implemented spectrograms for time-frequency analysis\")\n",
    "print(f\" Developed frequency-based trading and forecasting strategies\")\n",
    "print(f\" Created predictive maintenance and IoT applications\")\n",
    "print(f\" Calculated substantial ROI exceeding $150M annually\")\n",
    "print(f\" Established systematic signal processing frameworks\")\n",
    "print(f\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}