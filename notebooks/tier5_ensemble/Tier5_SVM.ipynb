{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 5: Support Vector Machine Classification\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** e874fd9d-091e-4438-9a00-597b6ec197a2\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 5: Support Vector Machine Classification,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** e874fd9d-091e-4438-9a00-597b6ec197a2\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115fd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, validation_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.datasets import make_classification, make_circles, make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 5: Support Vector Machine Classification - Libraries Loaded!\")\n",
    "print(\"=\"*68)\n",
    "print(\"Support Vector Machine Techniques:\")\n",
    "print(\"• Linear SVM (Support Vector Classifier)\")\n",
    "print(\"• RBF Kernel SVM (Radial Basis Function)\")\n",
    "print(\"• Polynomial Kernel SVM\")\n",
    "print(\"• Sigmoid Kernel SVM\")\n",
    "print(\"• Linear SVC (Large scale linear classification)\")\n",
    "print(\"• Hyperparameter optimization (C, gamma, kernel parameters)\")\n",
    "print(\"• Decision boundary visualization and analysis\")\n",
    "print(\"• High-dimensional sparse data classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8846efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive SVM datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Text classification dataset (high-dimensional sparse)\n",
    "def generate_text_dataset(n_samples=3000):\n",
    " \"\"\"Generate realistic text classification dataset.\"\"\"\n",
    "\n",
    " categories = ['Technology', 'Healthcare', 'Finance', 'Education', 'Sports']\n",
    "\n",
    " # Technology keywords\n",
    " tech_words = ['algorithm', 'software', 'computer', 'data', 'programming', 'AI', 'machine learning',\n",
    " 'cloud', 'database', 'network', 'security', 'innovation', 'digital', 'technology']\n",
    "\n",
    " # Healthcare keywords\n",
    " health_words = ['patient', 'medical', 'treatment', 'diagnosis', 'hospital', 'doctor', 'surgery',\n",
    " 'medicine', 'health', 'disease', 'therapy', 'clinical', 'research', 'pharmaceutical']\n",
    "\n",
    " # Finance keywords\n",
    " finance_words = ['investment', 'bank', 'market', 'stock', 'profit', 'revenue', 'financial',\n",
    " 'economy', 'trading', 'currency', 'portfolio', 'asset', 'risk', 'capital']\n",
    "\n",
    " # Education keywords\n",
    " education_words = ['student', 'school', 'learning', 'education', 'teacher', 'university', 'course',\n",
    " 'study', 'academic', 'research', 'knowledge', 'curriculum', 'degree', 'classroom']\n",
    "\n",
    " # Sports keywords\n",
    " sports_words = ['game', 'player', 'team', 'sport', 'competition', 'championship', 'training',\n",
    " 'coach', 'athlete', 'performance', 'fitness', 'exercise', 'match', 'score']\n",
    "\n",
    " keyword_sets = [tech_words, health_words, finance_words, education_words, sports_words]\n",
    "\n",
    " data = []\n",
    "\n",
    " for i in range(n_samples):\n",
    " # Choose category\n",
    " category_idx = np.random.choice(5, p=[0.25, 0.20, 0.20, 0.20, 0.15])\n",
    " category = categories[category_idx]\n",
    "\n",
    " # Generate text document\n",
    " doc_length = np.random.randint(50, 200) # Document length in words\n",
    "\n",
    " # Primary keywords (from chosen category)\n",
    " primary_keywords = np.random.choice(keyword_sets[category_idx],\n",
    " size=min(8, len(keyword_sets[category_idx])),\n",
    " replace=False).tolist()\n",
    "\n",
    " # Add some noise keywords from other categories\n",
    " noise_keywords = []\n",
    " for other_idx in range(5):\n",
    " if other_idx != category_idx and np.random.random() < 0.3:\n",
    " noise_keywords.extend(np.random.choice(keyword_sets[other_idx],\n",
    " size=np.random.randint(0, 3)).tolist())\n",
    "\n",
    " # Common words\n",
    " common_words = ['the', 'and', 'to', 'of', 'in', 'is', 'for', 'with', 'on', 'as',\n",
    " 'this', 'that', 'can', 'will', 'from', 'by', 'at', 'an', 'are', 'be']\n",
    "\n",
    " # Build document\n",
    " all_keywords = primary_keywords + noise_keywords + common_words\n",
    " document = ' '.join(np.random.choice(all_keywords, size=doc_length, replace=True))\n",
    "\n",
    " # Add some numerical features (document stats)\n",
    " data.append({\n",
    " 'document_id': f'DOC_{i:06d}',\n",
    " 'text': document,\n",
    " 'word_count': doc_length,\n",
    " 'unique_words': len(set(document.split())),\n",
    " 'avg_word_length': np.mean([len(word) for word in document.split()]),\n",
    " 'category': category,\n",
    " 'category_code': category_idx\n",
    " })\n",
    "\n",
    " return pd.DataFrame(data)\n",
    "\n",
    "# 2. Image-like high-dimensional dataset\n",
    "def generate_image_dataset(n_samples=2500):\n",
    " \"\"\"Generate high-dimensional dataset simulating image features.\"\"\"\n",
    "\n",
    " object_types = ['Circle', 'Square', 'Triangle']\n",
    " data = []\n",
    "\n",
    " for i in range(n_samples):\n",
    " # Choose object type\n",
    " object_idx = np.random.choice(3, p=[0.4, 0.35, 0.25])\n",
    " object_type = object_types[object_idx]\n",
    "\n",
    " # Generate features based on object type\n",
    " if object_type == 'Circle':\n",
    " # Circular patterns - higher values in center, lower at edges\n",
    " center_features = np.random.normal(0.8, 0.1, 25) # Center pixels\n",
    " edge_features = np.random.normal(0.2, 0.05, 75) # Edge pixels\n",
    " texture_features = np.random.beta(2, 5, 50) # Smooth texture\n",
    "\n",
    " elif object_type == 'Square':\n",
    " # Square patterns - uniform intensity in regions\n",
    " center_features = np.random.normal(0.6, 0.15, 25)\n",
    " edge_features = np.random.normal(0.7, 0.1, 75) # Sharp edges\n",
    " texture_features = np.random.beta(5, 2, 50) # Angular texture\n",
    "\n",
    " else: # Triangle\n",
    " # Triangular patterns - gradient features\n",
    " center_features = np.random.normal(0.5, 0.2, 25)\n",
    " edge_features = np.random.normal(0.4, 0.15, 75)\n",
    " texture_features = np.random.beta(3, 3, 50) # Mixed texture\n",
    "\n",
    " # Combine all features\n",
    " pixel_features = np.concatenate([center_features, edge_features])\n",
    " all_features = np.concatenate([pixel_features, texture_features])\n",
    "\n",
    " # Add noise\n",
    " noise = np.random.normal(0, 0.05, len(all_features))\n",
    " final_features = np.clip(all_features + noise, 0, 1)\n",
    "\n",
    " # Create feature dictionary\n",
    " feature_dict = {f'feature_{j:03d}': final_features[j] for j in range(len(final_features))}\n",
    " feature_dict.update({\n",
    " 'image_id': f'IMG_{i:06d}',\n",
    " 'brightness': np.mean(final_features),\n",
    " 'contrast': np.std(final_features),\n",
    " 'symmetry': 1 - np.abs(np.mean(final_features[:50]) - np.mean(final_features[50:])),\n",
    " 'object_type': object_type,\n",
    " 'type_code': object_idx\n",
    " })\n",
    "\n",
    " data.append(feature_dict)\n",
    "\n",
    " return pd.DataFrame(data)\n",
    "\n",
    "# 3. Non-linear separable dataset\n",
    "def generate_nonlinear_dataset():\n",
    " \"\"\"Generate non-linearly separable datasets for kernel demonstration.\"\"\"\n",
    "\n",
    " # Circles dataset\n",
    " X_circles, y_circles = make_circles(n_samples=1000, factor=0.3, noise=0.1, random_state=42)\n",
    "\n",
    " # Moons dataset\n",
    " X_moons, y_moons = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
    "\n",
    " # Complex multi-class dataset\n",
    " X_complex, y_complex = make_classification(\n",
    " n_samples=1500, n_features=20, n_informative=15,\n",
    " n_redundant=5, n_classes=4, n_clusters_per_class=2,\n",
    " class_sep=0.8, random_state=42\n",
    " )\n",
    "\n",
    " return (X_circles, y_circles), (X_moons, y_moons), (X_complex, y_complex)\n",
    "\n",
    "# Generate datasets\n",
    "text_df = generate_text_dataset()\n",
    "image_df = generate_image_dataset()\n",
    "(X_circles, y_circles), (X_moons, y_moons), (X_complex, y_complex) = generate_nonlinear_dataset()\n",
    "\n",
    "print(\" Support Vector Machine Datasets Created:\")\n",
    "print(f\"Text classification: {text_df.shape}\")\n",
    "print(f\"Category distribution: {text_df['category'].value_counts().to_dict()}\")\n",
    "print(f\"\\nImage classification: {image_df.shape}\")\n",
    "print(f\"Object type distribution: {image_df['object_type'].value_counts().to_dict()}\")\n",
    "print(f\"\\nNon-linear datasets:\")\n",
    "print(f\"Circles: {X_circles.shape}, Moons: {X_moons.shape}, Complex: {X_complex.shape}\")\n",
    "\n",
    "# Show some text examples\n",
    "print(f\"\\nSample text documents:\")\n",
    "for category in ['Technology', 'Healthcare', 'Finance']:\n",
    " sample = text_df[text_df['category'] == category].iloc[0]\n",
    " text_preview = ' '.join(sample['text'].split()[:15]) + '...'\n",
    " print(f\"• {category}: {text_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. KERNEL COMPARISON AND HYPERPARAMETER OPTIMIZATION\n",
    "print(\" 1. KERNEL COMPARISON AND HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*56)\n",
    "\n",
    "# Text classification with TF-IDF\n",
    "print(\"Text Classification with Different SVM Kernels:\")\n",
    "\n",
    "# Prepare text data\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_text_tfidf = vectorizer.fit_transform(text_df['text']).toarray()\n",
    "y_text = text_df['category_code'].values\n",
    "\n",
    "# Add numerical features\n",
    "numerical_features = text_df[['word_count', 'unique_words', 'avg_word_length']].values\n",
    "scaler_text = StandardScaler()\n",
    "numerical_features_scaled = scaler_text.fit_transform(numerical_features)\n",
    "\n",
    "# Combine TF-IDF and numerical features\n",
    "X_text_combined = np.hstack([X_text_tfidf, numerical_features_scaled])\n",
    "\n",
    "# Split data\n",
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(\n",
    " X_text_combined, y_text, test_size=0.2, random_state=42, stratify=y_text\n",
    ")\n",
    "\n",
    "# Define SVM kernels to compare\n",
    "svm_kernels = {\n",
    " 'Linear': SVC(kernel='linear', random_state=42),\n",
    " 'RBF': SVC(kernel='rbf', random_state=42),\n",
    " 'Polynomial': SVC(kernel='poly', degree=3, random_state=42),\n",
    " 'Sigmoid': SVC(kernel='sigmoid', random_state=42)\n",
    "}\n",
    "\n",
    "# Compare kernels on text data\n",
    "text_results = {}\n",
    "print(\"\\nKernel performance on text classification:\")\n",
    "\n",
    "for kernel_name, svm_model in svm_kernels.items():\n",
    " print(f\"Training {kernel_name} SVM...\")\n",
    "\n",
    " # Train model\n",
    " svm_model.fit(X_text_train, y_text_train)\n",
    "\n",
    " # Evaluate\n",
    " train_score = svm_model.score(X_text_train, y_text_train)\n",
    " test_score = svm_model.score(X_text_test, y_text_test)\n",
    " cv_scores = cross_val_score(svm_model, X_text_train, y_text_train, cv=3)\n",
    "\n",
    " text_results[kernel_name] = {\n",
    " 'train_score': train_score,\n",
    " 'test_score': test_score,\n",
    " 'cv_mean': cv_scores.mean(),\n",
    " 'cv_std': cv_scores.std(),\n",
    " 'model': svm_model\n",
    " }\n",
    "\n",
    " print(f\" Train: {train_score:.3f}, Test: {test_score:.3f}, CV: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "# Hyperparameter optimization for best kernel\n",
    "best_text_kernel = max(text_results.keys(), key=lambda x: text_results[x]['test_score'])\n",
    "print(f\"\\nBest kernel for text: {best_text_kernel}\")\n",
    "print(f\"Optimizing hyperparameters for {best_text_kernel} kernel...\")\n",
    "\n",
    "if best_text_kernel == 'Linear':\n",
    " param_grid = {'C': [0.1, 1, 10, 100]}\n",
    " base_model = SVC(kernel='linear', random_state=42)\n",
    "elif best_text_kernel == 'RBF':\n",
    " param_grid = {'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]}\n",
    " base_model = SVC(kernel='rbf', random_state=42)\n",
    "elif best_text_kernel == 'Polynomial':\n",
    " param_grid = {'C': [0.1, 1, 10], 'degree': [2, 3, 4], 'gamma': ['scale', 'auto']}\n",
    " base_model = SVC(kernel='poly', random_state=42)\n",
    "else: # Sigmoid\n",
    " param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.01, 0.1]}\n",
    " base_model = SVC(kernel='sigmoid', random_state=42)\n",
    "\n",
    "# Grid search\n",
    "grid_search_text = GridSearchCV(base_model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_text.fit(X_text_train, y_text_train)\n",
    "\n",
    "optimized_text_score = grid_search_text.score(X_text_test, y_text_test)\n",
    "print(f\"Best parameters: {grid_search_text.best_params_}\")\n",
    "print(f\"Optimized accuracy: {optimized_text_score:.3f}\")\n",
    "print(f\"Improvement: {optimized_text_score - text_results[best_text_kernel]['test_score']:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555cb567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. HIGH-DIMENSIONAL IMAGE CLASSIFICATION\n",
    "print(\" 2. HIGH-DIMENSIONAL IMAGE CLASSIFICATION\")\n",
    "print(\"=\"*42)\n",
    "\n",
    "# Prepare image data\n",
    "image_feature_cols = [col for col in image_df.columns\n",
    " if col.startswith('feature_') or col in ['brightness', 'contrast', 'symmetry']]\n",
    "X_image = image_df[image_feature_cols].values\n",
    "y_image = image_df['type_code'].values\n",
    "\n",
    "# Scale features\n",
    "scaler_image = StandardScaler()\n",
    "X_image_scaled = scaler_image.fit_transform(X_image)\n",
    "\n",
    "# Split data\n",
    "X_img_train, X_img_test, y_img_train, y_img_test = train_test_split(\n",
    " X_image_scaled, y_image, test_size=0.2, random_state=42, stratify=y_image\n",
    ")\n",
    "\n",
    "print(f\"Image dataset: {X_image_scaled.shape[0]} samples, {X_image_scaled.shape[1]} features\")\n",
    "\n",
    "# Compare SVM performance on high-dimensional data\n",
    "image_results = {}\n",
    "print(\"\\nKernel performance on image classification:\")\n",
    "\n",
    "for kernel_name, svm_model in svm_kernels.items():\n",
    " print(f\"Training {kernel_name} SVM on image data...\")\n",
    "\n",
    " # For high-dimensional data, use smaller subset for complex kernels\n",
    " if kernel_name in ['RBF', 'Polynomial'] and X_img_train.shape[0] > 1000:\n",
    " # Use subset for expensive kernels\n",
    " subset_indices = np.random.choice(len(X_img_train), 1000, replace=False)\n",
    " X_train_subset = X_img_train[subset_indices]\n",
    " y_train_subset = y_img_train[subset_indices]\n",
    " else:\n",
    " X_train_subset = X_img_train\n",
    " y_train_subset = y_img_train\n",
    "\n",
    " # Train model\n",
    " svm_model.fit(X_train_subset, y_train_subset)\n",
    "\n",
    " # Evaluate\n",
    " train_score = svm_model.score(X_train_subset, y_train_subset)\n",
    " test_score = svm_model.score(X_img_test, y_img_test)\n",
    "\n",
    " image_results[kernel_name] = {\n",
    " 'train_score': train_score,\n",
    " 'test_score': test_score,\n",
    " 'model': svm_model,\n",
    " 'training_size': len(X_train_subset)\n",
    " }\n",
    "\n",
    " print(f\" Train: {train_score:.3f}, Test: {test_score:.3f} (trained on {len(X_train_subset)} samples)\")\n",
    "\n",
    "# C parameter analysis for Linear SVM (most suitable for high-dimensional)\n",
    "print(f\"\\nC Parameter Analysis for Linear SVM:\")\n",
    "C_values = [0.01, 0.1, 1, 10, 100, 1000]\n",
    "C_train_scores = []\n",
    "C_test_scores = []\n",
    "\n",
    "for C in C_values:\n",
    " linear_svm_c = SVC(kernel='linear', C=C, random_state=42)\n",
    " linear_svm_c.fit(X_img_train, y_img_train)\n",
    "\n",
    " train_score = linear_svm_c.score(X_img_train, y_img_train)\n",
    " test_score = linear_svm_c.score(X_img_test, y_img_test)\n",
    "\n",
    " C_train_scores.append(train_score)\n",
    " C_test_scores.append(test_score)\n",
    "\n",
    " print(f\"C={C:6.2f}: Train={train_score:.3f}, Test={test_score:.3f}\")\n",
    "\n",
    "best_C_idx = np.argmax(C_test_scores)\n",
    "best_C = C_values[best_C_idx]\n",
    "print(f\"Best C value: {best_C} (Test accuracy: {C_test_scores[best_C_idx]:.3f})\")\n",
    "\n",
    "# Linear SVC for large-scale classification\n",
    "print(f\"\\nLinear SVC (optimized for large datasets):\")\n",
    "linear_svc = LinearSVC(C=best_C, random_state=42, max_iter=2000)\n",
    "linear_svc.fit(X_img_train, y_img_train)\n",
    "\n",
    "svc_train_score = linear_svc.score(X_img_train, y_img_train)\n",
    "svc_test_score = linear_svc.score(X_img_test, y_img_test)\n",
    "\n",
    "print(f\"LinearSVC - Train: {svc_train_score:.3f}, Test: {svc_test_score:.3f}\")\n",
    "print(f\"Number of support vectors (Linear SVM): {image_results['Linear']['model'].n_support_}\")\n",
    "print(f\"Total support vectors: {sum(image_results['Linear']['model'].n_support_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b493ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. NON-LINEAR KERNEL ANALYSIS AND DECISION BOUNDARIES\n",
    "print(\" 3. NON-LINEAR KERNEL ANALYSIS AND DECISION BOUNDARIES\")\n",
    "print(\"=\"*58)\n",
    "\n",
    "# Analyze non-linear datasets\n",
    "nonlinear_datasets = {\n",
    " 'Circles': (X_circles, y_circles),\n",
    " 'Moons': (X_moons, y_moons)\n",
    "}\n",
    "\n",
    "nonlinear_results = {}\n",
    "\n",
    "for dataset_name, (X_data, y_data) in nonlinear_datasets.items():\n",
    " print(f\"\\n{dataset_name} Dataset Analysis:\")\n",
    "\n",
    " # Split data\n",
    " X_train, X_test, y_train, y_test = train_test_split(\n",
    " X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
    " )\n",
    "\n",
    " # Test different kernels\n",
    " kernel_performance = {}\n",
    "\n",
    " for kernel_name in ['Linear', 'RBF', 'Polynomial']:\n",
    " if kernel_name == 'Linear':\n",
    " svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
    " elif kernel_name == 'RBF':\n",
    " svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    " else: # Polynomial\n",
    " svm = SVC(kernel='poly', degree=3, C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    " svm.fit(X_train, y_train)\n",
    " test_score = svm.score(X_test, y_test)\n",
    "\n",
    " kernel_performance[kernel_name] = {\n",
    " 'score': test_score,\n",
    " 'model': svm,\n",
    " 'n_support': sum(svm.n_support_) if hasattr(svm, 'n_support_') else 0\n",
    " }\n",
    "\n",
    " print(f\" {kernel_name}: {test_score:.3f} (Support vectors: {sum(svm.n_support_)})\")\n",
    "\n",
    " nonlinear_results[dataset_name] = kernel_performance\n",
    "\n",
    "# RBF kernel gamma analysis\n",
    "print(f\"\\nRBF Kernel Gamma Analysis on Circles Dataset:\")\n",
    "gamma_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "gamma_scores = []\n",
    "gamma_support_vectors = []\n",
    "\n",
    "X_circles_train, X_circles_test, y_circles_train, y_circles_test = train_test_split(\n",
    " X_circles, y_circles, test_size=0.2, random_state=42, stratify=y_circles\n",
    ")\n",
    "\n",
    "for gamma in gamma_values:\n",
    " rbf_svm = SVC(kernel='rbf', C=1.0, gamma=gamma, random_state=42)\n",
    " rbf_svm.fit(X_circles_train, y_circles_train)\n",
    "\n",
    " test_score = rbf_svm.score(X_circles_test, y_circles_test)\n",
    " n_sv = sum(rbf_svm.n_support_)\n",
    "\n",
    " gamma_scores.append(test_score)\n",
    " gamma_support_vectors.append(n_sv)\n",
    "\n",
    " print(f\"Gamma={gamma:6.3f}: Accuracy={test_score:.3f}, Support Vectors={n_sv}\")\n",
    "\n",
    "best_gamma_idx = np.argmax(gamma_scores)\n",
    "best_gamma = gamma_values[best_gamma_idx]\n",
    "print(f\"Best gamma: {best_gamma} (Accuracy: {gamma_scores[best_gamma_idx]:.3f})\")\n",
    "\n",
    "# Complex multi-class analysis\n",
    "print(f\"\\nComplex Multi-class Dataset (4 classes, 20 features):\")\n",
    "\n",
    "# Scale the complex dataset\n",
    "scaler_complex = StandardScaler()\n",
    "X_complex_scaled = scaler_complex.fit_transform(X_complex)\n",
    "\n",
    "X_comp_train, X_comp_test, y_comp_train, y_comp_test = train_test_split(\n",
    " X_complex_scaled, y_complex, test_size=0.2, random_state=42, stratify=y_complex\n",
    ")\n",
    "\n",
    "# Multi-class SVM comparison\n",
    "multiclass_kernels = ['Linear', 'RBF', 'Polynomial']\n",
    "multiclass_results = {}\n",
    "\n",
    "for kernel_name in multiclass_kernels:\n",
    " if kernel_name == 'Linear':\n",
    " svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
    " elif kernel_name == 'RBF':\n",
    " svm = SVC(kernel='rbf', C=1.0, gamma=best_gamma, random_state=42)\n",
    " else: # Polynomial\n",
    " svm = SVC(kernel='poly', degree=3, C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    " svm.fit(X_comp_train, y_comp_train)\n",
    "\n",
    " train_score = svm.score(X_comp_train, y_comp_train)\n",
    " test_score = svm.score(X_comp_test, y_comp_test)\n",
    "\n",
    " # Get per-class performance\n",
    " y_pred = svm.predict(X_comp_test)\n",
    "\n",
    " multiclass_results[kernel_name] = {\n",
    " 'train_score': train_score,\n",
    " 'test_score': test_score,\n",
    " 'model': svm,\n",
    " 'y_pred': y_pred\n",
    " }\n",
    "\n",
    " print(f\"{kernel_name}: Train={train_score:.3f}, Test={test_score:.3f}\")\n",
    "\n",
    "# Best performing model detailed analysis\n",
    "best_multiclass_kernel = max(multiclass_results.keys(),\n",
    " key=lambda x: multiclass_results[x]['test_score'])\n",
    "best_multiclass_model = multiclass_results[best_multiclass_kernel]['model']\n",
    "\n",
    "print(f\"\\nBest multi-class kernel: {best_multiclass_kernel}\")\n",
    "print(f\"Classification report:\")\n",
    "y_pred_best = multiclass_results[best_multiclass_kernel]['y_pred']\n",
    "print(classification_report(y_comp_test, y_pred_best,\n",
    " target_names=[f'Class_{i}' for i in range(4)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. COMPREHENSIVE SVM VISUALIZATION DASHBOARD\n",
    "print(\" 4. COMPREHENSIVE SVM VISUALIZATION DASHBOARD\")\n",
    "print(\"=\"*49)\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "fig = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=[\n",
    " 'Kernel Performance: Text Classification',\n",
    " 'C Parameter Analysis: High-Dimensional Data',\n",
    " 'Gamma Parameter Analysis: RBF Kernel',\n",
    " 'Decision Boundaries: Non-linear Data',\n",
    " 'Multi-class Performance Comparison',\n",
    " 'Support Vector Analysis'\n",
    " ],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# 1. Text classification kernel comparison\n",
    "text_kernels = list(text_results.keys())\n",
    "text_scores = [text_results[k]['test_score'] for k in text_kernels]\n",
    "text_cv_scores = [text_results[k]['cv_mean'] for k in text_kernels]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=text_kernels,\n",
    " y=text_scores,\n",
    " name='Test Accuracy',\n",
    " marker_color='lightblue',\n",
    " text=[f'{score:.3f}' for score in text_scores],\n",
    " textposition='auto'\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=text_kernels,\n",
    " y=text_cv_scores,\n",
    " name='CV Mean',\n",
    " marker_color='lightcoral',\n",
    " text=[f'{score:.3f}' for score in text_cv_scores],\n",
    " textposition='auto'\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. C parameter analysis\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=C_values,\n",
    " y=C_train_scores,\n",
    " mode='lines+markers',\n",
    " name='Training Score',\n",
    " line=dict(color='blue'),\n",
    " xaxis='log'\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=C_values,\n",
    " y=C_test_scores,\n",
    " mode='lines+markers',\n",
    " name='Test Score',\n",
    " line=dict(color='red'),\n",
    " xaxis='log'\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Gamma analysis\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=gamma_values,\n",
    " y=gamma_scores,\n",
    " mode='lines+markers',\n",
    " name='RBF Accuracy',\n",
    " line=dict(color='green'),\n",
    " yaxis='y',\n",
    " xaxis='log'\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# Secondary y-axis for support vectors\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=gamma_values,\n",
    " y=gamma_support_vectors,\n",
    " mode='lines+markers',\n",
    " name='Support Vectors',\n",
    " line=dict(color='purple', dash='dash'),\n",
    " yaxis='y2'\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Non-linear dataset visualization (circles)\n",
    "# Plot circles dataset with decision boundary\n",
    "colors = ['red', 'blue']\n",
    "for class_idx in [0, 1]:\n",
    " mask = y_circles == class_idx\n",
    " fig.add_trace(\n",
    " go.Scatter(\n",
    " x=X_circles[mask, 0],\n",
    " y=X_circles[mask, 1],\n",
    " mode='markers',\n",
    " name=f'Class {class_idx}',\n",
    " marker=dict(color=colors[class_idx], size=4)\n",
    " ),\n",
    " row=2, col=2\n",
    " )\n",
    "\n",
    "# 5. Multi-class performance\n",
    "multiclass_kernel_names = list(multiclass_results.keys())\n",
    "multiclass_scores = [multiclass_results[k]['test_score'] for k in multiclass_kernel_names]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=multiclass_kernel_names,\n",
    " y=multiclass_scores,\n",
    " name='Multi-class Accuracy',\n",
    " marker_color='orange',\n",
    " text=[f'{score:.3f}' for score in multiclass_scores],\n",
    " textposition='auto'\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Support vector analysis across datasets\n",
    "datasets = ['Text', 'Image', 'Circles', 'Moons']\n",
    "linear_sv = [\n",
    " sum(text_results['Linear']['model'].n_support_),\n",
    " sum(image_results['Linear']['model'].n_support_),\n",
    " nonlinear_results['Circles']['Linear']['n_support'],\n",
    " nonlinear_results['Moons']['Linear']['n_support']\n",
    "]\n",
    "rbf_sv = [\n",
    " sum(text_results['RBF']['model'].n_support_),\n",
    " sum(image_results['RBF']['model'].n_support_),\n",
    " nonlinear_results['Circles']['RBF']['n_support'],\n",
    " nonlinear_results['Moons']['RBF']['n_support']\n",
    "]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=datasets,\n",
    " y=linear_sv,\n",
    " name='Linear SV',\n",
    " marker_color='lightblue'\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=datasets,\n",
    " y=rbf_sv,\n",
    " name='RBF SV',\n",
    " marker_color='lightgreen'\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    " height=1200,\n",
    " title=\"Support Vector Machine Classification - Comprehensive Analysis Dashboard\",\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Kernel Type\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"C Parameter (log scale)\", type=\"log\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Gamma Parameter (log scale)\", type=\"log\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Feature 1\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"Kernel Type\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"Dataset\", row=3, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Feature 2\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Support Vectors\", row=3, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5331723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. BUSINESS INSIGHTS AND ROI ANALYSIS\n",
    "print(\" 5. BUSINESS INSIGHTS AND ROI ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Text classification business impact\n",
    "print(\"Document Classification System ROI:\")\n",
    "monthly_documents = 100_000 # Documents processed per month\n",
    "classification_accuracy = optimized_text_score\n",
    "\n",
    "# Manual processing costs\n",
    "manual_cost_per_doc = 0.50 # $0.50 per document for manual classification\n",
    "manual_monthly_cost = monthly_documents * manual_cost_per_doc\n",
    "\n",
    "# Automated system costs\n",
    "system_monthly_cost = 8_000 # System operational cost\n",
    "manual_review_rate = 1 - classification_accuracy # Documents requiring manual review\n",
    "manual_review_cost = monthly_documents * manual_review_rate * manual_cost_per_doc\n",
    "\n",
    "total_automated_cost = system_monthly_cost + manual_review_cost\n",
    "monthly_savings = manual_monthly_cost - total_automated_cost\n",
    "annual_savings = monthly_savings * 12\n",
    "\n",
    "# Productivity improvements\n",
    "time_saved_per_doc = 2.5 # minutes saved per document\n",
    "correctly_classified = monthly_documents * classification_accuracy\n",
    "total_time_saved = correctly_classified * time_saved_per_doc / 60 # hours\n",
    "hourly_rate = 25 # average hourly rate\n",
    "productivity_value = total_time_saved * hourly_rate\n",
    "\n",
    "print(f\"\\nDocument Classification Impact:\")\n",
    "print(f\"• Monthly documents: {monthly_documents:,}\")\n",
    "print(f\"• Classification accuracy: {classification_accuracy:.1%}\")\n",
    "print(f\"• Manual processing cost: ${manual_monthly_cost:,.0f}/month\")\n",
    "print(f\"• Automated system cost: ${total_automated_cost:,.0f}/month\")\n",
    "print(f\"• Monthly cost savings: ${monthly_savings:,.0f}\")\n",
    "print(f\"• Annual cost savings: ${annual_savings:,.0f}\")\n",
    "print(f\"• Additional productivity value: ${productivity_value:,.0f}/month\")\n",
    "\n",
    "# Image classification ROI\n",
    "print(f\"\\nImage Recognition System ROI:\")\n",
    "daily_images = 50_000 # Images processed per day\n",
    "image_accuracy = max([image_results[k]['test_score'] for k in image_results.keys()])\n",
    "\n",
    "# Quality control improvements\n",
    "defect_detection_rate = image_accuracy\n",
    "defect_rate = 0.02 # 2% defect rate\n",
    "cost_per_defect = 150 # Cost if defect reaches customer\n",
    "\n",
    "monthly_images = daily_images * 30\n",
    "expected_defects = monthly_images * defect_rate\n",
    "defects_caught = expected_defects * defect_detection_rate\n",
    "defects_missed = expected_defects - defects_caught\n",
    "\n",
    "cost_avoidance = defects_caught * cost_per_defect\n",
    "remaining_defect_cost = defects_missed * cost_per_defect\n",
    "\n",
    "# System costs\n",
    "image_system_cost = 15_000 # Monthly system cost\n",
    "net_monthly_benefit = cost_avoidance - image_system_cost\n",
    "image_roi = net_monthly_benefit / image_system_cost\n",
    "\n",
    "print(f\"• Daily images processed: {daily_images:,}\")\n",
    "print(f\"• Detection accuracy: {image_accuracy:.1%}\")\n",
    "print(f\"• Expected monthly defects: {expected_defects:.0f}\")\n",
    "print(f\"• Defects caught: {defects_caught:.0f}\")\n",
    "print(f\"• Cost avoidance: ${cost_avoidance:,.0f}/month\")\n",
    "print(f\"• System cost: ${image_system_cost:,.0f}/month\")\n",
    "print(f\"• Net monthly benefit: ${net_monthly_benefit:,.0f}\")\n",
    "print(f\"• ROI: {image_roi*100:.0f}%\")\n",
    "\n",
    "# High-dimensional analysis ROI\n",
    "print(f\"\\nHigh-Dimensional Analysis ROI:\")\n",
    "research_projects = 50 # Number of research projects per year\n",
    "analysis_time_saved = 40 # Hours saved per project\n",
    "researcher_hourly_rate = 75 # PhD researcher hourly rate\n",
    "\n",
    "annual_time_savings = research_projects * analysis_time_saved\n",
    "annual_cost_savings_research = annual_time_savings * researcher_hourly_rate\n",
    "\n",
    "# Accuracy improvements leading to better research outcomes\n",
    "improved_discovery_rate = 0.15 # 15% improvement in discovery rate\n",
    "avg_discovery_value = 500_000 # Average value of a research discovery\n",
    "additional_discoveries = research_projects * improved_discovery_rate\n",
    "discovery_value = additional_discoveries * avg_discovery_value\n",
    "\n",
    "research_system_cost = 25_000 # Annual system cost\n",
    "total_research_benefit = annual_cost_savings_research + discovery_value\n",
    "research_roi = (total_research_benefit - research_system_cost) / research_system_cost\n",
    "\n",
    "print(f\"• Research projects per year: {research_projects}\")\n",
    "print(f\"• Time savings: {annual_time_savings:,} hours/year\")\n",
    "print(f\"• Cost savings: ${annual_cost_savings_research:,.0f}/year\")\n",
    "print(f\"• Additional discoveries: {additional_discoveries:.1f}/year\")\n",
    "print(f\"• Discovery value: ${discovery_value:,.0f}/year\")\n",
    "print(f\"• Total benefit: ${total_research_benefit:,.0f}/year\")\n",
    "print(f\"• System cost: ${research_system_cost:,.0f}/year\")\n",
    "print(f\"• ROI: {research_roi*100:.0f}%\")\n",
    "\n",
    "# Combined systems summary\n",
    "total_annual_investment = (system_monthly_cost * 12) + (image_system_cost * 12) + research_system_cost\n",
    "total_annual_benefits = annual_savings + (net_monthly_benefit * 12) + (total_research_benefit - research_system_cost)\n",
    "combined_roi = total_annual_benefits / total_annual_investment\n",
    "\n",
    "print(f\"\\nCombined SVM Systems Summary:\")\n",
    "print(f\"• Total annual investment: ${total_annual_investment:,.0f}\")\n",
    "print(f\"• Total annual benefits: ${total_annual_benefits:,.0f}\")\n",
    "print(f\"• Combined ROI: {combined_roi*100:.0f}%\")\n",
    "print(f\"• Payback period: {total_annual_investment/total_annual_benefits*12:.1f} months\")\n",
    "\n",
    "# Implementation guidelines\n",
    "print(f\"\\nSVM Implementation Guidelines:\")\n",
    "print(f\"• Use Linear SVM for high-dimensional sparse data (text, genes)\")\n",
    "print(f\"• Use RBF kernel for non-linear problems with moderate dimensions\")\n",
    "print(f\"• Scale features when using RBF or polynomial kernels\")\n",
    "print(f\"• Start with C=1.0, optimize via cross-validation\")\n",
    "print(f\"• Use LinearSVC for large-scale linear problems\")\n",
    "print(f\"• Consider kernel approximation for very large datasets\")\n",
    "\n",
    "print(f\"\\nCross-Reference Learning Path:\")\n",
    "print(f\"• Foundation: Tier2_SVM.ipynb (basic SVM concepts)\")\n",
    "print(f\"• Building On: Tier2_LinearRegression.ipynb (linear methods)\")\n",
    "print(f\"• Comparison: Tier5_Classification.ipynb (algorithm comparison)\")\n",
    "print(f\"• Advanced: Advanced_KernelMethods.ipynb, Advanced_HighDimensionalClassification.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}