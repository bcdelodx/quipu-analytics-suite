{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 5: Random Forest Classification\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** e4bd9f90-d13a-4de7-be04-925cc2fee1ed\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 5: Random Forest Classification,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** e4bd9f90-d13a-4de7-be04-925cc2fee1ed\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a04376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, validation_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 5: Random Forest Classification - Libraries Loaded!\")\n",
    "print(\"=\"*58)\n",
    "print(\"Random Forest Classification Techniques:\")\n",
    "print(\"• Bootstrap aggregating (bagging) ensemble method\")\n",
    "print(\"• Random feature selection at each split\")\n",
    "print(\"• Out-of-bag (OOB) error estimation\")\n",
    "print(\"• Feature importance and selection\")\n",
    "print(\"• Extremely Randomized Trees (Extra Trees)\")\n",
    "print(\"• Ensemble diversity and bias-variance tradeoff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive Random Forest datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Customer segmentation dataset\n",
    "def generate_customer_dataset(n_samples=4000):\n",
    " \"\"\"Generate realistic customer segmentation dataset.\"\"\"\n",
    "\n",
    " # Base customer features\n",
    " X, y = make_classification(\n",
    " n_samples=n_samples,\n",
    " n_features=15,\n",
    " n_informative=10,\n",
    " n_redundant=3,\n",
    " n_clusters_per_class=2,\n",
    " n_classes=4,\n",
    " class_sep=0.8,\n",
    " random_state=42\n",
    " )\n",
    "\n",
    " # Create realistic business features\n",
    " data = []\n",
    " segments = ['Budget', 'Standard', 'Premium', 'Enterprise']\n",
    "\n",
    " for i in range(n_samples):\n",
    " segment_idx = y[i]\n",
    " segment = segments[segment_idx]\n",
    "\n",
    " # Generate segment-specific features\n",
    " if segment == 'Budget':\n",
    " annual_revenue = np.random.lognormal(7, 0.5) # ~$1K\n",
    " transaction_frequency = np.random.poisson(2)\n",
    " support_tickets = np.random.poisson(3)\n",
    " satisfaction_score = np.random.beta(5, 3) * 10\n",
    " elif segment == 'Standard':\n",
    " annual_revenue = np.random.lognormal(8, 0.4) # ~$3K\n",
    " transaction_frequency = np.random.poisson(5)\n",
    " support_tickets = np.random.poisson(2)\n",
    " satisfaction_score = np.random.beta(6, 2) * 10\n",
    " elif segment == 'Premium':\n",
    " annual_revenue = np.random.lognormal(9, 0.3) # ~$8K\n",
    " transaction_frequency = np.random.poisson(8)\n",
    " support_tickets = np.random.poisson(1)\n",
    " satisfaction_score = np.random.beta(8, 2) * 10\n",
    " else: # Enterprise\n",
    " annual_revenue = np.random.lognormal(10, 0.4) # ~$22K\n",
    " transaction_frequency = np.random.poisson(12)\n",
    " support_tickets = np.random.poisson(1)\n",
    " satisfaction_score = np.random.beta(9, 1) * 10\n",
    "\n",
    " # Additional features\n",
    " data.append({\n",
    " 'customer_id': f'CUST_{i:06d}',\n",
    " 'annual_revenue': annual_revenue,\n",
    " 'transaction_frequency': transaction_frequency,\n",
    " 'support_tickets': support_tickets,\n",
    " 'satisfaction_score': satisfaction_score,\n",
    " 'customer_age_months': np.random.exponential(24),\n",
    " 'product_usage_score': np.random.beta(3, 2) * 100,\n",
    " 'referral_count': np.random.poisson(1),\n",
    " 'contract_length': np.random.choice([6, 12, 24, 36], p=[0.1, 0.4, 0.3, 0.2]),\n",
    " 'payment_method_risk': np.random.beta(2, 5),\n",
    " 'geographic_tier': np.random.choice([1, 2, 3], p=[0.3, 0.5, 0.2]),\n",
    " 'segment': segment,\n",
    " 'segment_code': segment_idx\n",
    " })\n",
    "\n",
    " # Add technical features from make_classification\n",
    " for j in range(X.shape[1]):\n",
    " data[i][f'tech_feature_{j+1:02d}'] = X[i, j]\n",
    "\n",
    " return pd.DataFrame(data)\n",
    "\n",
    "# 2. Gene expression dataset (high-dimensional)\n",
    "def generate_gene_dataset(n_samples=1000, n_genes=200):\n",
    " \"\"\"Generate synthetic gene expression dataset.\"\"\"\n",
    "\n",
    " X, y = make_classification(\n",
    " n_samples=n_samples,\n",
    " n_features=n_genes,\n",
    " n_informative=int(n_genes * 0.1), # 10% informative genes\n",
    " n_redundant=int(n_genes * 0.05), # 5% redundant genes\n",
    " n_clusters_per_class=1,\n",
    " n_classes=3,\n",
    " class_sep=1.2,\n",
    " random_state=42\n",
    " )\n",
    "\n",
    " # Create gene names\n",
    " gene_names = [f'GENE_{i+1:04d}' for i in range(n_genes)]\n",
    "\n",
    " # Create DataFrame\n",
    " gene_df = pd.DataFrame(X, columns=gene_names)\n",
    " gene_df['condition'] = ['Healthy', 'Disease_A', 'Disease_B'][y[i] for i in range(len(y))]\n",
    " gene_df['condition_code'] = y\n",
    "\n",
    " return gene_df, gene_names\n",
    "\n",
    "# Generate datasets\n",
    "customer_df = generate_customer_dataset()\n",
    "gene_df, gene_names = generate_gene_dataset()\n",
    "\n",
    "print(\" Random Forest Datasets Created:\")\n",
    "print(f\"Customer segmentation: {customer_df.shape}\")\n",
    "print(f\"Segment distribution: {customer_df['segment'].value_counts().to_dict()}\")\n",
    "print(f\"\\nGene expression: {gene_df.shape}\")\n",
    "print(f\"Condition distribution: {gene_df['condition'].value_counts().to_dict()}\")\n",
    "print(f\"Average annual revenue by segment:\")\n",
    "for segment in customer_df['segment'].unique():\n",
    " avg_revenue = customer_df[customer_df['segment'] == segment]['annual_revenue'].mean()\n",
    " print(f\" {segment}: ${avg_revenue:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. RANDOM FOREST CLASSIFICATION AND HYPERPARAMETER OPTIMIZATION\n",
    "print(\" 1. RANDOM FOREST CLASSIFICATION AND HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*68)\n",
    "\n",
    "# Prepare customer data\n",
    "customer_features = [col for col in customer_df.columns if col not in ['customer_id', 'segment', 'segment_code']]\n",
    "X_customer = customer_df[customer_features].values\n",
    "y_customer = customer_df['segment_code'].values\n",
    "\n",
    "# Split the data\n",
    "X_cust_train, X_cust_test, y_cust_train, y_cust_test = train_test_split(\n",
    " X_customer, y_customer, test_size=0.2, random_state=42, stratify=y_customer\n",
    ")\n",
    "\n",
    "# Train baseline Random Forest\n",
    "rf_baseline = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_baseline.fit(X_cust_train, y_cust_train)\n",
    "y_pred_baseline = rf_baseline.predict(X_cust_test)\n",
    "baseline_accuracy = accuracy_score(y_cust_test, y_pred_baseline)\n",
    "\n",
    "print(f\"Baseline Random Forest Performance:\")\n",
    "print(f\"Accuracy: {baseline_accuracy:.3f}\")\n",
    "print(f\"OOB Score: {rf_baseline.oob_score_:.3f}\" if hasattr(rf_baseline, 'oob_score_') else \"OOB not calculated\")\n",
    "\n",
    "# Hyperparameter optimization\n",
    "print(f\"\\nHyperparameter Optimization:\")\n",
    "param_grid = {\n",
    " 'n_estimators': [50, 100, 200],\n",
    " 'max_depth': [5, 10, 15, None],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Reduced grid for faster execution\n",
    "reduced_param_grid = {\n",
    " 'n_estimators': [100, 200],\n",
    " 'max_depth': [10, None],\n",
    " 'min_samples_split': [2, 5],\n",
    " 'max_features': ['sqrt', None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    " RandomForestClassifier(random_state=42, oob_score=True),\n",
    " reduced_param_grid,\n",
    " cv=3,\n",
    " scoring='accuracy',\n",
    " n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_cust_train, y_cust_train)\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate optimized model\n",
    "y_pred_optimized = best_rf.predict(X_cust_test)\n",
    "optimized_accuracy = accuracy_score(y_cust_test, y_pred_optimized)\n",
    "\n",
    "print(f\"\\nOptimized Random Forest Performance:\")\n",
    "print(f\"Accuracy: {optimized_accuracy:.3f}\")\n",
    "print(f\"Improvement: {optimized_accuracy - baseline_accuracy:+.3f}\")\n",
    "print(f\"OOB Score: {best_rf.oob_score_:.3f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    " 'feature': customer_features,\n",
    " 'importance': best_rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "for i, (idx, row) in enumerate(feature_importance.head(10).iterrows()):\n",
    " print(f\"{i+1:2d}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Compare with single decision tree\n",
    "single_tree = DecisionTreeClassifier(random_state=42)\n",
    "single_tree.fit(X_cust_train, y_cust_train)\n",
    "tree_accuracy = accuracy_score(y_cust_test, single_tree.predict(X_cust_test))\n",
    "\n",
    "print(f\"\\nEnsemble vs Single Tree Comparison:\")\n",
    "print(f\"Random Forest: {optimized_accuracy:.3f}\")\n",
    "print(f\"Single Tree: {tree_accuracy:.3f}\")\n",
    "print(f\"Ensemble benefit: {optimized_accuracy - tree_accuracy:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddff69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. HIGH-DIMENSIONAL DATA ANALYSIS (GENE EXPRESSION)\n",
    "print(\" 2. HIGH-DIMENSIONAL DATA ANALYSIS (GENE EXPRESSION)\")\n",
    "print(\"=\"*56)\n",
    "\n",
    "# Prepare gene expression data\n",
    "X_gene = gene_df[gene_names].values\n",
    "y_gene = gene_df['condition_code'].values\n",
    "\n",
    "# Split the data\n",
    "X_gene_train, X_gene_test, y_gene_train, y_gene_test = train_test_split(\n",
    " X_gene, y_gene, test_size=0.2, random_state=42, stratify=y_gene\n",
    ")\n",
    "\n",
    "# Train Random Forest for gene data\n",
    "rf_gene = RandomForestClassifier(\n",
    " n_estimators=200,\n",
    " max_features='sqrt', # Important for high-dimensional data\n",
    " random_state=42,\n",
    " oob_score=True\n",
    ")\n",
    "\n",
    "rf_gene.fit(X_gene_train, y_gene_train)\n",
    "y_gene_pred = rf_gene.predict(X_gene_test)\n",
    "gene_accuracy = accuracy_score(y_gene_test, y_gene_pred)\n",
    "\n",
    "print(f\"Gene Expression Classification Performance:\")\n",
    "print(f\"Accuracy: {gene_accuracy:.3f}\")\n",
    "print(f\"OOB Score: {rf_gene.oob_score_:.3f}\")\n",
    "\n",
    "# Feature selection using Random Forest\n",
    "gene_importance = pd.DataFrame({\n",
    " 'gene': gene_names,\n",
    " 'importance': rf_gene.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Select top genes\n",
    "top_genes = gene_importance.head(20)\n",
    "print(f\"\\nTop 10 Most Important Genes:\")\n",
    "for i, (idx, row) in enumerate(top_genes.head(10).iterrows()):\n",
    " print(f\"{i+1:2d}. {row['gene']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Train model with reduced features\n",
    "top_gene_names = top_genes['gene'].tolist()\n",
    "X_gene_reduced = gene_df[top_gene_names].values\n",
    "X_gene_red_train, X_gene_red_test, _, _ = train_test_split(\n",
    " X_gene_reduced, y_gene, test_size=0.2, random_state=42, stratify=y_gene\n",
    ")\n",
    "\n",
    "rf_gene_reduced = RandomForestClassifier(\n",
    " n_estimators=200, random_state=42, oob_score=True\n",
    ")\n",
    "rf_gene_reduced.fit(X_gene_red_train, y_gene_train)\n",
    "y_gene_red_pred = rf_gene_reduced.predict(X_gene_red_test)\n",
    "gene_reduced_accuracy = accuracy_score(y_gene_test, y_gene_red_pred)\n",
    "\n",
    "print(f\"\\nFeature Selection Results:\")\n",
    "print(f\"All genes ({len(gene_names)}): {gene_accuracy:.3f}\")\n",
    "print(f\"Top genes ({len(top_gene_names)}): {gene_reduced_accuracy:.3f}\")\n",
    "print(f\"Performance change: {gene_reduced_accuracy - gene_accuracy:+.3f}\")\n",
    "print(f\"Dimensionality reduction: {(1 - len(top_gene_names)/len(gene_names))*100:.1f}%\")\n",
    "\n",
    "# Permutation importance for verification\n",
    "print(f\"\\nPermutation Importance Analysis:\")\n",
    "perm_importance = permutation_importance(\n",
    " rf_gene_reduced, X_gene_red_test, y_gene_test, n_repeats=5, random_state=42\n",
    ")\n",
    "\n",
    "perm_importance_df = pd.DataFrame({\n",
    " 'gene': top_gene_names,\n",
    " 'perm_importance': perm_importance.importances_mean,\n",
    " 'perm_std': perm_importance.importances_std\n",
    "}).sort_values('perm_importance', ascending=False)\n",
    "\n",
    "print(f\"Top 5 genes by permutation importance:\")\n",
    "for i, (idx, row) in enumerate(perm_importance_df.head(5).iterrows()):\n",
    " print(f\"{i+1}. {row['gene']}: {row['perm_importance']:.4f} ± {row['perm_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ENSEMBLE BEHAVIOR ANALYSIS\n",
    "print(\" 3. ENSEMBLE BEHAVIOR ANALYSIS\")\n",
    "print(\"=\"*33)\n",
    "\n",
    "# Analyze how ensemble size affects performance\n",
    "n_estimators_range = [1, 5, 10, 25, 50, 100, 200, 300]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "oob_scores = []\n",
    "\n",
    "print(\"Analyzing ensemble size effect...\")\n",
    "for n_est in n_estimators_range:\n",
    " rf_temp = RandomForestClassifier(\n",
    " n_estimators=n_est,\n",
    " random_state=42,\n",
    " oob_score=True\n",
    " )\n",
    "\n",
    " rf_temp.fit(X_cust_train, y_cust_train)\n",
    "\n",
    " train_score = rf_temp.score(X_cust_train, y_cust_train)\n",
    " test_score = rf_temp.score(X_cust_test, y_cust_test)\n",
    " oob_score = rf_temp.oob_score_\n",
    "\n",
    " train_scores.append(train_score)\n",
    " test_scores.append(test_score)\n",
    " oob_scores.append(oob_score)\n",
    "\n",
    " print(f\"n_estimators={n_est:3d}: Train={train_score:.3f}, Test={test_score:.3f}, OOB={oob_score:.3f}\")\n",
    "\n",
    "# Analyze max_features effect\n",
    "max_features_options = ['sqrt', 'log2', 0.3, 0.5, 0.7, None]\n",
    "max_features_scores = []\n",
    "\n",
    "print(f\"\\nAnalyzing max_features effect:\")\n",
    "for max_feat in max_features_options:\n",
    " rf_temp = RandomForestClassifier(\n",
    " n_estimators=100,\n",
    " max_features=max_feat,\n",
    " random_state=42,\n",
    " oob_score=True\n",
    " )\n",
    "\n",
    " cv_scores = cross_val_score(rf_temp, X_cust_train, y_cust_train, cv=3)\n",
    " max_features_scores.append(cv_scores.mean())\n",
    "\n",
    " print(f\"max_features={str(max_feat):6s}: CV Score={cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "# Best max_features\n",
    "best_max_feat_idx = np.argmax(max_features_scores)\n",
    "best_max_feat = max_features_options[best_max_feat_idx]\n",
    "print(f\"\\nBest max_features: {best_max_feat} (Score: {max_features_scores[best_max_feat_idx]:.3f})\")\n",
    "\n",
    "# Compare with Extra Trees\n",
    "print(f\"\\nComparing Random Forest vs Extra Trees:\")\n",
    "extra_trees = ExtraTreesClassifier(\n",
    " n_estimators=100,\n",
    " random_state=42,\n",
    " oob_score=True\n",
    ")\n",
    "\n",
    "extra_trees.fit(X_cust_train, y_cust_train)\n",
    "et_accuracy = extra_trees.score(X_cust_test, y_cust_test)\n",
    "rf_accuracy = best_rf.score(X_cust_test, y_cust_test)\n",
    "\n",
    "print(f\"Random Forest: {rf_accuracy:.3f}\")\n",
    "print(f\"Extra Trees: {et_accuracy:.3f}\")\n",
    "print(f\"Difference: {et_accuracy - rf_accuracy:+.3f}\")\n",
    "\n",
    "# Bootstrap sampling analysis\n",
    "print(f\"\\nBootstrap Sampling Analysis:\")\n",
    "n_samples_bootstrap = X_cust_train.shape[0]\n",
    "unique_samples_ratios = []\n",
    "\n",
    "for i in range(100): # 100 bootstrap samples\n",
    " bootstrap_indices = np.random.choice(n_samples_bootstrap, n_samples_bootstrap, replace=True)\n",
    " unique_samples = len(np.unique(bootstrap_indices))\n",
    " unique_ratio = unique_samples / n_samples_bootstrap\n",
    " unique_samples_ratios.append(unique_ratio)\n",
    "\n",
    "avg_unique_ratio = np.mean(unique_samples_ratios)\n",
    "print(f\"Average unique samples in bootstrap: {avg_unique_ratio:.3f} ({avg_unique_ratio*100:.1f}%)\")\n",
    "print(f\"Average out-of-bag samples: {1-avg_unique_ratio:.3f} ({(1-avg_unique_ratio)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. COMPREHENSIVE RANDOM FOREST VISUALIZATION DASHBOARD\n",
    "print(\" 4. COMPREHENSIVE RANDOM FOREST VISUALIZATION DASHBOARD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "fig = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=[\n",
    " 'Ensemble Size vs Performance',\n",
    " 'Feature Importance: Customer Segmentation',\n",
    " 'max_features Parameter Analysis',\n",
    " 'Confusion Matrix: Customer Segments',\n",
    " 'Gene Expression: Top Important Genes',\n",
    " 'OOB Error vs Training Error'\n",
    " ],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"type\": \"heatmap\"}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# 1. Ensemble size analysis\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=train_scores,\n",
    " mode='lines+markers',\n",
    " name='Training Score',\n",
    " line=dict(color='blue')\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=test_scores,\n",
    " mode='lines+markers',\n",
    " name='Test Score',\n",
    " line=dict(color='red')\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=oob_scores,\n",
    " mode='lines+markers',\n",
    " name='OOB Score',\n",
    " line=dict(color='green')\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Feature importance\n",
    "top_features = feature_importance.head(15)\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=top_features['importance'],\n",
    " y=top_features['feature'],\n",
    " orientation='h',\n",
    " marker_color='forestgreen',\n",
    " name='Feature Importance'\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. max_features analysis\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=[str(x) for x in max_features_options],\n",
    " y=max_features_scores,\n",
    " marker_color='lightblue',\n",
    " name='CV Score'\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Confusion matrix\n",
    "cm_customer = confusion_matrix(y_cust_test, y_pred_optimized)\n",
    "segments = ['Budget', 'Standard', 'Premium', 'Enterprise']\n",
    "\n",
    "fig.add_trace(\n",
    " go.Heatmap(\n",
    " z=cm_customer,\n",
    " x=segments,\n",
    " y=segments,\n",
    " colorscale='Blues',\n",
    " text=cm_customer,\n",
    " texttemplate='%{text}',\n",
    " hovertemplate='Predicted: %{x}<br>Actual: %{y}<br>Count: %{z}<extra></extra>'\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "# 5. Gene importance\n",
    "top_genes_plot = gene_importance.head(15)\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=top_genes_plot['importance'],\n",
    " y=top_genes_plot['gene'],\n",
    " orientation='h',\n",
    " marker_color='orange',\n",
    " name='Gene Importance'\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Training vs OOB error comparison\n",
    "error_train = [1 - score for score in train_scores]\n",
    "error_oob = [1 - score for score in oob_scores]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=error_train,\n",
    " mode='lines+markers',\n",
    " name='Training Error',\n",
    " line=dict(color='blue', dash='solid')\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=error_oob,\n",
    " mode='lines+markers',\n",
    " name='OOB Error',\n",
    " line=dict(color='red', dash='dash')\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    " height=1200,\n",
    " title=\"Random Forest Classification - Comprehensive Analysis Dashboard\",\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Number of Estimators\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Feature Importance\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"max_features Parameter\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Predicted Segment\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"Gene Importance\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"Number of Estimators\", row=3, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Accuracy Score\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Features\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"CV Score\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Actual Segment\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Gene Names\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Error Rate\", row=3, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77520fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. BUSINESS INSIGHTS AND ROI ANALYSIS\n",
    "print(\" 5. BUSINESS INSIGHTS AND ROI ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Customer segmentation business impact\n",
    "print(\"Customer Segmentation System ROI:\")\n",
    "total_customers = 100000 # Total customer base\n",
    "segmentation_accuracy = optimized_accuracy\n",
    "\n",
    "# Revenue impact by segment\n",
    "segment_revenues = {\n",
    " 'Budget': 1000,\n",
    " 'Standard': 3000,\n",
    " 'Premium': 8000,\n",
    " 'Enterprise': 22000\n",
    "}\n",
    "\n",
    "# Calculate improved targeting efficiency\n",
    "baseline_conversion = 0.05 # 5% conversion without segmentation\n",
    "segment_conversion_improvement = {\n",
    " 'Budget': 0.02, # 2% improvement\n",
    " 'Standard': 0.03, # 3% improvement\n",
    " 'Premium': 0.05, # 5% improvement\n",
    " 'Enterprise': 0.08 # 8% improvement\n",
    "}\n",
    "\n",
    "# Calculate segment distribution\n",
    "segment_distribution = customer_df['segment'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "total_revenue_improvement = 0\n",
    "for segment, proportion in segment_distribution.items():\n",
    " customers_in_segment = total_customers * proportion\n",
    " correctly_identified = customers_in_segment * segmentation_accuracy\n",
    "\n",
    " base_revenue = correctly_identified * baseline_conversion * segment_revenues[segment]\n",
    " improved_conversion = baseline_conversion + segment_conversion_improvement[segment]\n",
    " improved_revenue = correctly_identified * improved_conversion * segment_revenues[segment]\n",
    "\n",
    " segment_improvement = improved_revenue - base_revenue\n",
    " total_revenue_improvement += segment_improvement\n",
    "\n",
    " print(f\"• {segment}: {customers_in_segment:,.0f} customers, \"\n",
    " f\"${segment_improvement:,.0f} additional revenue\")\n",
    "\n",
    "# System costs\n",
    "implementation_cost = 250000 # Initial development\n",
    "annual_operational_cost = 80000 # Maintenance and updates\n",
    "net_annual_benefit = total_revenue_improvement - annual_operational_cost\n",
    "roi = (net_annual_benefit - implementation_cost) / implementation_cost\n",
    "\n",
    "print(f\"\\nCustomer Segmentation ROI Summary:\")\n",
    "print(f\"• Total revenue improvement: ${total_revenue_improvement:,.0f}/year\")\n",
    "print(f\"• Implementation cost: ${implementation_cost:,.0f}\")\n",
    "print(f\"• Annual operational cost: ${annual_operational_cost:,.0f}\")\n",
    "print(f\"• Net annual benefit: ${net_annual_benefit:,.0f}\")\n",
    "print(f\"• ROI: {roi*100:.0f}%\")\n",
    "print(f\"• Payback period: {implementation_cost/net_annual_benefit*12:.1f} months\")\n",
    "\n",
    "# Gene expression analysis business impact\n",
    "print(f\"\\nBiomedical Research Cost Savings:\")\n",
    "total_genes_analyzed = len(gene_names)\n",
    "genes_selected = len(top_gene_names)\n",
    "cost_per_gene_analysis = 500 # Cost to analyze each gene\n",
    "\n",
    "baseline_analysis_cost = total_genes_analyzed * cost_per_gene_analysis\n",
    "reduced_analysis_cost = genes_selected * cost_per_gene_analysis\n",
    "cost_savings = baseline_analysis_cost - reduced_analysis_cost\n",
    "\n",
    "print(f\"• Genes reduced from {total_genes_analyzed} to {genes_selected}\")\n",
    "print(f\"• Cost per gene analysis: ${cost_per_gene_analysis}\")\n",
    "print(f\"• Analysis cost savings: ${cost_savings:,.0f} per study\")\n",
    "print(f\"• Accuracy maintained: {gene_reduced_accuracy:.3f} vs {gene_accuracy:.3f}\")\n",
    "print(f\"• Dimensionality reduction: {(1-genes_selected/total_genes_analyzed)*100:.1f}%\")\n",
    "\n",
    "# Random Forest advantages summary\n",
    "print(f\"\\nRandom Forest Key Advantages:\")\n",
    "print(f\"• Handles high-dimensional data effectively\")\n",
    "print(f\"• Provides feature importance for interpretability\")\n",
    "print(f\"• Built-in cross-validation through OOB error\")\n",
    "print(f\"• Resistant to overfitting with large ensembles\")\n",
    "print(f\"• Handles missing values and categorical features\")\n",
    "print(f\"• Parallelizable for fast training on large datasets\")\n",
    "\n",
    "print(f\"\\nImplementation Guidelines:\")\n",
    "print(f\"• n_estimators: Start with 100, increase until OOB error stabilizes\")\n",
    "print(f\"• max_features: Use 'sqrt' for classification, 'None' for small datasets\")\n",
    "print(f\"• max_depth: Start with None, add constraints if overfitting occurs\")\n",
    "print(f\"• min_samples_split: Increase (5-10) for noisy data\")\n",
    "print(f\"• Feature selection: Use importance scores for dimensionality reduction\")\n",
    "\n",
    "print(f\"\\nCross-Reference Learning Path:\")\n",
    "print(f\"• Foundation: Tier2_DecisionTree.ipynb (tree fundamentals)\")\n",
    "print(f\"• Building On: Tier2_RandomForest.ipynb (basic implementation)\")\n",
    "print(f\"• Comparison: Tier5_GradientBoosting.ipynb (boosting vs bagging)\")\n",
    "print(f\"• Advanced: Advanced_EnsembleClassification.ipynb, Advanced_FeatureSelection.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}