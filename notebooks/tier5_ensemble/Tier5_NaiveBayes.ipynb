{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34728ba",
   "metadata": {},
   "source": [
    "# Tier 5: Naive Bayes Classification\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** b0b14187-df8b-4626-823d-a105acb62f35\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 5: Naive Bayes Classification,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** b0b14187-df8b-4626-823d-a105acb62f35\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, ComplementNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.datasets import make_classification, fetch_20newsgroups\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 5: Naive Bayes Classification - Libraries Loaded!\")\n",
    "print(\"=\"*55)\n",
    "print(\"Naive Bayes Classification Techniques:\")\n",
    "print(\"• Gaussian Naive Bayes for continuous features\")\n",
    "print(\"• Multinomial Naive Bayes for count data\")\n",
    "print(\"• Bernoulli Naive Bayes for binary features\")\n",
    "print(\"• Text classification and document analysis\")\n",
    "print(\"• Probability estimation and feature independence\")\n",
    "print(\"• Real-time classification applications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive Naive Bayes datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Medical diagnosis dataset (Gaussian NB)\n",
    "def generate_medical_dataset(n_samples=2000):\n",
    " \"\"\"Generate realistic medical diagnosis dataset.\"\"\"\n",
    "\n",
    " # Define disease classes\n",
    " diseases = ['Healthy', 'Flu', 'COVID-19', 'Pneumonia']\n",
    " n_classes = len(diseases)\n",
    "\n",
    " data = []\n",
    "\n",
    " for i in range(n_samples):\n",
    " # Random disease assignment\n",
    " disease_idx = np.random.choice(n_classes, p=[0.4, 0.3, 0.2, 0.1])\n",
    " disease = diseases[disease_idx]\n",
    "\n",
    " # Generate symptoms based on disease\n",
    " if disease == 'Healthy':\n",
    " temperature = np.random.normal(98.6, 0.5)\n",
    " heart_rate = np.random.normal(70, 8)\n",
    " blood_pressure = np.random.normal(120, 10)\n",
    " oxygen_saturation = np.random.normal(98, 1)\n",
    " white_cell_count = np.random.normal(7000, 1000)\n",
    "\n",
    " elif disease == 'Flu':\n",
    " temperature = np.random.normal(101.5, 1.2)\n",
    " heart_rate = np.random.normal(85, 12)\n",
    " blood_pressure = np.random.normal(125, 12)\n",
    " oxygen_saturation = np.random.normal(97, 1.5)\n",
    " white_cell_count = np.random.normal(9000, 1500)\n",
    "\n",
    " elif disease == 'COVID-19':\n",
    " temperature = np.random.normal(102.2, 1.5)\n",
    " heart_rate = np.random.normal(90, 15)\n",
    " blood_pressure = np.random.normal(118, 15)\n",
    " oxygen_saturation = np.random.normal(94, 3)\n",
    " white_cell_count = np.random.normal(6000, 2000)\n",
    "\n",
    " else: # Pneumonia\n",
    " temperature = np.random.normal(103.1, 1.8)\n",
    " heart_rate = np.random.normal(95, 18)\n",
    " blood_pressure = np.random.normal(115, 18)\n",
    " oxygen_saturation = np.random.normal(91, 4)\n",
    " white_cell_count = np.random.normal(12000, 2500)\n",
    "\n",
    " data.append({\n",
    " 'patient_id': f'PAT_{i:06d}',\n",
    " 'temperature': temperature,\n",
    " 'heart_rate': heart_rate,\n",
    " 'blood_pressure': blood_pressure,\n",
    " 'oxygen_saturation': oxygen_saturation,\n",
    " 'white_cell_count': white_cell_count,\n",
    " 'age': np.random.normal(45, 15),\n",
    " 'diagnosis': disease,\n",
    " 'diagnosis_code': disease_idx\n",
    " })\n",
    "\n",
    " return pd.DataFrame(data)\n",
    "\n",
    "# 2. Email spam detection dataset (Multinomial/Bernoulli NB)\n",
    "def generate_email_dataset(n_samples=3000):\n",
    " \"\"\"Generate email spam detection dataset.\"\"\"\n",
    "\n",
    " # Common words in spam vs ham emails\n",
    " spam_words = ['free', 'money', 'win', 'prize', 'urgent', 'limited', 'offer', 'click',\n",
    " 'buy', 'discount', 'sale', 'deal', 'cash', 'credit', 'loan']\n",
    " ham_words = ['meeting', 'project', 'report', 'team', 'schedule', 'work', 'office',\n",
    " 'client', 'proposal', 'budget', 'deadline', 'presentation', 'conference']\n",
    "\n",
    " emails = []\n",
    "\n",
    " for i in range(n_samples):\n",
    " is_spam = np.random.choice([0, 1], p=[0.7, 0.3]) # 30% spam\n",
    "\n",
    " if is_spam:\n",
    " # Generate spam email\n",
    " email_words = np.random.choice(spam_words, size=np.random.randint(10, 30))\n",
    " # Add some random normal words\n",
    " normal_words = np.random.choice(ham_words, size=np.random.randint(2, 8))\n",
    " all_words = list(email_words) + list(normal_words)\n",
    " else:\n",
    " # Generate ham email\n",
    " email_words = np.random.choice(ham_words, size=np.random.randint(15, 40))\n",
    " # Add occasional spam words (false positives)\n",
    " occasional_spam = np.random.choice(spam_words, size=np.random.randint(0, 3))\n",
    " all_words = list(email_words) + list(occasional_spam)\n",
    "\n",
    " # Create email text\n",
    " email_text = ' '.join(all_words)\n",
    "\n",
    " # Email features\n",
    " emails.append({\n",
    " 'email_id': f'EMAIL_{i:06d}',\n",
    " 'text': email_text,\n",
    " 'word_count': len(all_words),\n",
    " 'exclamation_count': email_text.count('!'),\n",
    " 'capital_ratio': sum(1 for c in email_text if c.isupper()) / len(email_text),\n",
    " 'spam_word_count': sum(1 for word in all_words if word in spam_words),\n",
    " 'is_spam': is_spam\n",
    " })\n",
    "\n",
    " return pd.DataFrame(emails)\n",
    "\n",
    "# Generate datasets\n",
    "medical_df = generate_medical_dataset()\n",
    "email_df = generate_email_dataset()\n",
    "\n",
    "print(\" Naive Bayes Datasets Created:\")\n",
    "print(f\"Medical diagnosis dataset: {medical_df.shape}\")\n",
    "print(f\"Disease distribution: {medical_df['diagnosis'].value_counts().to_dict()}\")\n",
    "print(f\"\\nEmail spam dataset: {email_df.shape}\")\n",
    "print(f\"Spam distribution: {email_df['is_spam'].value_counts().to_dict()}\")\n",
    "print(f\"Sample email text: '{email_df['text'].iloc[0][:50]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GAUSSIAN NAIVE BAYES FOR MEDICAL DIAGNOSIS\n",
    "print(\" 1. GAUSSIAN NAIVE BAYES FOR MEDICAL DIAGNOSIS\")\n",
    "print(\"=\"*48)\n",
    "\n",
    "# Prepare medical data\n",
    "medical_features = ['temperature', 'heart_rate', 'blood_pressure', 'oxygen_saturation', 'white_cell_count', 'age']\n",
    "X_medical = medical_df[medical_features].values\n",
    "y_medical = medical_df['diagnosis_code'].values\n",
    "\n",
    "# Split the data\n",
    "X_med_train, X_med_test, y_med_train, y_med_test = train_test_split(\n",
    " X_medical, y_medical, test_size=0.2, random_state=42, stratify=y_medical\n",
    ")\n",
    "\n",
    "# Train Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_med_train, y_med_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_med_pred = gnb.predict(X_med_test)\n",
    "y_med_proba = gnb.predict_proba(X_med_test)\n",
    "\n",
    "# Performance metrics\n",
    "med_accuracy = accuracy_score(y_med_test, y_med_pred)\n",
    "med_cv_scores = cross_val_score(gnb, X_med_train, y_med_train, cv=5)\n",
    "\n",
    "print(f\"Medical Diagnosis Performance:\")\n",
    "print(f\"Accuracy: {med_accuracy:.3f}\")\n",
    "print(f\"Cross-validation: {med_cv_scores.mean():.3f} ± {med_cv_scores.std():.3f}\")\n",
    "\n",
    "# Feature analysis\n",
    "print(f\"\\nFeature Analysis (class means):\")\n",
    "diseases = ['Healthy', 'Flu', 'COVID-19', 'Pneumonia']\n",
    "for i, disease in enumerate(diseases):\n",
    " class_means = gnb.theta_[i]\n",
    " print(f\"{disease}:\")\n",
    " for j, feature in enumerate(medical_features):\n",
    " print(f\" {feature}: {class_means[j]:.1f}\")\n",
    "\n",
    "# Sample predictions with probabilities\n",
    "print(f\"\\nSample Predictions (with probabilities):\")\n",
    "for i in range(5):\n",
    " actual = diseases[y_med_test[i]]\n",
    " predicted = diseases[y_med_pred[i]]\n",
    " proba = y_med_proba[i]\n",
    " max_proba = np.max(proba)\n",
    " print(f\"Patient {i+1}: Actual={actual}, Predicted={predicted} (confidence: {max_proba:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd440bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MULTINOMIAL AND BERNOULLI NAIVE BAYES FOR TEXT CLASSIFICATION\n",
    "print(\" 2. MULTINOMIAL AND BERNOULLI NAIVE BAYES FOR TEXT CLASSIFICATION\")\n",
    "print(\"=\"*66)\n",
    "\n",
    "# Prepare email text data\n",
    "# Create TF-IDF features for Multinomial NB\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_email_tfidf = tfidf_vectorizer.fit_transform(email_df['text'])\n",
    "\n",
    "# Create binary features for Bernoulli NB\n",
    "binary_vectorizer = CountVectorizer(max_features=500, binary=True, stop_words='english')\n",
    "X_email_binary = binary_vectorizer.fit_transform(email_df['text'])\n",
    "\n",
    "y_email = email_df['is_spam'].values\n",
    "\n",
    "# Split data\n",
    "X_email_tfidf_train, X_email_tfidf_test, y_email_train, y_email_test = train_test_split(\n",
    " X_email_tfidf, y_email, test_size=0.2, random_state=42, stratify=y_email\n",
    ")\n",
    "\n",
    "X_email_bin_train, X_email_bin_test, _, _ = train_test_split(\n",
    " X_email_binary, y_email, test_size=0.2, random_state=42, stratify=y_email\n",
    ")\n",
    "\n",
    "# Train Multinomial Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_email_tfidf_train, y_email_train)\n",
    "y_email_pred_mnb = mnb.predict(X_email_tfidf_test)\n",
    "mnb_accuracy = accuracy_score(y_email_test, y_email_pred_mnb)\n",
    "\n",
    "# Train Bernoulli Naive Bayes\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_email_bin_train, y_email_train)\n",
    "y_email_pred_bnb = bnb.predict(X_email_bin_test)\n",
    "bnb_accuracy = accuracy_score(y_email_test, y_email_pred_bnb)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "print(\"Hyperparameter Optimization:\")\n",
    "\n",
    "# Multinomial NB alpha tuning\n",
    "alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "mnb_scores = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    " mnb_temp = MultinomialNB(alpha=alpha)\n",
    " scores = cross_val_score(mnb_temp, X_email_tfidf_train, y_email_train, cv=5)\n",
    " mnb_scores.append(scores.mean())\n",
    "\n",
    "best_alpha_mnb = alpha_values[np.argmax(mnb_scores)]\n",
    "print(f\"Best alpha for Multinomial NB: {best_alpha_mnb} (CV score: {max(mnb_scores):.3f})\")\n",
    "\n",
    "# Bernoulli NB alpha tuning\n",
    "bnb_scores = []\n",
    "for alpha in alpha_values:\n",
    " bnb_temp = BernoulliNB(alpha=alpha)\n",
    " scores = cross_val_score(bnb_temp, X_email_bin_train, y_email_train, cv=5)\n",
    " bnb_scores.append(scores.mean())\n",
    "\n",
    "best_alpha_bnb = alpha_values[np.argmax(bnb_scores)]\n",
    "print(f\"Best alpha for Bernoulli NB: {best_alpha_bnb} (CV score: {max(bnb_scores):.3f})\")\n",
    "\n",
    "# Retrain with optimal parameters\n",
    "mnb_best = MultinomialNB(alpha=best_alpha_mnb)\n",
    "mnb_best.fit(X_email_tfidf_train, y_email_train)\n",
    "y_email_pred_mnb_best = mnb_best.predict(X_email_tfidf_test)\n",
    "\n",
    "bnb_best = BernoulliNB(alpha=best_alpha_bnb)\n",
    "bnb_best.fit(X_email_bin_train, y_email_train)\n",
    "y_email_pred_bnb_best = bnb_best.predict(X_email_bin_test)\n",
    "\n",
    "print(f\"\\nEmail Spam Detection Performance:\")\n",
    "print(f\"Multinomial NB: {accuracy_score(y_email_test, y_email_pred_mnb_best):.3f}\")\n",
    "print(f\"Bernoulli NB: {accuracy_score(y_email_test, y_email_pred_bnb_best):.3f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print(f\"\\nTop spam-indicating words (Multinomial NB):\")\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "spam_log_prob = mnb_best.feature_log_prob_[1] # Spam class\n",
    "ham_log_prob = mnb_best.feature_log_prob_[0] # Ham class\n",
    "\n",
    "# Calculate feature importance as difference in log probabilities\n",
    "feature_importance = spam_log_prob - ham_log_prob\n",
    "top_spam_indices = np.argsort(feature_importance)[-10:]\n",
    "\n",
    "for idx in reversed(top_spam_indices):\n",
    " word = feature_names[idx]\n",
    " importance = feature_importance[idx]\n",
    " print(f\" {word}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. COMPREHENSIVE NAIVE BAYES VISUALIZATION DASHBOARD\n",
    "print(\" 3. COMPREHENSIVE NAIVE BAYES VISUALIZATION DASHBOARD\")\n",
    "print(\"=\"*58)\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "fig = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=[\n",
    " 'Medical Diagnosis: Feature Distributions by Class',\n",
    " 'Email Classification: Algorithm Comparison',\n",
    " 'Hyperparameter Tuning: Alpha vs Performance',\n",
    " 'Confusion Matrix: Medical Diagnosis',\n",
    " 'ROC Curves: Binary Classification (Spam Detection)',\n",
    " 'Feature Importance: Top Spam Indicators'\n",
    " ],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"type\": \"heatmap\"}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# 1. Medical feature distributions\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "diseases = ['Healthy', 'Flu', 'COVID-19', 'Pneumonia']\n",
    "\n",
    "for i, disease in enumerate(diseases):\n",
    " disease_data = medical_df[medical_df['diagnosis'] == disease]\n",
    " fig.add_trace(\n",
    " go.Violin(\n",
    " y=disease_data['temperature'],\n",
    " name=disease,\n",
    " line_color=colors[i],\n",
    " box_visible=True,\n",
    " meanline_visible=True\n",
    " ),\n",
    " row=1, col=1\n",
    " )\n",
    "\n",
    "# 2. Algorithm comparison\n",
    "algorithms = ['Multinomial NB', 'Bernoulli NB']\n",
    "accuracies = [\n",
    " accuracy_score(y_email_test, y_email_pred_mnb_best),\n",
    " accuracy_score(y_email_test, y_email_pred_bnb_best)\n",
    "]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=algorithms,\n",
    " y=accuracies,\n",
    " marker_color=['lightblue', 'lightcoral'],\n",
    " text=[f'{acc:.3f}' for acc in accuracies],\n",
    " textposition='auto'\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Hyperparameter tuning curves\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=alpha_values,\n",
    " y=mnb_scores,\n",
    " mode='lines+markers',\n",
    " name='Multinomial NB',\n",
    " line=dict(color='blue')\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=alpha_values,\n",
    " y=bnb_scores,\n",
    " mode='lines+markers',\n",
    " name='Bernoulli NB',\n",
    " line=dict(color='red')\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Confusion matrix for medical diagnosis\n",
    "cm_medical = confusion_matrix(y_med_test, y_med_pred)\n",
    "fig.add_trace(\n",
    " go.Heatmap(\n",
    " z=cm_medical,\n",
    " x=diseases,\n",
    " y=diseases,\n",
    " colorscale='Blues',\n",
    " text=cm_medical,\n",
    " texttemplate='%{text}',\n",
    " hovertemplate='Predicted: %{x}<br>Actual: %{y}<br>Count: %{z}<extra></extra>'\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "# 5. ROC curves for spam detection\n",
    "# Multinomial NB ROC\n",
    "y_score_mnb = mnb_best.predict_proba(X_email_tfidf_test)[:, 1]\n",
    "fpr_mnb, tpr_mnb, _ = roc_curve(y_email_test, y_score_mnb)\n",
    "auc_mnb = auc(fpr_mnb, tpr_mnb)\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=fpr_mnb,\n",
    " y=tpr_mnb,\n",
    " mode='lines',\n",
    " name=f'Multinomial NB (AUC = {auc_mnb:.3f})',\n",
    " line=dict(color='blue')\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "# Bernoulli NB ROC\n",
    "y_score_bnb = bnb_best.predict_proba(X_email_bin_test)[:, 1]\n",
    "fpr_bnb, tpr_bnb, _ = roc_curve(y_email_test, y_score_bnb)\n",
    "auc_bnb = auc(fpr_bnb, tpr_bnb)\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=fpr_bnb,\n",
    " y=tpr_bnb,\n",
    " mode='lines',\n",
    " name=f'Bernoulli NB (AUC = {auc_bnb:.3f})',\n",
    " line=dict(color='red')\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "# Diagonal line\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=[0, 1],\n",
    " y=[0, 1],\n",
    " mode='lines',\n",
    " line=dict(dash='dash', color='black'),\n",
    " name='Random Classifier',\n",
    " showlegend=False\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Feature importance for spam detection\n",
    "top_words = [feature_names[idx] for idx in reversed(top_spam_indices[-10:])]\n",
    "top_importance = [feature_importance[idx] for idx in reversed(top_spam_indices[-10:])]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=top_importance,\n",
    " y=top_words,\n",
    " orientation='h',\n",
    " marker_color='orange'\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    " height=1200,\n",
    " title=\"Naive Bayes Classification - Comprehensive Analysis Dashboard\",\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_yaxes(title_text=\"Temperature (°F)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"CV Score\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Actual\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"True Positive Rate\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Words\", row=3, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Disease Class\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Algorithm\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Alpha (Smoothing Parameter)\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Predicted\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"False Positive Rate\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"Log Probability Difference\", row=3, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. BUSINESS INSIGHTS AND ROI ANALYSIS\n",
    "print(\" 4. BUSINESS INSIGHTS AND ROI ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Medical diagnosis business impact\n",
    "print(\"Medical Diagnosis System ROI:\")\n",
    "total_patients = 50000 # Annual patient volume\n",
    "diagnosis_accuracy = med_accuracy\n",
    "misdiagnosis_cost = 25000 # Average cost of misdiagnosis\n",
    "system_cost = 150000 # Annual system cost\n",
    "\n",
    "# Calculate prevented misdiagnoses\n",
    "baseline_accuracy = 0.75 # Human doctor baseline\n",
    "improvement = diagnosis_accuracy - baseline_accuracy\n",
    "prevented_misdiagnoses = total_patients * improvement\n",
    "cost_savings = prevented_misdiagnoses * misdiagnosis_cost\n",
    "net_benefit = cost_savings - system_cost\n",
    "\n",
    "print(f\"• Diagnostic accuracy improvement: {improvement*100:.1f}%\")\n",
    "print(f\"• Prevented misdiagnoses: {prevented_misdiagnoses:.0f} cases/year\")\n",
    "print(f\"• Cost savings: ${cost_savings:,.0f}/year\")\n",
    "print(f\"• Net benefit: ${net_benefit:,.0f}/year\")\n",
    "print(f\"• ROI: {(net_benefit/system_cost)*100:.0f}%\")\n",
    "\n",
    "# Email spam detection business impact\n",
    "print(f\"\\nEmail Spam Detection System ROI:\")\n",
    "daily_emails = 100000 # Emails processed per day\n",
    "spam_accuracy = max(accuracy_score(y_email_test, y_email_pred_mnb_best),\n",
    " accuracy_score(y_email_test, y_email_pred_bnb_best))\n",
    "time_saved_per_spam = 0.5 # Minutes saved per correctly filtered spam\n",
    "hourly_wage = 30 # Average employee hourly wage\n",
    "system_cost_email = 75000 # Annual system cost\n",
    "\n",
    "# Calculate time and cost savings\n",
    "annual_emails = daily_emails * 365\n",
    "spam_emails = annual_emails * 0.3 # 30% spam rate\n",
    "correctly_filtered = spam_emails * spam_accuracy\n",
    "time_saved_hours = (correctly_filtered * time_saved_per_spam) / 60\n",
    "labor_cost_savings = time_saved_hours * hourly_wage\n",
    "net_benefit_email = labor_cost_savings - system_cost_email\n",
    "\n",
    "print(f\"• Spam detection accuracy: {spam_accuracy*100:.1f}%\")\n",
    "print(f\"• Emails correctly filtered: {correctly_filtered:,.0f}/year\")\n",
    "print(f\"• Time saved: {time_saved_hours:,.0f} hours/year\")\n",
    "print(f\"• Labor cost savings: ${labor_cost_savings:,.0f}/year\")\n",
    "print(f\"• Net benefit: ${net_benefit_email:,.0f}/year\")\n",
    "print(f\"• ROI: {(net_benefit_email/system_cost_email)*100:.0f}%\")\n",
    "\n",
    "# Combined system benefits\n",
    "total_investment = system_cost + system_cost_email\n",
    "total_benefits = net_benefit + net_benefit_email\n",
    "combined_roi = (total_benefits / total_investment) * 100\n",
    "\n",
    "print(f\"\\nCombined Naive Bayes Systems ROI:\")\n",
    "print(f\"• Total investment: ${total_investment:,.0f}\")\n",
    "print(f\"• Total annual benefits: ${total_benefits:,.0f}\")\n",
    "print(f\"• Combined ROI: {combined_roi:.0f}%\")\n",
    "print(f\"• Payback period: {total_investment/total_benefits*12:.1f} months\")\n",
    "\n",
    "# Algorithm selection recommendations\n",
    "print(f\"\\nNaive Bayes Algorithm Selection Guide:\")\n",
    "print(f\"• Gaussian NB: Continuous features (medical data, sensor readings)\")\n",
    "print(f\"• Multinomial NB: Count data (text classification, word frequencies)\")\n",
    "print(f\"• Bernoulli NB: Binary features (presence/absence, yes/no data)\")\n",
    "print(f\"• Complement NB: Imbalanced datasets with many classes\")\n",
    "\n",
    "print(f\"\\nImplementation Considerations:\")\n",
    "print(f\"• Feature independence assumption: Monitor correlation matrices\")\n",
    "print(f\"• Smoothing parameter (alpha): Use cross-validation for optimization\")\n",
    "print(f\"• Real-time performance: Naive Bayes excels in streaming applications\")\n",
    "print(f\"• Interpretability: Clear probability outputs aid decision-making\")\n",
    "print(f\"• Scalability: Linear time complexity enables big data processing\")\n",
    "\n",
    "print(f\"\\nCross-Reference Learning Path:\")\n",
    "print(f\"• Foundation: Tier1_Distribution.ipynb (probability distributions)\")\n",
    "print(f\"• Building On: Tier2_LogisticRegression.ipynb (probabilistic classification)\")\n",
    "print(f\"• Comparison: Tier5_Classification.ipynb (algorithm comparison)\")\n",
    "print(f\"• Advanced: Advanced_TextClassification.ipynb, Advanced_BayesianMethods.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}