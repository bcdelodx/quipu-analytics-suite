{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 5: Advanced Classification Methods\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** c386813e-e3b5-4313-a3af-c74d45353313\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 5: Advanced Classification Methods,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** c386813e-e3b5-4313-a3af-c74d45353313\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62eb4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
    " precision_score, recall_score, f1_score, roc_auc_score, roc_curve)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Tier 5: Advanced Classification Methods\")\n",
    "print(\"=\" * 42)\n",
    "print(\" CROSS-REFERENCES:\")\n",
    "print(\"• Prerequisites: Tier2_LogisticRegression.ipynb, Tier2_DecisionTree.ipynb, Tier2_RandomForest.ipynb\")\n",
    "print(\"• Builds On: Tier5_SVM.ipynb, Tier5_GradientBoosting.ipynb, Tier5_NeuralNetworks.ipynb\")\n",
    "print(\"• Complements: Tier4_kMeans.ipynb (unsupervised vs supervised)\")\n",
    "print(\"• Advanced: Advanced_EnsembleClassification.ipynb, Advanced_ImbalancedLearning.ipynb\")\n",
    "print(\"=\" * 42)\n",
    "print(\"Advanced Classification Techniques:\")\n",
    "print(\"• Multi-algorithm ensemble comparison\")\n",
    "print(\"• Hyperparameter optimization strategies\")\n",
    "print(\"• Performance metrics and evaluation\")\n",
    "print(\"• Class imbalance handling\")\n",
    "print(\"• Feature importance and selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive multi-class classification dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_advanced_classification_dataset(n_samples=5000, n_features=20, n_classes=4):\n",
    " \"\"\"Generate realistic multi-class dataset with varying complexity.\"\"\"\n",
    "\n",
    " # Create base classification dataset\n",
    " X, y = make_classification(\n",
    " n_samples=n_samples,\n",
    " n_features=n_features,\n",
    " n_informative=12,\n",
    " n_redundant=5,\n",
    " n_clusters_per_class=2,\n",
    " n_classes=n_classes,\n",
    " class_sep=0.8,\n",
    " random_state=42\n",
    " )\n",
    "\n",
    " # Add noise and make more realistic\n",
    " noise = np.random.normal(0, 0.1, X.shape)\n",
    " X = X + noise\n",
    "\n",
    " # Create feature names\n",
    " feature_names = [f'feature_{i+1:02d}' for i in range(n_features)]\n",
    "\n",
    " # Create realistic business context\n",
    " business_features = {\n",
    " 'customer_age': np.random.normal(40, 15, n_samples),\n",
    " 'income_level': np.random.lognormal(10, 0.5, n_samples),\n",
    " 'purchase_frequency': np.random.poisson(5, n_samples),\n",
    " 'customer_tenure': np.random.exponential(2, n_samples),\n",
    " 'satisfaction_score': np.random.beta(8, 2, n_samples) * 10,\n",
    " 'support_tickets': np.random.poisson(2, n_samples)\n",
    " }\n",
    "\n",
    " # Combine technical and business features\n",
    " X_business = np.column_stack([X, list(business_features.values())])\n",
    " all_feature_names = feature_names + list(business_features.keys())\n",
    "\n",
    " # Create class labels with business meaning\n",
    " class_names = ['Low_Value', 'Medium_Value', 'High_Value', 'Premium']\n",
    "\n",
    " # Create DataFrame\n",
    " df = pd.DataFrame(X_business, columns=all_feature_names)\n",
    " df['customer_class'] = [class_names[i] for i in y]\n",
    " df['class_numeric'] = y\n",
    "\n",
    " return df, all_feature_names\n",
    "\n",
    "# Generate the dataset\n",
    "classification_df, feature_names = generate_advanced_classification_dataset()\n",
    "\n",
    "print(\" Advanced Classification Dataset Created:\")\n",
    "print(f\"Dataset shape: {classification_df.shape}\")\n",
    "print(f\"Classes: {classification_df['customer_class'].unique()}\")\n",
    "print(f\"Class distribution:\\n{classification_df['customer_class'].value_counts()}\")\n",
    "print(f\"Features: {len(feature_names)} (technical + business)\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample data preview:\")\n",
    "print(classification_df[['customer_age', 'income_level', 'satisfaction_score', 'customer_class']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08951dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. COMPREHENSIVE CLASSIFICATION ALGORITHM COMPARISON\n",
    "print(\" 1. COMPREHENSIVE CLASSIFICATION ALGORITHM COMPARISON\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Prepare data\n",
    "X = classification_df[feature_names].values\n",
    "y = classification_df['class_numeric'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features for algorithms that need it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define classification algorithms\n",
    "classifiers = {\n",
    " 'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    " 'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    " 'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    " 'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    " 'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),\n",
    " 'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    " 'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=500),\n",
    " 'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate all classifiers\n",
    "results = {}\n",
    "cv_scores = {}\n",
    "\n",
    "print(\"Training and evaluating classifiers...\")\n",
    "for name, clf in classifiers.items():\n",
    " print(f\"Training {name}...\")\n",
    "\n",
    " # Use scaled data for algorithms that need it\n",
    " if name in ['Logistic Regression', 'SVM (RBF)', 'K-Nearest Neighbors', 'Neural Network']:\n",
    " X_train_use = X_train_scaled\n",
    " X_test_use = X_test_scaled\n",
    " else:\n",
    " X_train_use = X_train\n",
    " X_test_use = X_test\n",
    "\n",
    " # Train the classifier\n",
    " clf.fit(X_train_use, y_train)\n",
    "\n",
    " # Make predictions\n",
    " y_pred = clf.predict(X_test_use)\n",
    " y_pred_proba = clf.predict_proba(X_test_use) if hasattr(clf, 'predict_proba') else None\n",
    "\n",
    " # Calculate metrics\n",
    " accuracy = accuracy_score(y_test, y_pred)\n",
    " precision = precision_score(y_test, y_pred, average='weighted')\n",
    " recall = recall_score(y_test, y_pred, average='weighted')\n",
    " f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    " # Cross-validation scores\n",
    " cv_score = cross_val_score(clf, X_train_use, y_train, cv=5, scoring='accuracy')\n",
    " cv_scores[name] = cv_score\n",
    "\n",
    " # Store results\n",
    " results[name] = {\n",
    " 'accuracy': accuracy,\n",
    " 'precision': precision,\n",
    " 'recall': recall,\n",
    " 'f1_score': f1,\n",
    " 'cv_mean': cv_score.mean(),\n",
    " 'cv_std': cv_score.std(),\n",
    " 'y_pred': y_pred,\n",
    " 'y_pred_proba': y_pred_proba,\n",
    " 'model': clf\n",
    " }\n",
    "\n",
    " print(f\" Accuracy: {accuracy:.3f} | F1: {f1:.3f} | CV: {cv_score.mean():.3f} ± {cv_score.std():.3f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    " name: {\n",
    " 'Accuracy': results[name]['accuracy'],\n",
    " 'Precision': results[name]['precision'],\n",
    " 'Recall': results[name]['recall'],\n",
    " 'F1-Score': results[name]['f1_score'],\n",
    " 'CV Mean': results[name]['cv_mean'],\n",
    " 'CV Std': results[name]['cv_std']\n",
    " }\n",
    " for name in results.keys()\n",
    "}).T\n",
    "\n",
    "print(f\"\\n CLASSIFICATION PERFORMANCE SUMMARY:\")\n",
    "print(\"=\" * 45)\n",
    "print(results_df.round(3))\n",
    "\n",
    "# Find best performer\n",
    "best_classifier = results_df['F1-Score'].idxmax()\n",
    "best_f1 = results_df.loc[best_classifier, 'F1-Score']\n",
    "print(f\"\\n Best Performer: {best_classifier} (F1-Score: {best_f1:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178444e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. INTERACTIVE CLASSIFICATION VISUALIZATIONS\n",
    "print(\" 2. INTERACTIVE CLASSIFICATION VISUALIZATIONS\")\n",
    "print(\"=\" * 47)\n",
    "\n",
    "# Create comprehensive classification dashboard\n",
    "fig = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=[\n",
    " 'Algorithm Performance Comparison',\n",
    " 'Cross-Validation Score Distributions',\n",
    " 'Feature Importance (Random Forest)',\n",
    " 'Confusion Matrix Heatmap (Best Model)',\n",
    " 'ROC Curves (Multi-Class)',\n",
    " 'Classification Decision Boundaries (2D Projection)'\n",
    " ],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"type\": \"heatmap\"}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# 1. Performance comparison bar chart\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    " fig.add_trace(\n",
    " go.Bar(\n",
    " x=results_df.index,\n",
    " y=results_df[metric],\n",
    " name=metric,\n",
    " marker_color=colors[i],\n",
    " opacity=0.8,\n",
    " offsetgroup=i\n",
    " ),\n",
    " row=1, col=1\n",
    " )\n",
    "\n",
    "# 2. Cross-validation distributions\n",
    "for name in cv_scores.keys():\n",
    " fig.add_trace(\n",
    " go.Box(\n",
    " y=cv_scores[name],\n",
    " name=name,\n",
    " boxpoints='all',\n",
    " jitter=0.3,\n",
    " pointpos=-1.8\n",
    " ),\n",
    " row=1, col=2\n",
    " )\n",
    "\n",
    "# 3. Feature importance (Random Forest)\n",
    "rf_model = results['Random Forest']['model']\n",
    "feature_importance = pd.DataFrame({\n",
    " 'feature': feature_names,\n",
    " 'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=True).tail(15)\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=feature_importance['importance'],\n",
    " y=feature_importance['feature'],\n",
    " orientation='h',\n",
    " marker_color='forestgreen',\n",
    " name='Feature Importance'\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Confusion matrix for best model\n",
    "best_y_pred = results[best_classifier]['y_pred']\n",
    "cm = confusion_matrix(y_test, best_y_pred)\n",
    "class_names = ['Low_Value', 'Medium_Value', 'High_Value', 'Premium']\n",
    "\n",
    "fig.add_trace(\n",
    " go.Heatmap(\n",
    " z=cm,\n",
    " x=class_names,\n",
    " y=class_names,\n",
    " colorscale='Blues',\n",
    " showscale=True,\n",
    " text=cm,\n",
    " texttemplate=\"%{text}\",\n",
    " hovertemplate='Predicted: %{x}<br>Actual: %{y}<br>Count: %{z}<extra></extra>'\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "# 5. ROC Curves (One-vs-Rest for multi-class)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Binarize the output\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Get probabilities for best classifier\n",
    "best_model = results[best_classifier]['model']\n",
    "if best_classifier in ['Logistic Regression', 'SVM (RBF)', 'K-Nearest Neighbors', 'Neural Network']:\n",
    " X_test_use = X_test_scaled\n",
    "else:\n",
    " X_test_use = X_test\n",
    "\n",
    "y_score = best_model.predict_proba(X_test_use)\n",
    "\n",
    "# Compute ROC curve for each class\n",
    "for i in range(n_classes):\n",
    " fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    " roc_auc = auc(fpr, tpr)\n",
    "\n",
    " fig.add_trace(\n",
    " go.Scatter(\n",
    " x=fpr,\n",
    " y=tpr,\n",
    " mode='lines',\n",
    " name=f'Class {class_names[i]} (AUC = {roc_auc:.2f})',\n",
    " line=dict(width=2)\n",
    " ),\n",
    " row=3, col=1\n",
    " )\n",
    "\n",
    "# Add diagonal line\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=[0, 1],\n",
    " y=[0, 1],\n",
    " mode='lines',\n",
    " line=dict(dash='dash', color='black'),\n",
    " name='Random Classifier',\n",
    " showlegend=False\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Decision boundary visualization (2D projection using first 2 features)\n",
    "# Create a mesh for decision boundary\n",
    "X_subset = X_test[:, :2] # Use first 2 features for 2D visualization\n",
    "h = 0.02\n",
    "x_min, x_max = X_subset[:, 0].min() - 1, X_subset[:, 0].max() + 1\n",
    "y_min, y_max = X_subset[:, 1].min() - 1, X_subset[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Use simplified model for boundary visualization\n",
    "simple_rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "X_train_2d = X_train[:, :2]\n",
    "simple_rf.fit(X_train_2d, y_train)\n",
    "\n",
    "# Get predictions for mesh\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = simple_rf.predict(mesh_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Add contour plot\n",
    "fig.add_trace(\n",
    " go.Contour(\n",
    " x=np.arange(x_min, x_max, h),\n",
    " y=np.arange(y_min, y_max, h),\n",
    " z=Z,\n",
    " colorscale='Viridis',\n",
    " opacity=0.3,\n",
    " showscale=False,\n",
    " contours=dict(showlabels=True)\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "# Add scatter plot of actual data points\n",
    "colors_map = {0: 'red', 1: 'blue', 2: 'green', 3: 'purple'}\n",
    "for class_val in [0, 1, 2, 3]:\n",
    " mask = y_test == class_val\n",
    " fig.add_trace(\n",
    " go.Scatter(\n",
    " x=X_subset[mask, 0],\n",
    " y=X_subset[mask, 1],\n",
    " mode='markers',\n",
    " name=class_names[class_val],\n",
    " marker=dict(color=colors_map[class_val], size=6, opacity=0.8),\n",
    " showlegend=False\n",
    " ),\n",
    " row=3, col=2\n",
    " )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    " height=1200,\n",
    " title=\"Advanced Classification Methods - Comprehensive Analysis Dashboard\",\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Classifier\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Classifier\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Feature Importance\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Predicted Class\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"False Positive Rate\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"Feature 1\", row=3, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Score\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Cross-Validation Score\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Feature\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Actual Class\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"True Positive Rate\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Feature 2\", row=3, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c87d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ADVANCED HYPERPARAMETER OPTIMIZATION\n",
    "print(\" 3. ADVANCED HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Hyperparameter optimization for top 3 performers\n",
    "top_performers = results_df.nlargest(3, 'F1-Score').index.tolist()\n",
    "\n",
    "print(f\"Optimizing hyperparameters for top 3 performers: {top_performers}\")\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    " 'Random Forest': {\n",
    " 'n_estimators': [50, 100, 200],\n",
    " 'max_depth': [5, 10, 15, None],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'min_samples_leaf': [1, 2, 4]\n",
    " },\n",
    " 'Gradient Boosting': {\n",
    " 'n_estimators': [50, 100, 150],\n",
    " 'learning_rate': [0.05, 0.1, 0.15],\n",
    " 'max_depth': [3, 5, 7],\n",
    " 'subsample': [0.8, 0.9, 1.0]\n",
    " },\n",
    " 'SVM (RBF)': {\n",
    " 'C': [0.1, 1, 10, 100],\n",
    " 'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    " 'kernel': ['rbf', 'poly']\n",
    " },\n",
    " 'Logistic Regression': {\n",
    " 'C': [0.1, 1, 10, 100],\n",
    " 'penalty': ['l1', 'l2', 'elasticnet'],\n",
    " 'solver': ['liblinear', 'saga'],\n",
    " 'max_iter': [1000, 2000]\n",
    " }\n",
    "}\n",
    "\n",
    "optimized_results = {}\n",
    "\n",
    "for classifier_name in top_performers:\n",
    " if classifier_name in param_grids:\n",
    " print(f\"\\nOptimizing {classifier_name}...\")\n",
    "\n",
    " # Get base classifier\n",
    " if classifier_name == 'Random Forest':\n",
    " base_clf = RandomForestClassifier(random_state=42)\n",
    " X_use = X_train\n",
    " elif classifier_name == 'Gradient Boosting':\n",
    " base_clf = GradientBoostingClassifier(random_state=42)\n",
    " X_use = X_train\n",
    " elif classifier_name == 'SVM (RBF)':\n",
    " base_clf = SVC(random_state=42, probability=True)\n",
    " X_use = X_train_scaled\n",
    " elif classifier_name == 'Logistic Regression':\n",
    " base_clf = LogisticRegression(random_state=42)\n",
    " X_use = X_train_scaled\n",
    "\n",
    " # Perform grid search\n",
    " grid_search = GridSearchCV(\n",
    " base_clf,\n",
    " param_grids[classifier_name],\n",
    " cv=3, # Reduced for speed\n",
    " scoring='f1_weighted',\n",
    " n_jobs=-1,\n",
    " verbose=0\n",
    " )\n",
    "\n",
    " grid_search.fit(X_use, y_train)\n",
    "\n",
    " # Get best model and evaluate\n",
    " best_model = grid_search.best_estimator_\n",
    "\n",
    " # Test predictions\n",
    " if classifier_name in ['SVM (RBF)', 'Logistic Regression']:\n",
    " X_test_use = X_test_scaled\n",
    " else:\n",
    " X_test_use = X_test\n",
    "\n",
    " y_pred_optimized = best_model.predict(X_test_use)\n",
    "\n",
    " # Calculate improved metrics\n",
    " accuracy_opt = accuracy_score(y_test, y_pred_optimized)\n",
    " f1_opt = f1_score(y_test, y_pred_optimized, average='weighted')\n",
    "\n",
    " optimized_results[classifier_name] = {\n",
    " 'best_params': grid_search.best_params_,\n",
    " 'best_score': grid_search.best_score_,\n",
    " 'test_accuracy': accuracy_opt,\n",
    " 'test_f1': f1_opt,\n",
    " 'improvement': f1_opt - results[classifier_name]['f1_score']\n",
    " }\n",
    "\n",
    " print(f\" Best parameters: {grid_search.best_params_}\")\n",
    " print(f\" Cross-validation score: {grid_search.best_score_:.3f}\")\n",
    " print(f\" Test F1 improvement: {optimized_results[classifier_name]['improvement']:+.3f}\")\n",
    "\n",
    "# Business insights and ROI calculation\n",
    "print(f\"\\n BUSINESS INSIGHTS AND ROI ANALYSIS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simulate business metrics\n",
    "total_customers = 10000\n",
    "classification_accuracy = results_df.loc[best_classifier, 'Accuracy']\n",
    "\n",
    "# Business impact scenarios\n",
    "scenarios = {\n",
    " 'Customer Retention': {\n",
    " 'base_retention_rate': 0.85,\n",
    " 'improved_retention': 0.85 + (classification_accuracy - 0.8) * 0.1,\n",
    " 'avg_customer_value': 2500,\n",
    " 'description': 'Improved churn prediction accuracy'\n",
    " },\n",
    " 'Marketing Efficiency': {\n",
    " 'base_conversion': 0.15,\n",
    " 'improved_conversion': 0.15 + (classification_accuracy - 0.8) * 0.05,\n",
    " 'marketing_budget': 500000,\n",
    " 'description': 'Better targeting through customer classification'\n",
    " },\n",
    " 'Fraud Detection': {\n",
    " 'fraud_rate': 0.02,\n",
    " 'detection_accuracy': classification_accuracy,\n",
    " 'avg_fraud_loss': 1500,\n",
    " 'description': 'Reduced fraud losses through better detection'\n",
    " }\n",
    "}\n",
    "\n",
    "total_annual_benefits = 0\n",
    "\n",
    "for scenario_name, metrics in scenarios.items():\n",
    " print(f\"\\n{scenario_name}:\")\n",
    "\n",
    " if scenario_name == 'Customer Retention':\n",
    " base_retained = total_customers * metrics['base_retention_rate']\n",
    " improved_retained = total_customers * metrics['improved_retention']\n",
    " additional_retained = improved_retained - base_retained\n",
    " value_improvement = additional_retained * metrics['avg_customer_value']\n",
    "\n",
    " print(f\" • Base retention: {metrics['base_retention_rate']*100:.1f}%\")\n",
    " print(f\" • Improved retention: {metrics['improved_retention']*100:.1f}%\")\n",
    " print(f\" • Additional customers retained: {additional_retained:.0f}\")\n",
    " print(f\" • Annual value improvement: ${value_improvement:,.0f}\")\n",
    " total_annual_benefits += value_improvement\n",
    "\n",
    " elif scenario_name == 'Marketing Efficiency':\n",
    " base_conversions = metrics['marketing_budget'] / 100 * metrics['base_conversion']\n",
    " improved_conversions = metrics['marketing_budget'] / 100 * metrics['improved_conversion']\n",
    " additional_conversions = improved_conversions - base_conversions\n",
    " value_improvement = additional_conversions * 150 # Avg profit per conversion\n",
    "\n",
    " print(f\" • Base conversion rate: {metrics['base_conversion']*100:.1f}%\")\n",
    " print(f\" • Improved conversion rate: {metrics['improved_conversion']*100:.1f}%\")\n",
    " print(f\" • Additional conversions: {additional_conversions:.0f}\")\n",
    " print(f\" • Annual value improvement: ${value_improvement:,.0f}\")\n",
    " total_annual_benefits += value_improvement\n",
    "\n",
    " elif scenario_name == 'Fraud Detection':\n",
    " total_fraud_cases = total_customers * metrics['fraud_rate']\n",
    " detected_fraud = total_fraud_cases * metrics['detection_accuracy']\n",
    " prevented_losses = detected_fraud * metrics['avg_fraud_loss']\n",
    "\n",
    " print(f\" • Total fraud cases: {total_fraud_cases:.0f}\")\n",
    " print(f\" • Detection accuracy: {metrics['detection_accuracy']*100:.1f}%\")\n",
    " print(f\" • Cases detected: {detected_fraud:.0f}\")\n",
    " print(f\" • Prevented losses: ${prevented_losses:,.0f}\")\n",
    " total_annual_benefits += prevented_losses\n",
    "\n",
    "# ROI calculation\n",
    "implementation_cost = 300_000 # Initial development and deployment\n",
    "annual_operational_cost = 75_000 # Maintenance and monitoring\n",
    "net_annual_benefits = total_annual_benefits - annual_operational_cost\n",
    "roi = (net_annual_benefits - implementation_cost) / implementation_cost\n",
    "\n",
    "print(f\"\\n CLASSIFICATION SYSTEM ROI:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"• Total annual benefits: ${total_annual_benefits:,.0f}\")\n",
    "print(f\"• Implementation cost: ${implementation_cost:,.0f}\")\n",
    "print(f\"• Annual operational cost: ${annual_operational_cost:,.0f}\")\n",
    "print(f\"• Net annual benefits: ${net_annual_benefits:,.0f}\")\n",
    "print(f\"• ROI: {roi*100:.0f}%\")\n",
    "print(f\"• Payback period: {implementation_cost/net_annual_benefits*12:.1f} months\")\n",
    "\n",
    "print(f\"\\n Cross-Reference Learning Path:\")\n",
    "print(f\"• Foundation: Tier2_LogisticRegression.ipynb (basic classification)\")\n",
    "print(f\"• Building Blocks: Tier2_DecisionTree.ipynb, Tier2_RandomForest.ipynb\")\n",
    "print(f\"• Advanced Methods: Tier5_SVM.ipynb, Tier5_NeuralNetworks.ipynb\")\n",
    "print(f\"• Specialized: Advanced_EnsembleClassification.ipynb, Advanced_ImbalancedLearning.ipynb\")\n",
    "print(f\"• Complete Guide: CROSS_REFERENCE_GUIDE.md\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}