{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 5: Gradient Boosting Classification\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** cd8c8259-f6c0-41ca-984d-c071e0066ad5\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 5: Gradient Boosting Classification,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** cd8c8259-f6c0-41ca-984d-c071e0066ad5\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ba5dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tier 5: Gradient Boosting Classification - Libraries Loaded!\n",
      "==============================================================\n",
      "Gradient Boosting Classification Techniques:\n",
      "• Gradient Boosting Classifier (scikit-learn)\n",
      "• AdaBoost (Adaptive Boosting)\n",
      "• XGBoost: Available\n",
      "• LightGBM: Available\n",
      "• CatBoost: Available\n",
      "• Learning rate optimization and regularization\n",
      "• Early stopping and cross-validation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, validation_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import advanced boosting libraries\n",
    "try:\n",
    " import xgboost as xgb\n",
    " XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    " XGBOOST_AVAILABLE = False\n",
    "\n",
    "try:\n",
    " import lightgbm as lgb\n",
    " LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    " LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    " import catboost as cb\n",
    " CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    " CATBOOST_AVAILABLE = False\n",
    "\n",
    "print(\" Tier 5: Gradient Boosting Classification - Libraries Loaded!\")\n",
    "print(\"=\"*62)\n",
    "print(\"Gradient Boosting Classification Techniques:\")\n",
    "print(\"• Gradient Boosting Classifier (scikit-learn)\")\n",
    "print(\"• AdaBoost (Adaptive Boosting)\")\n",
    "print(f\"• XGBoost: {'Available' if XGBOOST_AVAILABLE else 'Not Available'}\")\n",
    "print(f\"• LightGBM: {'Available' if LIGHTGBM_AVAILABLE else 'Not Available'}\")\n",
    "print(f\"• CatBoost: {'Available' if CATBOOST_AVAILABLE else 'Not Available'}\")\n",
    "print(\"• Learning rate optimization and regularization\")\n",
    "print(\"• Early stopping and cross-validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ee99ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gradient Boosting Datasets Created:\n",
      "Financial risk assessment: (5000, 13)\n",
      "Risk distribution: {'Low': 1999, 'Medium': 1484, 'High': 1019, 'Very_High': 498}\n",
      "\n",
      "E-commerce customers: (4000, 12)\n",
      "Customer type distribution: {'Casual': 2028, 'Regular': 1172, 'Premium': 800}\n",
      "\n",
      "Financial Dataset - Credit Score by Risk Level:\n",
      " Low: 748 ± 50\n",
      " Medium: 682 ± 40\n",
      " High: 621 ± 34\n",
      " Very_High: 552 ± 50\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive boosting datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Financial risk assessment dataset\n",
    "def generate_financial_dataset(n_samples=5000):\n",
    "    \"\"\"Generate realistic financial risk dataset.\"\"\"\n",
    "\n",
    "    data = []\n",
    "    risk_levels = ['Low', 'Medium', 'High', 'Very_High']\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Risk level assignment\n",
    "        risk_idx = np.random.choice(4, p=[0.4, 0.3, 0.2, 0.1])\n",
    "        risk_level = risk_levels[risk_idx]\n",
    "\n",
    "        # Generate features based on risk level\n",
    "        if risk_level == 'Low':\n",
    "            credit_score = np.random.normal(750, 50)\n",
    "            debt_to_income = np.random.beta(2, 8) * 0.5 # Lower DTI\n",
    "            payment_history = np.random.beta(9, 1) # Excellent history\n",
    "            account_age = np.random.exponential(5) + 2 # Longer history\n",
    "            income = np.random.lognormal(11, 0.3) # Higher income\n",
    "\n",
    "        elif risk_level == 'Medium':\n",
    "            credit_score = np.random.normal(680, 40)\n",
    "            debt_to_income = np.random.beta(3, 5) * 0.6\n",
    "            payment_history = np.random.beta(7, 2)\n",
    "            account_age = np.random.exponential(3) + 1\n",
    "            income = np.random.lognormal(10.5, 0.4)\n",
    "\n",
    "        elif risk_level == 'High':\n",
    "            credit_score = np.random.normal(620, 35)\n",
    "            debt_to_income = np.random.beta(5, 3) * 0.8\n",
    "            payment_history = np.random.beta(5, 4)\n",
    "            account_age = np.random.exponential(2) + 0.5\n",
    "            income = np.random.lognormal(10, 0.5)\n",
    "\n",
    "        else: # Very_High\n",
    "            credit_score = np.random.normal(550, 50)\n",
    "            debt_to_income = np.random.beta(8, 2) * 1.0\n",
    "            payment_history = np.random.beta(3, 7)\n",
    "            account_age = np.random.exponential(1) + 0.1\n",
    "            income = np.random.lognormal(9.5, 0.6)\n",
    "\n",
    "        # Additional features\n",
    "        data.append({\n",
    "            'customer_id': f'CUST_{i:06d}',\n",
    "            'credit_score': max(300, min(850, credit_score)), # Clamp to valid range\n",
    "            'debt_to_income_ratio': min(1.5, debt_to_income), # Cap at 150%\n",
    "            'payment_history_score': payment_history,\n",
    "            'account_age_years': account_age,\n",
    "            'annual_income': income,\n",
    "            'number_of_accounts': np.random.poisson(5) + 1,\n",
    "            'recent_inquiries': np.random.poisson(1),\n",
    "            'utilization_ratio': np.random.beta(2, 3),\n",
    "            'employment_length': np.random.exponential(3) + 0.5,\n",
    "            'homeowner': np.random.choice([0, 1], p=[0.7, 0.3]),\n",
    "            'risk_level': risk_level,\n",
    "            'risk_code': risk_idx\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# 2. E-commerce customer dataset\n",
    "def generate_ecommerce_dataset(n_samples=4000):\n",
    "    \"\"\"Generate e-commerce customer behavior dataset.\"\"\"\n",
    "\n",
    "    data = []\n",
    "    customer_types = ['Casual', 'Regular', 'Premium']\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Customer type assignment\n",
    "        type_idx = np.random.choice(3, p=[0.5, 0.3, 0.2])\n",
    "        customer_type = customer_types[type_idx]\n",
    "\n",
    "        # Generate features based on customer type\n",
    "        if customer_type == 'Casual':\n",
    "            monthly_purchases = np.random.poisson(1) + 1\n",
    "            avg_order_value = np.random.lognormal(3, 0.8) # ~$25\n",
    "            session_duration = np.random.exponential(5)\n",
    "            page_views = np.random.poisson(8)\n",
    "\n",
    "        elif customer_type == 'Regular':\n",
    "            monthly_purchases = np.random.poisson(3) + 2\n",
    "            avg_order_value = np.random.lognormal(4, 0.6) # ~$75\n",
    "            session_duration = np.random.exponential(12)\n",
    "            page_views = np.random.poisson(15)\n",
    "\n",
    "        else: # Premium\n",
    "            monthly_purchases = np.random.poisson(6) + 4\n",
    "            avg_order_value = np.random.lognormal(5, 0.5) # ~$200\n",
    "            session_duration = np.random.exponential(20)\n",
    "            page_views = np.random.poisson(25)\n",
    "\n",
    "        data.append({\n",
    "            'customer_id': f'ECOM_{i:06d}',\n",
    "            'monthly_purchases': monthly_purchases,\n",
    "            'avg_order_value': avg_order_value,\n",
    "            'session_duration_min': session_duration,\n",
    "            'page_views_per_session': page_views,\n",
    "            'cart_abandonment_rate': np.random.beta(3, 5),\n",
    "            'time_on_site_total': session_duration * monthly_purchases,\n",
    "            'mobile_usage_ratio': np.random.beta(4, 3),\n",
    "            'email_open_rate': np.random.beta(3, 4),\n",
    "            'social_media_referrals': np.random.poisson(2),\n",
    "            'customer_type': customer_type,\n",
    "            'type_code': type_idx\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate datasets\n",
    "financial_df = generate_financial_dataset()\n",
    "ecommerce_df = generate_ecommerce_dataset()\n",
    "\n",
    "print(\" Gradient Boosting Datasets Created:\")\n",
    "print(f\"Financial risk assessment: {financial_df.shape}\")\n",
    "print(f\"Risk distribution: {financial_df['risk_level'].value_counts().to_dict()}\")\n",
    "print(f\"\\nE-commerce customers: {ecommerce_df.shape}\")\n",
    "print(f\"Customer type distribution: {ecommerce_df['customer_type'].value_counts().to_dict()}\")\n",
    "\n",
    "# Show sample statistics\n",
    "print(f\"\\nFinancial Dataset - Credit Score by Risk Level:\")\n",
    "risk_stats = financial_df.groupby('risk_level')['credit_score'].agg(['mean', 'std'])\n",
    "for risk_level in ['Low', 'Medium', 'High', 'Very_High']:\n",
    "    if risk_level in risk_stats.index:\n",
    "        mean_score = risk_stats.loc[risk_level, 'mean']\n",
    "        std_score = risk_stats.loc[risk_level, 'std']\n",
    "        print(f\" {risk_level}: {mean_score:.0f} ± {std_score:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04bf75a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. GRADIENT BOOSTING ALGORITHM COMPARISON\n",
      "============================================\n",
      "Training boosting algorithms...\n",
      "Training Gradient Boosting...\n",
      " Accuracy: 0.926 | CV: 0.920 ± 0.010\n",
      "Training AdaBoost...\n",
      " Accuracy: 0.926 | CV: 0.920 ± 0.010\n",
      "Training AdaBoost...\n",
      " Accuracy: 0.791 | CV: 0.832 ± 0.016\n",
      "Training XGBoost...\n",
      " Accuracy: 0.791 | CV: 0.832 ± 0.016\n",
      "Training XGBoost...\n",
      " Accuracy: 0.929 | CV: 0.918 ± 0.008\n",
      "Training LightGBM...\n",
      " Accuracy: 0.929 | CV: 0.918 ± 0.008\n",
      "Training LightGBM...\n",
      " Accuracy: 0.930 | CV: 0.916 ± 0.009\n",
      "Training CatBoost...\n",
      " Accuracy: 0.930 | CV: 0.916 ± 0.009\n",
      "Training CatBoost...\n",
      " Accuracy: 0.927 | CV: 0.923 ± 0.010\n",
      "\n",
      "Boosting Algorithm Performance Comparison:\n",
      "Algorithm Accuracy CV Mean CV Std\n",
      "--------------------------------------------------\n",
      "Gradient Boosting 0.926 0.920 0.010\n",
      "AdaBoost        0.791 0.832 0.016\n",
      "XGBoost         0.929 0.918 0.008\n",
      "LightGBM        0.930 0.916 0.009\n",
      "CatBoost        0.927 0.923 0.010\n",
      "\n",
      "Best performing algorithm: LightGBM (Accuracy: 0.930)\n",
      " Accuracy: 0.927 | CV: 0.923 ± 0.010\n",
      "\n",
      "Boosting Algorithm Performance Comparison:\n",
      "Algorithm Accuracy CV Mean CV Std\n",
      "--------------------------------------------------\n",
      "Gradient Boosting 0.926 0.920 0.010\n",
      "AdaBoost        0.791 0.832 0.016\n",
      "XGBoost         0.929 0.918 0.008\n",
      "LightGBM        0.930 0.916 0.009\n",
      "CatBoost        0.927 0.923 0.010\n",
      "\n",
      "Best performing algorithm: LightGBM (Accuracy: 0.930)\n"
     ]
    }
   ],
   "source": [
    "# 1. GRADIENT BOOSTING ALGORITHM COMPARISON\n",
    "print(\" 1. GRADIENT BOOSTING ALGORITHM COMPARISON\")\n",
    "print(\"=\"*44)\n",
    "\n",
    "# Prepare financial data\n",
    "financial_features = [col for col in financial_df.columns\n",
    " if col not in ['customer_id', 'risk_level', 'risk_code']]\n",
    "X_financial = financial_df[financial_features].values\n",
    "y_financial = financial_df['risk_code'].values\n",
    "\n",
    "# Split the data\n",
    "X_fin_train, X_fin_test, y_fin_train, y_fin_test = train_test_split(\n",
    " X_financial, y_financial, test_size=0.2, random_state=42, stratify=y_financial\n",
    ")\n",
    "\n",
    "# Initialize boosting algorithms\n",
    "boosting_algorithms = {}\n",
    "results = {}\n",
    "\n",
    "# 1. Scikit-learn Gradient Boosting\n",
    "gb_sklearn = GradientBoostingClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=0.1,\n",
    " max_depth=6,\n",
    " random_state=42\n",
    ")\n",
    "boosting_algorithms['Gradient Boosting'] = gb_sklearn\n",
    "\n",
    "# 2. AdaBoost\n",
    "ada_boost = AdaBoostClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=1.0,\n",
    " random_state=42\n",
    ")\n",
    "boosting_algorithms['AdaBoost'] = ada_boost\n",
    "\n",
    "# 3. XGBoost (if available)\n",
    "if XGBOOST_AVAILABLE:\n",
    " xgb_classifier = xgb.XGBClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=0.1,\n",
    " max_depth=6,\n",
    " random_state=42,\n",
    " eval_metric='mlogloss'\n",
    " )\n",
    " boosting_algorithms['XGBoost'] = xgb_classifier\n",
    "\n",
    "# 4. LightGBM (if available)\n",
    "if LIGHTGBM_AVAILABLE:\n",
    " lgb_classifier = lgb.LGBMClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=0.1,\n",
    " max_depth=6,\n",
    " random_state=42,\n",
    " verbose=-1\n",
    " )\n",
    " boosting_algorithms['LightGBM'] = lgb_classifier\n",
    "\n",
    "# 5. CatBoost (if available)\n",
    "if CATBOOST_AVAILABLE:\n",
    " cat_classifier = cb.CatBoostClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=0.1,\n",
    " max_depth=6,\n",
    " random_state=42,\n",
    " verbose=False\n",
    " )\n",
    " boosting_algorithms['CatBoost'] = cat_classifier\n",
    "\n",
    "# Train and evaluate all algorithms\n",
    "print(\"Training boosting algorithms...\")\n",
    "for name, algorithm in boosting_algorithms.items():\n",
    "    print(f\"Training {name}...\")\n",
    "\n",
    "    # Train the algorithm\n",
    "    algorithm.fit(X_fin_train, y_fin_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = algorithm.predict(X_fin_test)\n",
    "    y_pred_proba = algorithm.predict_proba(X_fin_test)\n",
    "\n",
    "    # Performance metrics\n",
    "    accuracy = accuracy_score(y_fin_test, y_pred)\n",
    "    cv_scores = cross_val_score(algorithm, X_fin_train, y_fin_train, cv=3)\n",
    "\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'model': algorithm\n",
    "    }\n",
    "\n",
    "    print(f\" Accuracy: {accuracy:.3f} | CV: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\nBoosting Algorithm Performance Comparison:\")\n",
    "print(\"Algorithm Accuracy CV Mean CV Std\")\n",
    "print(\"-\" * 50)\n",
    "for name, metrics in results.items():\n",
    " print(f\"{name:<15} {metrics['accuracy']:.3f} {metrics['cv_mean']:.3f} {metrics['cv_std']:.3f}\")\n",
    "\n",
    "# Find best performer\n",
    "best_algorithm = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "best_accuracy = results[best_algorithm]['accuracy']\n",
    "print(f\"\\nBest performing algorithm: {best_algorithm} (Accuracy: {best_accuracy:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11c8f603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2. HYPERPARAMETER OPTIMIZATION AND LEARNING CURVES\n",
      "=======================================================\n",
      "Detailed Gradient Boosting Analysis:\n",
      "\n",
      "Learning Rate Optimization:\n",
      "Learning Rate 0.01: 0.906 ± 0.013\n",
      "Learning Rate 0.01: 0.906 ± 0.013\n",
      "Learning Rate 0.05: 0.917 ± 0.012\n",
      "Learning Rate 0.05: 0.917 ± 0.012\n",
      "Learning Rate 0.10: 0.920 ± 0.010\n",
      "Learning Rate 0.10: 0.920 ± 0.010\n",
      "Learning Rate 0.20: 0.920 ± 0.010\n",
      "Learning Rate 0.20: 0.920 ± 0.010\n",
      "Learning Rate 0.30: 0.923 ± 0.008\n",
      "Best learning rate: 0.3 (Score: 0.923)\n",
      "\n",
      "Max Depth Optimization:\n",
      "Learning Rate 0.30: 0.923 ± 0.008\n",
      "Best learning rate: 0.3 (Score: 0.923)\n",
      "\n",
      "Max Depth Optimization:\n",
      "Max Depth 3: 0.920 ± 0.008\n",
      "Max Depth 3: 0.920 ± 0.008\n",
      "Max Depth 4: 0.920 ± 0.009\n",
      "Max Depth 4: 0.920 ± 0.009\n",
      "Max Depth 5: 0.918 ± 0.012\n",
      "Max Depth 5: 0.918 ± 0.012\n",
      "Max Depth 6: 0.923 ± 0.008\n",
      "Max Depth 6: 0.923 ± 0.008\n",
      "Max Depth 7: 0.920 ± 0.009\n",
      "Max Depth 7: 0.920 ± 0.009\n",
      "Max Depth 8: 0.917 ± 0.011\n",
      "Best max depth: 6 (Score: 0.923)\n",
      "\n",
      "N_estimators Analysis:\n",
      "Max Depth 8: 0.917 ± 0.011\n",
      "Best max depth: 6 (Score: 0.923)\n",
      "\n",
      "N_estimators Analysis:\n",
      "N_estimators  25: Train=1.000, Test=0.927\n",
      "N_estimators  25: Train=1.000, Test=0.927\n",
      "N_estimators  50: Train=1.000, Test=0.929\n",
      "N_estimators  50: Train=1.000, Test=0.929\n",
      "N_estimators  75: Train=1.000, Test=0.934\n",
      "N_estimators  75: Train=1.000, Test=0.934\n",
      "N_estimators 100: Train=1.000, Test=0.933\n",
      "N_estimators 100: Train=1.000, Test=0.933\n",
      "N_estimators 150: Train=1.000, Test=0.933\n",
      "N_estimators 150: Train=1.000, Test=0.933\n",
      "N_estimators 200: Train=1.000, Test=0.934\n",
      "N_estimators 200: Train=1.000, Test=0.934\n",
      "N_estimators 300: Train=1.000, Test=0.933\n",
      "Best n_estimators: 75 (Test Score: 0.934)\n",
      "N_estimators 300: Train=1.000, Test=0.933\n",
      "Best n_estimators: 75 (Test Score: 0.934)\n",
      "\n",
      "Optimized Gradient Boosting Performance:\n",
      "Final accuracy: 0.934\n",
      "Improvement over baseline: +0.008\n",
      "\n",
      "Optimized Gradient Boosting Performance:\n",
      "Final accuracy: 0.934\n",
      "Improvement over baseline: +0.008\n"
     ]
    }
   ],
   "source": [
    "# 2. HYPERPARAMETER OPTIMIZATION AND LEARNING CURVES\n",
    "print(\" 2. HYPERPARAMETER OPTIMIZATION AND LEARNING CURVES\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Focus on Gradient Boosting for detailed analysis\n",
    "print(\"Detailed Gradient Boosting Analysis:\")\n",
    "\n",
    "# Learning rate analysis\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "lr_scores = []\n",
    "\n",
    "print(\"\\nLearning Rate Optimization:\")\n",
    "for lr in learning_rates:\n",
    "    gb_temp = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=lr,\n",
    "        max_depth=6,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    cv_scores = cross_val_score(gb_temp, X_fin_train, y_fin_train, cv=3)\n",
    "    lr_scores.append(cv_scores.mean())\n",
    "\n",
    "    print(f\"Learning Rate {lr:.2f}: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "best_lr = learning_rates[np.argmax(lr_scores)]\n",
    "print(f\"Best learning rate: {best_lr} (Score: {max(lr_scores):.3f})\")\n",
    "\n",
    "# Max depth analysis\n",
    "max_depths = [3, 4, 5, 6, 7, 8]\n",
    "depth_scores = []\n",
    "\n",
    "print(f\"\\nMax Depth Optimization:\")\n",
    "for depth in max_depths:\n",
    "    gb_temp = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=best_lr,\n",
    "        max_depth=depth,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    cv_scores = cross_val_score(gb_temp, X_fin_train, y_fin_train, cv=3)\n",
    "    depth_scores.append(cv_scores.mean())\n",
    "\n",
    "    print(f\"Max Depth {depth}: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "best_depth = max_depths[np.argmax(depth_scores)]\n",
    "print(f\"Best max depth: {best_depth} (Score: {max(depth_scores):.3f})\")\n",
    "\n",
    "# N_estimators analysis with early stopping simulation\n",
    "n_estimators_range = [25, 50, 75, 100, 150, 200, 300]\n",
    "train_scores_est = []\n",
    "test_scores_est = []\n",
    "\n",
    "print(f\"\\nN_estimators Analysis:\")\n",
    "for n_est in n_estimators_range:\n",
    "    gb_temp = GradientBoostingClassifier(\n",
    "        n_estimators=n_est,\n",
    "        learning_rate=best_lr,\n",
    "        max_depth=best_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    gb_temp.fit(X_fin_train, y_fin_train)\n",
    "\n",
    "    train_score = gb_temp.score(X_fin_train, y_fin_train)\n",
    "    test_score = gb_temp.score(X_fin_test, y_fin_test)\n",
    "\n",
    "    train_scores_est.append(train_score)\n",
    "    test_scores_est.append(test_score)\n",
    "\n",
    "    print(f\"N_estimators {n_est:3d}: Train={train_score:.3f}, Test={test_score:.3f}\")\n",
    "\n",
    "# Find optimal n_estimators (where test score peaks)\n",
    "best_n_est_idx = np.argmax(test_scores_est)\n",
    "best_n_est = n_estimators_range[best_n_est_idx]\n",
    "print(f\"Best n_estimators: {best_n_est} (Test Score: {test_scores_est[best_n_est_idx]:.3f})\")\n",
    "\n",
    "# Train final optimized model\n",
    "gb_optimized = GradientBoostingClassifier(\n",
    " n_estimators=best_n_est,\n",
    " learning_rate=best_lr,\n",
    " max_depth=best_depth,\n",
    " random_state=42\n",
    ")\n",
    "\n",
    "gb_optimized.fit(X_fin_train, y_fin_train)\n",
    "optimized_accuracy = gb_optimized.score(X_fin_test, y_fin_test)\n",
    "\n",
    "print(f\"\\nOptimized Gradient Boosting Performance:\")\n",
    "print(f\"Final accuracy: {optimized_accuracy:.3f}\")\n",
    "print(f\"Improvement over baseline: {optimized_accuracy - results['Gradient Boosting']['accuracy']:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e75295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3. FEATURE IMPORTANCE AND MODEL INTERPRETATION\n",
      "===================================================\n",
      "Feature Importance (Gradient Boosting):\n",
      " 1. debt_to_income_ratio     : 0.5355\n",
      " 2. credit_score             : 0.1679\n",
      " 3. account_age_years        : 0.0935\n",
      " 4. annual_income            : 0.0928\n",
      " 5. payment_history_score    : 0.0918\n",
      " 6. employment_length        : 0.0069\n",
      " 7. utilization_ratio        : 0.0057\n",
      " 8. number_of_accounts       : 0.0033\n",
      " 9. recent_inquiries         : 0.0014\n",
      "10. homeowner                : 0.0010\n",
      "\n",
      "Top 3 Feature Analysis:\n",
      "\n",
      "debt_to_income_ratio impact on risk prediction:\n",
      " 0.01 - 0.10: Avg risk level 0.13\n",
      " 0.10 - 0.21: Avg risk level 0.43\n",
      " 0.21 - 0.43: Avg risk level 1.08\n",
      " 0.43 - 0.98: Avg risk level 2.37\n",
      "\n",
      "credit_score impact on risk prediction:\n",
      " 394.83 - 631.33: Avg risk level 2.24\n",
      " 631.33 - 687.06: Avg risk level 1.17\n",
      " 687.06 - 739.85: Avg risk level 0.49\n",
      " 739.85 - 850.00: Avg risk level 0.11\n",
      "\n",
      "account_age_years impact on risk prediction:\n",
      " 0.10 - 1.75: Avg risk level 2.05\n",
      " 1.75 - 3.44: Avg risk level 1.00\n",
      " 3.44 - 6.28: Avg risk level 0.70\n",
      " 6.28 - 33.70: Avg risk level 0.27\n",
      "\n",
      "E-commerce Customer Classification:\n",
      "E-commerce classification accuracy: 0.939\n",
      "\n",
      "E-commerce Feature Importance:\n",
      " 1. monthly_purchases        : 0.5596\n",
      " 2. page_views_per_session   : 0.2544\n",
      " 3. avg_order_value          : 0.1113\n",
      " 4. time_on_site_total       : 0.0384\n",
      " 5. email_open_rate          : 0.0097\n",
      " 6. cart_abandonment_rate    : 0.0088\n",
      " 7. session_duration_min     : 0.0082\n",
      " 8. mobile_usage_ratio       : 0.0078\n",
      "\n",
      "Learning Curve Analysis:\n",
      "E-commerce classification accuracy: 0.939\n",
      "\n",
      "E-commerce Feature Importance:\n",
      " 1. monthly_purchases        : 0.5596\n",
      " 2. page_views_per_session   : 0.2544\n",
      " 3. avg_order_value          : 0.1113\n",
      " 4. time_on_site_total       : 0.0384\n",
      " 5. email_open_rate          : 0.0097\n",
      " 6. cart_abandonment_rate    : 0.0088\n",
      " 7. session_duration_min     : 0.0082\n",
      " 8. mobile_usage_ratio       : 0.0078\n",
      "\n",
      "Learning Curve Analysis:\n",
      "Training size  10%: Train=1.000, Test=0.906\n",
      "Training size  10%: Train=1.000, Test=0.906\n",
      "Training size  20%: Train=1.000, Test=0.905\n",
      "Training size  20%: Train=1.000, Test=0.905\n",
      "Training size  40%: Train=1.000, Test=0.918\n",
      "Training size  40%: Train=1.000, Test=0.918\n",
      "Training size  60%: Train=1.000, Test=0.915\n",
      "Training size  60%: Train=1.000, Test=0.915\n",
      "Training size  80%: Train=1.000, Test=0.930\n",
      "Training size  80%: Train=1.000, Test=0.930\n",
      "Training size 100%: Train=1.000, Test=0.934\n",
      "Training size 100%: Train=1.000, Test=0.934\n"
     ]
    }
   ],
   "source": [
    "# 3. FEATURE IMPORTANCE AND MODEL INTERPRETATION\n",
    "print(\" 3. FEATURE IMPORTANCE AND MODEL INTERPRETATION\")\n",
    "print(\"=\"*51)\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance_gb = pd.DataFrame({\n",
    " 'feature': financial_features,\n",
    " 'importance': gb_optimized.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Gradient Boosting):\")\n",
    "for i, (idx, row) in enumerate(feature_importance_gb.head(10).iterrows()):\n",
    " print(f\"{i+1:2d}. {row['feature']:<25}: {row['importance']:.4f}\")\n",
    "\n",
    "# Partial dependence analysis (simplified)\n",
    "print(f\"\\nTop 3 Feature Analysis:\")\n",
    "top_features = feature_importance_gb.head(3)['feature'].tolist()\n",
    "\n",
    "for feature in top_features:\n",
    "    feature_idx = financial_features.index(feature)\n",
    "    feature_values = X_fin_test[:, feature_idx]\n",
    "\n",
    "    # Calculate average prediction for different feature value ranges\n",
    "    percentiles = [0, 25, 50, 75, 100]\n",
    "    thresholds = np.percentile(feature_values, percentiles)\n",
    "\n",
    "    print(f\"\\n{feature} impact on risk prediction:\")\n",
    "    for i in range(len(thresholds) - 1):\n",
    "        mask = (feature_values >= thresholds[i]) & (feature_values < thresholds[i+1])\n",
    "        if np.sum(mask) > 0:\n",
    "            avg_risk = np.mean(y_fin_test[mask])\n",
    "            print(f\" {thresholds[i]:.2f} - {thresholds[i+1]:.2f}: Avg risk level {avg_risk:.2f}\")\n",
    "\n",
    "# E-commerce dataset analysis\n",
    "print(f\"\\nE-commerce Customer Classification:\")\n",
    "ecommerce_features = [col for col in ecommerce_df.columns\n",
    " if col not in ['customer_id', 'customer_type', 'type_code']]\n",
    "X_ecom = ecommerce_df[ecommerce_features].values\n",
    "y_ecom = ecommerce_df['type_code'].values\n",
    "\n",
    "X_ecom_train, X_ecom_test, y_ecom_train, y_ecom_test = train_test_split(\n",
    " X_ecom, y_ecom, test_size=0.2, random_state=42, stratify=y_ecom\n",
    ")\n",
    "\n",
    "# Train model for e-commerce data\n",
    "gb_ecommerce = GradientBoostingClassifier(\n",
    " n_estimators=best_n_est,\n",
    " learning_rate=best_lr,\n",
    " max_depth=best_depth,\n",
    " random_state=42\n",
    ")\n",
    "\n",
    "gb_ecommerce.fit(X_ecom_train, y_ecom_train)\n",
    "ecom_accuracy = gb_ecommerce.score(X_ecom_test, y_ecom_test)\n",
    "\n",
    "print(f\"E-commerce classification accuracy: {ecom_accuracy:.3f}\")\n",
    "\n",
    "# E-commerce feature importance\n",
    "ecom_feature_importance = pd.DataFrame({\n",
    " 'feature': ecommerce_features,\n",
    " 'importance': gb_ecommerce.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nE-commerce Feature Importance:\")\n",
    "for i, (idx, row) in enumerate(ecom_feature_importance.head(8).iterrows()):\n",
    " print(f\"{i+1:2d}. {row['feature']:<25}: {row['importance']:.4f}\")\n",
    "\n",
    "# Learning curve analysis\n",
    "print(f\"\\nLearning Curve Analysis:\")\n",
    "train_sizes = [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "lc_train_scores = []\n",
    "lc_test_scores = []\n",
    "\n",
    "for size in train_sizes:\n",
    "    n_samples = int(len(X_fin_train) * size)\n",
    "    X_subset = X_fin_train[:n_samples]\n",
    "    y_subset = y_fin_train[:n_samples]\n",
    "\n",
    "    gb_temp = GradientBoostingClassifier(\n",
    "        n_estimators=best_n_est,\n",
    "        learning_rate=best_lr,\n",
    "        max_depth=best_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    gb_temp.fit(X_subset, y_subset)\n",
    "\n",
    "    train_score = gb_temp.score(X_subset, y_subset)\n",
    "    test_score = gb_temp.score(X_fin_test, y_fin_test)\n",
    "\n",
    "    lc_train_scores.append(train_score)\n",
    "    lc_test_scores.append(test_score)\n",
    "\n",
    "    print(f\"Training size {size*100:3.0f}%: Train={train_score:.3f}, Test={test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf8e124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4. COMPREHENSIVE GRADIENT BOOSTING VISUALIZATION DASHBOARD\n",
      "==================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "lightblue"
         },
         "name": "Test Accuracy",
         "text": [
          "0.926",
          "0.791",
          "0.929",
          "0.930",
          "0.927"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          "Gradient Boosting",
          "AdaBoost",
          "XGBoost",
          "LightGBM",
          "CatBoost"
         ],
         "xaxis": "x",
         "y": [
          0.926,
          0.791,
          0.929,
          0.93,
          0.927
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "lightcoral"
         },
         "name": "CV Mean",
         "text": [
          "0.920",
          "0.832",
          "0.918",
          "0.916",
          "0.923"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          "Gradient Boosting",
          "AdaBoost",
          "XGBoost",
          "LightGBM",
          "CatBoost"
         ],
         "xaxis": "x",
         "y": [
          0.9199972406894826,
          0.8315017285055898,
          0.9184991150336309,
          0.9162491147524512,
          0.9229983657833499
         ],
         "yaxis": "y"
        },
        {
         "line": {
          "color": "blue",
          "width": 3
         },
         "marker": {
          "size": 8
         },
         "mode": "lines+markers",
         "name": "Learning Rate",
         "type": "scatter",
         "x": [
          0.01,
          0.05,
          0.1,
          0.2,
          0.3
         ],
         "xaxis": "x2",
         "y": [
          0.9064961142834433,
          0.9169970528614163,
          0.9199972406894826,
          0.9197477405333343,
          0.9227481158145608
         ],
         "yaxis": "y2"
        },
        {
         "marker": {
          "color": "red",
          "size": 12,
          "symbol": "star"
         },
         "mode": "markers",
         "name": "Best LR",
         "type": "scatter",
         "x": [
          0.3
         ],
         "xaxis": "x2",
         "y": [
          0.9227481158145608
         ],
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines+markers",
         "name": "Training Score",
         "type": "scatter",
         "x": [
          25,
          50,
          75,
          100,
          150,
          200,
          300
         ],
         "xaxis": "x3",
         "y": [
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "yaxis": "y3"
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines+markers",
         "name": "Test Score",
         "type": "scatter",
         "x": [
          25,
          50,
          75,
          100,
          150,
          200,
          300
         ],
         "xaxis": "x3",
         "y": [
          0.927,
          0.929,
          0.934,
          0.933,
          0.933,
          0.934,
          0.933
         ],
         "yaxis": "y3"
        },
        {
         "marker": {
          "color": "green"
         },
         "name": "Feature Importance",
         "orientation": "h",
         "type": "bar",
         "x": {
          "bdata": "NJiYWbsi4T+mQtYYnX3FP2aPkDR07bc/SOuS4ejDtz/b0Igd6oK3Py2j3+LsYXw/SwedWnyIdz882UiNsjBrP5xWY0X5tlc/Hq+zIvDmUD8=",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": [
          "debt_to_income_ratio",
          "credit_score",
          "account_age_years",
          "annual_income",
          "payment_history_score",
          "employment_length",
          "utilization_ratio",
          "number_of_accounts",
          "recent_inquiries",
          "homeowner"
         ],
         "yaxis": "y4"
        },
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines+markers",
         "name": "Training Score",
         "type": "scatter",
         "x": [
          10,
          20,
          40,
          60,
          80,
          100
         ],
         "xaxis": "x5",
         "y": [
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "yaxis": "y5"
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines+markers",
         "name": "Validation Score",
         "type": "scatter",
         "x": [
          10,
          20,
          40,
          60,
          80,
          100
         ],
         "xaxis": "x5",
         "y": [
          0.906,
          0.905,
          0.918,
          0.915,
          0.93,
          0.934
         ],
         "yaxis": "y5"
        },
        {
         "colorscale": [
          [
           0,
           "rgb(247,251,255)"
          ],
          [
           0.125,
           "rgb(222,235,247)"
          ],
          [
           0.25,
           "rgb(198,219,239)"
          ],
          [
           0.375,
           "rgb(158,202,225)"
          ],
          [
           0.5,
           "rgb(107,174,214)"
          ],
          [
           0.625,
           "rgb(66,146,198)"
          ],
          [
           0.75,
           "rgb(33,113,181)"
          ],
          [
           0.875,
           "rgb(8,81,156)"
          ],
          [
           1,
           "rgb(8,48,107)"
          ]
         ],
         "hovertemplate": "Predicted: %{x}<br>Actual: %{y}<br>Count: %{z}<extra></extra>",
         "text": {
          "bdata": "egEWAAAAAAAYAA4BAwAAAAAADQC/AAAAAAAAAAQAXwA=",
          "dtype": "i2",
          "shape": "4, 4"
         },
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          "Low",
          "Medium",
          "High",
          "Very_High"
         ],
         "xaxis": "x6",
         "y": [
          "Low",
          "Medium",
          "High",
          "Very_High"
         ],
         "yaxis": "y6",
         "z": {
          "bdata": "egEWAAAAAAAYAA4BAwAAAAAADQC/AAAAAAAAAAQAXwA=",
          "dtype": "i2",
          "shape": "4, 4"
         }
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Algorithm Performance Comparison",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Hyperparameter Optimization: Learning Rate",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "N_estimators vs Performance (Overfitting Analysis)",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.6111111111111112,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Feature Importance: Financial Risk Factors",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.6111111111111112,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Learning Curves: Training Size vs Performance",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.22222222222222224,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Confusion Matrix: Risk Level Prediction",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.22222222222222224,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 1200,
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Gradient Boosting Classification - Comprehensive Analysis Dashboard"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Algorithm"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "Learning Rate"
         }
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Number of Estimators"
         }
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "Feature Importance"
         }
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Training Set Size (%)"
         }
        },
        "xaxis6": {
         "anchor": "y6",
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "Predicted Risk Level"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.7777777777777778,
          1
         ],
         "title": {
          "text": "Accuracy"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.7777777777777778,
          1
         ],
         "title": {
          "text": "CV Score"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.3888888888888889,
          0.6111111111111112
         ],
         "title": {
          "text": "Accuracy"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.3888888888888889,
          0.6111111111111112
         ],
         "title": {
          "text": "Feature"
         }
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.22222222222222224
         ],
         "title": {
          "text": "Accuracy"
         }
        },
        "yaxis6": {
         "anchor": "x6",
         "domain": [
          0,
          0.22222222222222224
         ],
         "title": {
          "text": "Actual Risk Level"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. COMPREHENSIVE GRADIENT BOOSTING VISUALIZATION DASHBOARD\n",
    "print(\" 4. COMPREHENSIVE GRADIENT BOOSTING VISUALIZATION DASHBOARD\")\n",
    "print(\"=\"*66)\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "fig = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=[\n",
    " 'Algorithm Performance Comparison',\n",
    " 'Hyperparameter Optimization: Learning Rate',\n",
    " 'N_estimators vs Performance (Overfitting Analysis)',\n",
    " 'Feature Importance: Financial Risk Factors',\n",
    " 'Learning Curves: Training Size vs Performance',\n",
    " 'Confusion Matrix: Risk Level Prediction'\n",
    " ],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"type\": \"heatmap\"}]]\n",
    ")\n",
    "\n",
    "# 1. Algorithm comparison\n",
    "algorithms = list(results.keys())\n",
    "accuracies = [results[alg]['accuracy'] for alg in algorithms]\n",
    "cv_means = [results[alg]['cv_mean'] for alg in algorithms]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=algorithms,\n",
    " y=accuracies,\n",
    " name='Test Accuracy',\n",
    " marker_color='lightblue',\n",
    " text=[f'{acc:.3f}' for acc in accuracies],\n",
    " textposition='auto'\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=algorithms,\n",
    " y=cv_means,\n",
    " name='CV Mean',\n",
    " marker_color='lightcoral',\n",
    " text=[f'{cv:.3f}' for cv in cv_means],\n",
    " textposition='auto'\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Learning rate optimization\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=learning_rates,\n",
    " y=lr_scores,\n",
    " mode='lines+markers',\n",
    " name='Learning Rate',\n",
    " line=dict(color='blue', width=3),\n",
    " marker=dict(size=8)\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Highlight best learning rate\n",
    "best_lr_idx = np.argmax(lr_scores)\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=[learning_rates[best_lr_idx]],\n",
    " y=[lr_scores[best_lr_idx]],\n",
    " mode='markers',\n",
    " name='Best LR',\n",
    " marker=dict(color='red', size=12, symbol='star')\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. N_estimators analysis\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=train_scores_est,\n",
    " mode='lines+markers',\n",
    " name='Training Score',\n",
    " line=dict(color='blue')\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=test_scores_est,\n",
    " mode='lines+markers',\n",
    " name='Test Score',\n",
    " line=dict(color='red')\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Feature importance\n",
    "top_features_plot = feature_importance_gb.head(12)\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=top_features_plot['importance'],\n",
    " y=top_features_plot['feature'],\n",
    " orientation='h',\n",
    " marker_color='green',\n",
    " name='Feature Importance'\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "# 5. Learning curves\n",
    "training_sizes_pct = [size * 100 for size in train_sizes]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=training_sizes_pct,\n",
    " y=lc_train_scores,\n",
    " mode='lines+markers',\n",
    " name='Training Score',\n",
    " line=dict(color='blue')\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=training_sizes_pct,\n",
    " y=lc_test_scores,\n",
    " mode='lines+markers',\n",
    " name='Validation Score',\n",
    " line=dict(color='red')\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Confusion matrix\n",
    "y_pred_final = gb_optimized.predict(X_fin_test)\n",
    "cm_financial = confusion_matrix(y_fin_test, y_pred_final)\n",
    "risk_levels = ['Low', 'Medium', 'High', 'Very_High']\n",
    "\n",
    "fig.add_trace(\n",
    " go.Heatmap(\n",
    " z=cm_financial,\n",
    " x=risk_levels,\n",
    " y=risk_levels,\n",
    " colorscale='Blues',\n",
    " text=cm_financial,\n",
    " texttemplate='%{text}',\n",
    " hovertemplate='Predicted: %{x}<br>Actual: %{y}<br>Count: %{z}<extra></extra>'\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    " height=1200,\n",
    " title=\"Gradient Boosting Classification - Comprehensive Analysis Dashboard\",\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Algorithm\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Learning Rate\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Number of Estimators\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Feature Importance\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"Training Set Size (%)\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"Predicted Risk Level\", row=3, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"CV Score\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Feature\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Actual Risk Level\", row=3, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f3bbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5. BUSINESS INSIGHTS AND ROI ANALYSIS\n",
      "========================================\n",
      "Financial Risk Assessment System ROI:\n",
      "• Low: $199,900,000 portfolio, 1% default rate\n",
      "• Medium: $148,400,000 portfolio, 5% default rate\n",
      "• High: $101,900,000 portfolio, 15% default rate\n",
      "• Very_High: $49,800,000 portfolio, 35% default rate\n",
      "\n",
      "Financial Risk System Impact:\n",
      "• Baseline expected losses: $40,000,000\n",
      "• Improved expected losses: $29,493,800\n",
      "• Annual loss reduction: $10,506,200\n",
      "• System cost: $400,000\n",
      "• Net annual benefit: $10,106,200\n",
      "• ROI: 2527%\n",
      "\n",
      "E-commerce Customer Segmentation ROI:\n",
      "• Casual: 507,000 customers, $10,708,791 additional revenue\n",
      "• Regular: 293,000 customers, $30,943,547 additional revenue\n",
      "• Premium: 200,000 customers, $90,120,000 additional revenue\n",
      "\n",
      "E-commerce System Impact:\n",
      "• Total revenue improvement: $131,772,338\n",
      "• System cost: $200,000\n",
      "• Net annual benefit: $131,572,338\n",
      "• ROI: 65786%\n",
      "\n",
      "Combined Gradient Boosting Systems ROI:\n",
      "• Total investment: $600,000\n",
      "• Total annual benefits: $141,678,538\n",
      "• Combined ROI: 23613%\n",
      "• Payback period: 0.1 months\n",
      "\n",
      "Gradient Boosting Implementation Guidelines:\n",
      "• Start with learning_rate=0.1, adjust based on dataset size\n",
      "• Use max_depth=6 for complex problems, 3-4 for simple ones\n",
      "• Monitor train/validation curves to detect overfitting\n",
      "• Implement early stopping for optimal n_estimators\n",
      "• Consider XGBoost/LightGBM for large datasets\n",
      "• Use feature importance for interpretability and feature selection\n",
      "\n",
      "Cross-Reference Learning Path:\n",
      "• Foundation: Tier2_DecisionTree.ipynb (tree fundamentals)\n",
      "• Building On: Tier2_GradientBoosting.ipynb (basic boosting)\n",
      "• Comparison: Tier5_RandomForest.ipynb (bagging vs boosting)\n",
      "• Advanced: Advanced_EnsembleClassification.ipynb, Advanced_HyperparameterTuning.ipynb\n"
     ]
    }
   ],
   "source": [
    "# 5. BUSINESS INSIGHTS AND ROI ANALYSIS\n",
    "print(\" 5. BUSINESS INSIGHTS AND ROI ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Financial risk assessment business impact\n",
    "print(\"Financial Risk Assessment System ROI:\")\n",
    "loan_portfolio_value = 500_000_000 # $500M loan portfolio\n",
    "risk_assessment_accuracy = optimized_accuracy\n",
    "\n",
    "# Default rates by risk level\n",
    "default_rates = {\n",
    " 'Low': 0.01, # 1% default rate\n",
    " 'Medium': 0.05, # 5% default rate\n",
    " 'High': 0.15, # 15% default rate\n",
    " 'Very_High': 0.35 # 35% default rate\n",
    "}\n",
    "\n",
    "# Calculate risk-adjusted portfolio value\n",
    "portfolio_distribution = financial_df['risk_level'].value_counts(normalize=True).to_dict()\n",
    "expected_losses_baseline = 0\n",
    "expected_losses_improved = 0\n",
    "\n",
    "for risk_level, proportion in portfolio_distribution.items():\n",
    "    portfolio_segment = loan_portfolio_value * proportion\n",
    "    default_rate = default_rates[risk_level]\n",
    "\n",
    "    # Baseline (without ML): assume average default rate for all\n",
    "    avg_default_rate = 0.08 # 8% average default rate\n",
    "    baseline_loss = portfolio_segment * avg_default_rate\n",
    "    expected_losses_baseline += baseline_loss\n",
    "\n",
    "    # Improved (with ML): accurate risk assessment\n",
    "    # Assume we can reduce defaults by adjusting terms/rates based on risk\n",
    "    risk_adjustment_factor = 0.7 if risk_assessment_accuracy > 0.8 else 0.85\n",
    "    improved_loss = portfolio_segment * default_rate * risk_adjustment_factor\n",
    "    expected_losses_improved += improved_loss\n",
    "\n",
    "    print(f\"• {risk_level}: ${portfolio_segment:,.0f} portfolio, \"\n",
    "          f\"{default_rate*100:.0f}% default rate\")\n",
    "\n",
    "loss_reduction = expected_losses_baseline - expected_losses_improved\n",
    "system_cost = 400_000 # Annual system cost\n",
    "net_benefit = loss_reduction - system_cost\n",
    "roi = net_benefit / system_cost\n",
    "\n",
    "print(f\"\\nFinancial Risk System Impact:\")\n",
    "print(f\"• Baseline expected losses: ${expected_losses_baseline:,.0f}\")\n",
    "print(f\"• Improved expected losses: ${expected_losses_improved:,.0f}\")\n",
    "print(f\"• Annual loss reduction: ${loss_reduction:,.0f}\")\n",
    "print(f\"• System cost: ${system_cost:,.0f}\")\n",
    "print(f\"• Net annual benefit: ${net_benefit:,.0f}\")\n",
    "print(f\"• ROI: {roi*100:.0f}%\")\n",
    "\n",
    "# E-commerce customer segmentation ROI\n",
    "print(f\"\\nE-commerce Customer Segmentation ROI:\")\n",
    "total_customers_ecom = 1_000_000 # 1M customers\n",
    "segmentation_accuracy_ecom = ecom_accuracy\n",
    "\n",
    "# Revenue per customer by type\n",
    "customer_revenues = {\n",
    " 'Casual': 150, # Annual revenue per casual customer\n",
    " 'Regular': 450, # Annual revenue per regular customer\n",
    " 'Premium': 1200 # Annual revenue per premium customer\n",
    "}\n",
    "\n",
    "# Marketing efficiency improvements\n",
    "marketing_improvements = {\n",
    " 'Casual': 0.15, # 15% improvement in conversion\n",
    " 'Regular': 0.25, # 25% improvement\n",
    " 'Premium': 0.40 # 40% improvement for premium targeting\n",
    "}\n",
    "\n",
    "ecom_distribution = ecommerce_df['customer_type'].value_counts(normalize=True).to_dict()\n",
    "total_revenue_improvement_ecom = 0\n",
    "\n",
    "for customer_type, proportion in ecom_distribution.items():\n",
    "    customers_in_type = total_customers_ecom * proportion\n",
    "    correctly_segmented = customers_in_type * segmentation_accuracy_ecom\n",
    "\n",
    "    base_revenue = customer_revenues[customer_type]\n",
    "    improvement_factor = marketing_improvements[customer_type]\n",
    "    additional_revenue = correctly_segmented * base_revenue * improvement_factor\n",
    "\n",
    "    total_revenue_improvement_ecom += additional_revenue\n",
    "\n",
    "    print(f\"• {customer_type}: {customers_in_type:,.0f} customers, \"\n",
    "          f\"${additional_revenue:,.0f} additional revenue\")\n",
    "\n",
    "ecom_system_cost = 200_000 # Annual system cost\n",
    "ecom_net_benefit = total_revenue_improvement_ecom - ecom_system_cost\n",
    "ecom_roi = ecom_net_benefit / ecom_system_cost\n",
    "\n",
    "print(f\"\\nE-commerce System Impact:\")\n",
    "print(f\"• Total revenue improvement: ${total_revenue_improvement_ecom:,.0f}\")\n",
    "print(f\"• System cost: ${ecom_system_cost:,.0f}\")\n",
    "print(f\"• Net annual benefit: ${ecom_net_benefit:,.0f}\")\n",
    "print(f\"• ROI: {ecom_roi*100:.0f}%\")\n",
    "\n",
    "# Combined systems ROI\n",
    "total_investment = system_cost + ecom_system_cost\n",
    "total_benefits = net_benefit + ecom_net_benefit\n",
    "combined_roi = total_benefits / total_investment\n",
    "\n",
    "print(f\"\\nCombined Gradient Boosting Systems ROI:\")\n",
    "print(f\"• Total investment: ${total_investment:,.0f}\")\n",
    "print(f\"• Total annual benefits: ${total_benefits:,.0f}\")\n",
    "print(f\"• Combined ROI: {combined_roi*100:.0f}%\")\n",
    "print(f\"• Payback period: {total_investment/total_benefits*12:.1f} months\")\n",
    "\n",
    "# Implementation guidelines\n",
    "print(f\"\\nGradient Boosting Implementation Guidelines:\")\n",
    "print(f\"• Start with learning_rate=0.1, adjust based on dataset size\")\n",
    "print(f\"• Use max_depth=6 for complex problems, 3-4 for simple ones\")\n",
    "print(f\"• Monitor train/validation curves to detect overfitting\")\n",
    "print(f\"• Implement early stopping for optimal n_estimators\")\n",
    "print(f\"• Consider XGBoost/LightGBM for large datasets\")\n",
    "print(f\"• Use feature importance for interpretability and feature selection\")\n",
    "\n",
    "print(f\"\\nCross-Reference Learning Path:\")\n",
    "print(f\"• Foundation: Tier2_DecisionTree.ipynb (tree fundamentals)\")\n",
    "print(f\"• Building On: Tier2_GradientBoosting.ipynb (basic boosting)\")\n",
    "print(f\"• Comparison: Tier5_RandomForest.ipynb (bagging vs boosting)\")\n",
    "print(f\"• Advanced: Advanced_EnsembleClassification.ipynb, Advanced_HyperparameterTuning.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
