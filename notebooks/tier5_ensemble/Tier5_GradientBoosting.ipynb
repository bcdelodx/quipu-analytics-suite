{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 5: Gradient Boosting Classification\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Brandon Deloatch\n",
    "**Affiliation:** Quipu Research Labs, LLC\n",
    "**Date:** 2025-10-02\n",
    "**Version:** v1.3\n",
    "**License:** MIT\n",
    "**Notebook ID:** cd8c8259-f6c0-41ca-984d-c071e0066ad5\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "Brandon Deloatch, \"Tier 5: Gradient Boosting Classification,\" Quipu Research Labs, LLC, v1.3, 2025-10-02.\n",
    "\n",
    "Please cite this notebook if used or adapted in publications, presentations, or derivative work.\n",
    "\n",
    "---\n",
    "\n",
    "## Contributors / Acknowledgments\n",
    "- **Primary Author:** Brandon Deloatch (Quipu Research Labs, LLC)\n",
    "- **Institutional Support:** Quipu Research Labs, LLC - Advanced Analytics Division\n",
    "- **Technical Framework:** Built on scikit-learn, pandas, numpy, and plotly ecosystems\n",
    "- **Methodological Foundation:** Statistical learning principles and modern data science best practices\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "| Version | Date | Notes |\n",
    "|---------|------|-------|\n",
    "| v1.3 | 2025-10-02 | Enhanced professional formatting, comprehensive documentation, interactive visualizations |\n",
    "| v1.2 | 2024-09-15 | Updated analysis methods, improved data generation algorithms |\n",
    "| v1.0 | 2024-06-10 | Initial release with core analytical framework |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Dependencies\n",
    "- **Python:** 3.8+\n",
    "- **Core Libraries:** pandas 2.0+, numpy 1.24+, scikit-learn 1.3+\n",
    "- **Visualization:** plotly 5.0+, matplotlib 3.7+\n",
    "- **Statistical:** scipy 1.10+, statsmodels 0.14+\n",
    "- **Development:** jupyter-lab 4.0+, ipywidgets 8.0+\n",
    "\n",
    "> **Reproducibility Note:** Use requirements.txt or environment.yml for exact dependency matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Provenance\n",
    "| Dataset | Source | License | Notes |\n",
    "|---------|--------|---------|-------|\n",
    "| Synthetic Data | Generated in-notebook | MIT | Custom algorithms for realistic simulation |\n",
    "| Statistical Distributions | NumPy/SciPy | BSD-3-Clause | Standard library implementations |\n",
    "| ML Algorithms | Scikit-learn | BSD-3-Clause | Industry-standard implementations |\n",
    "| Visualization Schemas | Plotly | MIT | Interactive dashboard frameworks |\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Provenance Logs\n",
    "- **Created:** 2025-10-02\n",
    "- **Notebook ID:** cd8c8259-f6c0-41ca-984d-c071e0066ad5\n",
    "- **Execution Environment:** Jupyter Lab / VS Code\n",
    "- **Computational Requirements:** Standard laptop/workstation (2GB+ RAM recommended)\n",
    "\n",
    "> **Auto-tracking:** Execution metadata can be programmatically captured for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Disclaimer & Responsible Use\n",
    "This notebook is provided \"as-is\" for educational, research, and professional development purposes. Users assume full responsibility for any results, applications, or decisions derived from this analysis.\n",
    "\n",
    "**Professional Standards:**\n",
    "- Validate all results against domain expertise and additional data sources\n",
    "- Respect licensing and attribution requirements for all dependencies\n",
    "- Follow ethical guidelines for data analysis and algorithmic decision-making\n",
    "- Credit all methodological sources and derivative frameworks appropriately\n",
    "\n",
    "**Academic & Commercial Use:**\n",
    "- Permitted under MIT license with proper attribution\n",
    "- Suitable for educational curriculum and professional training\n",
    "- Appropriate for commercial adaptation with citation requirements\n",
    "- Recommended for reproducible research and transparent analytics\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, validation_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import advanced boosting libraries\n",
    "try:\n",
    " import xgboost as xgb\n",
    " XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    " XGBOOST_AVAILABLE = False\n",
    "\n",
    "try:\n",
    " import lightgbm as lgb\n",
    " LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    " LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    " import catboost as cb\n",
    " CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    " CATBOOST_AVAILABLE = False\n",
    "\n",
    "print(\" Tier 5: Gradient Boosting Classification - Libraries Loaded!\")\n",
    "print(\"=\"*62)\n",
    "print(\"Gradient Boosting Classification Techniques:\")\n",
    "print(\"• Gradient Boosting Classifier (scikit-learn)\")\n",
    "print(\"• AdaBoost (Adaptive Boosting)\")\n",
    "print(f\"• XGBoost: {'Available' if XGBOOST_AVAILABLE else 'Not Available'}\")\n",
    "print(f\"• LightGBM: {'Available' if LIGHTGBM_AVAILABLE else 'Not Available'}\")\n",
    "print(f\"• CatBoost: {'Available' if CATBOOST_AVAILABLE else 'Not Available'}\")\n",
    "print(\"• Learning rate optimization and regularization\")\n",
    "print(\"• Early stopping and cross-validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive boosting datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Financial risk assessment dataset\n",
    "def generate_financial_dataset(n_samples=5000):\n",
    " \"\"\"Generate realistic financial risk dataset.\"\"\"\n",
    "\n",
    " data = []\n",
    " risk_levels = ['Low', 'Medium', 'High', 'Very_High']\n",
    "\n",
    " for i in range(n_samples):\n",
    " # Risk level assignment\n",
    " risk_idx = np.random.choice(4, p=[0.4, 0.3, 0.2, 0.1])\n",
    " risk_level = risk_levels[risk_idx]\n",
    "\n",
    " # Generate features based on risk level\n",
    " if risk_level == 'Low':\n",
    " credit_score = np.random.normal(750, 50)\n",
    " debt_to_income = np.random.beta(2, 8) * 0.5 # Lower DTI\n",
    " payment_history = np.random.beta(9, 1) # Excellent history\n",
    " account_age = np.random.exponential(5) + 2 # Longer history\n",
    " income = np.random.lognormal(11, 0.3) # Higher income\n",
    "\n",
    " elif risk_level == 'Medium':\n",
    " credit_score = np.random.normal(680, 40)\n",
    " debt_to_income = np.random.beta(3, 5) * 0.6\n",
    " payment_history = np.random.beta(7, 2)\n",
    " account_age = np.random.exponential(3) + 1\n",
    " income = np.random.lognormal(10.5, 0.4)\n",
    "\n",
    " elif risk_level == 'High':\n",
    " credit_score = np.random.normal(620, 35)\n",
    " debt_to_income = np.random.beta(5, 3) * 0.8\n",
    " payment_history = np.random.beta(5, 4)\n",
    " account_age = np.random.exponential(2) + 0.5\n",
    " income = np.random.lognormal(10, 0.5)\n",
    "\n",
    " else: # Very_High\n",
    " credit_score = np.random.normal(550, 50)\n",
    " debt_to_income = np.random.beta(8, 2) * 1.0\n",
    " payment_history = np.random.beta(3, 7)\n",
    " account_age = np.random.exponential(1) + 0.1\n",
    " income = np.random.lognormal(9.5, 0.6)\n",
    "\n",
    " # Additional features\n",
    " data.append({\n",
    " 'customer_id': f'CUST_{i:06d}',\n",
    " 'credit_score': max(300, min(850, credit_score)), # Clamp to valid range\n",
    " 'debt_to_income_ratio': min(1.5, debt_to_income), # Cap at 150%\n",
    " 'payment_history_score': payment_history,\n",
    " 'account_age_years': account_age,\n",
    " 'annual_income': income,\n",
    " 'number_of_accounts': np.random.poisson(5) + 1,\n",
    " 'recent_inquiries': np.random.poisson(1),\n",
    " 'utilization_ratio': np.random.beta(2, 3),\n",
    " 'employment_length': np.random.exponential(3) + 0.5,\n",
    " 'homeowner': np.random.choice([0, 1], p=[0.7, 0.3]),\n",
    " 'risk_level': risk_level,\n",
    " 'risk_code': risk_idx\n",
    " })\n",
    "\n",
    " return pd.DataFrame(data)\n",
    "\n",
    "# 2. E-commerce customer dataset\n",
    "def generate_ecommerce_dataset(n_samples=4000):\n",
    " \"\"\"Generate e-commerce customer behavior dataset.\"\"\"\n",
    "\n",
    " data = []\n",
    " customer_types = ['Casual', 'Regular', 'Premium']\n",
    "\n",
    " for i in range(n_samples):\n",
    " # Customer type assignment\n",
    " type_idx = np.random.choice(3, p=[0.5, 0.3, 0.2])\n",
    " customer_type = customer_types[type_idx]\n",
    "\n",
    " # Generate features based on customer type\n",
    " if customer_type == 'Casual':\n",
    " monthly_purchases = np.random.poisson(1) + 1\n",
    " avg_order_value = np.random.lognormal(3, 0.8) # ~$25\n",
    " session_duration = np.random.exponential(5)\n",
    " page_views = np.random.poisson(8)\n",
    "\n",
    " elif customer_type == 'Regular':\n",
    " monthly_purchases = np.random.poisson(3) + 2\n",
    " avg_order_value = np.random.lognormal(4, 0.6) # ~$75\n",
    " session_duration = np.random.exponential(12)\n",
    " page_views = np.random.poisson(15)\n",
    "\n",
    " else: # Premium\n",
    " monthly_purchases = np.random.poisson(6) + 4\n",
    " avg_order_value = np.random.lognormal(5, 0.5) # ~$200\n",
    " session_duration = np.random.exponential(20)\n",
    " page_views = np.random.poisson(25)\n",
    "\n",
    " data.append({\n",
    " 'customer_id': f'ECOM_{i:06d}',\n",
    " 'monthly_purchases': monthly_purchases,\n",
    " 'avg_order_value': avg_order_value,\n",
    " 'session_duration_min': session_duration,\n",
    " 'page_views_per_session': page_views,\n",
    " 'cart_abandonment_rate': np.random.beta(3, 5),\n",
    " 'time_on_site_total': session_duration * monthly_purchases,\n",
    " 'mobile_usage_ratio': np.random.beta(4, 3),\n",
    " 'email_open_rate': np.random.beta(3, 4),\n",
    " 'social_media_referrals': np.random.poisson(2),\n",
    " 'customer_type': customer_type,\n",
    " 'type_code': type_idx\n",
    " })\n",
    "\n",
    " return pd.DataFrame(data)\n",
    "\n",
    "# Generate datasets\n",
    "financial_df = generate_financial_dataset()\n",
    "ecommerce_df = generate_ecommerce_dataset()\n",
    "\n",
    "print(\" Gradient Boosting Datasets Created:\")\n",
    "print(f\"Financial risk assessment: {financial_df.shape}\")\n",
    "print(f\"Risk distribution: {financial_df['risk_level'].value_counts().to_dict()}\")\n",
    "print(f\"\\nE-commerce customers: {ecommerce_df.shape}\")\n",
    "print(f\"Customer type distribution: {ecommerce_df['customer_type'].value_counts().to_dict()}\")\n",
    "\n",
    "# Show sample statistics\n",
    "print(f\"\\nFinancial Dataset - Credit Score by Risk Level:\")\n",
    "risk_stats = financial_df.groupby('risk_level')['credit_score'].agg(['mean', 'std'])\n",
    "for risk_level in ['Low', 'Medium', 'High', 'Very_High']:\n",
    " if risk_level in risk_stats.index:\n",
    " mean_score = risk_stats.loc[risk_level, 'mean']\n",
    " std_score = risk_stats.loc[risk_level, 'std']\n",
    " print(f\" {risk_level}: {mean_score:.0f} ± {std_score:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bf75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GRADIENT BOOSTING ALGORITHM COMPARISON\n",
    "print(\" 1. GRADIENT BOOSTING ALGORITHM COMPARISON\")\n",
    "print(\"=\"*44)\n",
    "\n",
    "# Prepare financial data\n",
    "financial_features = [col for col in financial_df.columns\n",
    " if col not in ['customer_id', 'risk_level', 'risk_code']]\n",
    "X_financial = financial_df[financial_features].values\n",
    "y_financial = financial_df['risk_code'].values\n",
    "\n",
    "# Split the data\n",
    "X_fin_train, X_fin_test, y_fin_train, y_fin_test = train_test_split(\n",
    " X_financial, y_financial, test_size=0.2, random_state=42, stratify=y_financial\n",
    ")\n",
    "\n",
    "# Initialize boosting algorithms\n",
    "boosting_algorithms = {}\n",
    "results = {}\n",
    "\n",
    "# 1. Scikit-learn Gradient Boosting\n",
    "gb_sklearn = GradientBoostingClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=0.1,\n",
    " max_depth=6,\n",
    " random_state=42\n",
    ")\n",
    "boosting_algorithms['Gradient Boosting'] = gb_sklearn\n",
    "\n",
    "# 2. AdaBoost\n",
    "ada_boost = AdaBoostClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=1.0,\n",
    " random_state=42\n",
    ")\n",
    "boosting_algorithms['AdaBoost'] = ada_boost\n",
    "\n",
    "# 3. XGBoost (if available)\n",
    "if XGBOOST_AVAILABLE:\n",
    " xgb_classifier = xgb.XGBClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=0.1,\n",
    " max_depth=6,\n",
    " random_state=42,\n",
    " eval_metric='mlogloss'\n",
    " )\n",
    " boosting_algorithms['XGBoost'] = xgb_classifier\n",
    "\n",
    "# 4. LightGBM (if available)\n",
    "if LIGHTGBM_AVAILABLE:\n",
    " lgb_classifier = lgb.LGBMClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=0.1,\n",
    " max_depth=6,\n",
    " random_state=42,\n",
    " verbose=-1\n",
    " )\n",
    " boosting_algorithms['LightGBM'] = lgb_classifier\n",
    "\n",
    "# 5. CatBoost (if available)\n",
    "if CATBOOST_AVAILABLE:\n",
    " cat_classifier = cb.CatBoostClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=0.1,\n",
    " max_depth=6,\n",
    " random_state=42,\n",
    " verbose=False\n",
    " )\n",
    " boosting_algorithms['CatBoost'] = cat_classifier\n",
    "\n",
    "# Train and evaluate all algorithms\n",
    "print(\"Training boosting algorithms...\")\n",
    "for name, algorithm in boosting_algorithms.items():\n",
    " print(f\"Training {name}...\")\n",
    "\n",
    " # Train the algorithm\n",
    " algorithm.fit(X_fin_train, y_fin_train)\n",
    "\n",
    " # Predictions\n",
    " y_pred = algorithm.predict(X_fin_test)\n",
    " y_pred_proba = algorithm.predict_proba(X_fin_test)\n",
    "\n",
    " # Performance metrics\n",
    " accuracy = accuracy_score(y_fin_test, y_pred)\n",
    " cv_scores = cross_val_score(algorithm, X_fin_train, y_fin_train, cv=3)\n",
    "\n",
    " results[name] = {\n",
    " 'accuracy': accuracy,\n",
    " 'cv_mean': cv_scores.mean(),\n",
    " 'cv_std': cv_scores.std(),\n",
    " 'y_pred': y_pred,\n",
    " 'y_pred_proba': y_pred_proba,\n",
    " 'model': algorithm\n",
    " }\n",
    "\n",
    " print(f\" Accuracy: {accuracy:.3f} | CV: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\nBoosting Algorithm Performance Comparison:\")\n",
    "print(\"Algorithm Accuracy CV Mean CV Std\")\n",
    "print(\"-\" * 50)\n",
    "for name, metrics in results.items():\n",
    " print(f\"{name:<15} {metrics['accuracy']:.3f} {metrics['cv_mean']:.3f} {metrics['cv_std']:.3f}\")\n",
    "\n",
    "# Find best performer\n",
    "best_algorithm = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "best_accuracy = results[best_algorithm]['accuracy']\n",
    "print(f\"\\nBest performing algorithm: {best_algorithm} (Accuracy: {best_accuracy:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. HYPERPARAMETER OPTIMIZATION AND LEARNING CURVES\n",
    "print(\" 2. HYPERPARAMETER OPTIMIZATION AND LEARNING CURVES\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Focus on Gradient Boosting for detailed analysis\n",
    "print(\"Detailed Gradient Boosting Analysis:\")\n",
    "\n",
    "# Learning rate analysis\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "lr_scores = []\n",
    "\n",
    "print(\"\\nLearning Rate Optimization:\")\n",
    "for lr in learning_rates:\n",
    " gb_temp = GradientBoostingClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=lr,\n",
    " max_depth=6,\n",
    " random_state=42\n",
    " )\n",
    "\n",
    " cv_scores = cross_val_score(gb_temp, X_fin_train, y_fin_train, cv=3)\n",
    " lr_scores.append(cv_scores.mean())\n",
    "\n",
    " print(f\"Learning Rate {lr:.2f}: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "best_lr = learning_rates[np.argmax(lr_scores)]\n",
    "print(f\"Best learning rate: {best_lr} (Score: {max(lr_scores):.3f})\")\n",
    "\n",
    "# Max depth analysis\n",
    "max_depths = [3, 4, 5, 6, 7, 8]\n",
    "depth_scores = []\n",
    "\n",
    "print(f\"\\nMax Depth Optimization:\")\n",
    "for depth in max_depths:\n",
    " gb_temp = GradientBoostingClassifier(\n",
    " n_estimators=100,\n",
    " learning_rate=best_lr,\n",
    " max_depth=depth,\n",
    " random_state=42\n",
    " )\n",
    "\n",
    " cv_scores = cross_val_score(gb_temp, X_fin_train, y_fin_train, cv=3)\n",
    " depth_scores.append(cv_scores.mean())\n",
    "\n",
    " print(f\"Max Depth {depth}: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "best_depth = max_depths[np.argmax(depth_scores)]\n",
    "print(f\"Best max depth: {best_depth} (Score: {max(depth_scores):.3f})\")\n",
    "\n",
    "# N_estimators analysis with early stopping simulation\n",
    "n_estimators_range = [25, 50, 75, 100, 150, 200, 300]\n",
    "train_scores_est = []\n",
    "test_scores_est = []\n",
    "\n",
    "print(f\"\\nN_estimators Analysis:\")\n",
    "for n_est in n_estimators_range:\n",
    " gb_temp = GradientBoostingClassifier(\n",
    " n_estimators=n_est,\n",
    " learning_rate=best_lr,\n",
    " max_depth=best_depth,\n",
    " random_state=42\n",
    " )\n",
    "\n",
    " gb_temp.fit(X_fin_train, y_fin_train)\n",
    "\n",
    " train_score = gb_temp.score(X_fin_train, y_fin_train)\n",
    " test_score = gb_temp.score(X_fin_test, y_fin_test)\n",
    "\n",
    " train_scores_est.append(train_score)\n",
    " test_scores_est.append(test_score)\n",
    "\n",
    " print(f\"N_estimators {n_est:3d}: Train={train_score:.3f}, Test={test_score:.3f}\")\n",
    "\n",
    "# Find optimal n_estimators (where test score peaks)\n",
    "best_n_est_idx = np.argmax(test_scores_est)\n",
    "best_n_est = n_estimators_range[best_n_est_idx]\n",
    "print(f\"Best n_estimators: {best_n_est} (Test Score: {test_scores_est[best_n_est_idx]:.3f})\")\n",
    "\n",
    "# Train final optimized model\n",
    "gb_optimized = GradientBoostingClassifier(\n",
    " n_estimators=best_n_est,\n",
    " learning_rate=best_lr,\n",
    " max_depth=best_depth,\n",
    " random_state=42\n",
    ")\n",
    "\n",
    "gb_optimized.fit(X_fin_train, y_fin_train)\n",
    "optimized_accuracy = gb_optimized.score(X_fin_test, y_fin_test)\n",
    "\n",
    "print(f\"\\nOptimized Gradient Boosting Performance:\")\n",
    "print(f\"Final accuracy: {optimized_accuracy:.3f}\")\n",
    "print(f\"Improvement over baseline: {optimized_accuracy - results['Gradient Boosting']['accuracy']:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e75295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. FEATURE IMPORTANCE AND MODEL INTERPRETATION\n",
    "print(\" 3. FEATURE IMPORTANCE AND MODEL INTERPRETATION\")\n",
    "print(\"=\"*51)\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance_gb = pd.DataFrame({\n",
    " 'feature': financial_features,\n",
    " 'importance': gb_optimized.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Gradient Boosting):\")\n",
    "for i, (idx, row) in enumerate(feature_importance_gb.head(10).iterrows()):\n",
    " print(f\"{i+1:2d}. {row['feature']:<25}: {row['importance']:.4f}\")\n",
    "\n",
    "# Partial dependence analysis (simplified)\n",
    "print(f\"\\nTop 3 Feature Analysis:\")\n",
    "top_features = feature_importance_gb.head(3)['feature'].tolist()\n",
    "\n",
    "for feature in top_features:\n",
    " feature_idx = financial_features.index(feature)\n",
    " feature_values = X_fin_test[:, feature_idx]\n",
    "\n",
    " # Calculate average prediction for different feature value ranges\n",
    " percentiles = [0, 25, 50, 75, 100]\n",
    " thresholds = np.percentile(feature_values, percentiles)\n",
    "\n",
    " print(f\"\\n{feature} impact on risk prediction:\")\n",
    " for i in range(len(thresholds) - 1):\n",
    " mask = (feature_values >= thresholds[i]) & (feature_values < thresholds[i+1])\n",
    " if np.sum(mask) > 0:\n",
    " avg_risk = np.mean(y_fin_test[mask])\n",
    " print(f\" {thresholds[i]:.2f} - {thresholds[i+1]:.2f}: Avg risk level {avg_risk:.2f}\")\n",
    "\n",
    "# E-commerce dataset analysis\n",
    "print(f\"\\nE-commerce Customer Classification:\")\n",
    "ecommerce_features = [col for col in ecommerce_df.columns\n",
    " if col not in ['customer_id', 'customer_type', 'type_code']]\n",
    "X_ecom = ecommerce_df[ecommerce_features].values\n",
    "y_ecom = ecommerce_df['type_code'].values\n",
    "\n",
    "X_ecom_train, X_ecom_test, y_ecom_train, y_ecom_test = train_test_split(\n",
    " X_ecom, y_ecom, test_size=0.2, random_state=42, stratify=y_ecom\n",
    ")\n",
    "\n",
    "# Train model for e-commerce data\n",
    "gb_ecommerce = GradientBoostingClassifier(\n",
    " n_estimators=best_n_est,\n",
    " learning_rate=best_lr,\n",
    " max_depth=best_depth,\n",
    " random_state=42\n",
    ")\n",
    "\n",
    "gb_ecommerce.fit(X_ecom_train, y_ecom_train)\n",
    "ecom_accuracy = gb_ecommerce.score(X_ecom_test, y_ecom_test)\n",
    "\n",
    "print(f\"E-commerce classification accuracy: {ecom_accuracy:.3f}\")\n",
    "\n",
    "# E-commerce feature importance\n",
    "ecom_feature_importance = pd.DataFrame({\n",
    " 'feature': ecommerce_features,\n",
    " 'importance': gb_ecommerce.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nE-commerce Feature Importance:\")\n",
    "for i, (idx, row) in enumerate(ecom_feature_importance.head(8).iterrows()):\n",
    " print(f\"{i+1:2d}. {row['feature']:<25}: {row['importance']:.4f}\")\n",
    "\n",
    "# Learning curve analysis\n",
    "print(f\"\\nLearning Curve Analysis:\")\n",
    "train_sizes = [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "lc_train_scores = []\n",
    "lc_test_scores = []\n",
    "\n",
    "for size in train_sizes:\n",
    " n_samples = int(len(X_fin_train) * size)\n",
    " X_subset = X_fin_train[:n_samples]\n",
    " y_subset = y_fin_train[:n_samples]\n",
    "\n",
    " gb_temp = GradientBoostingClassifier(\n",
    " n_estimators=best_n_est,\n",
    " learning_rate=best_lr,\n",
    " max_depth=best_depth,\n",
    " random_state=42\n",
    " )\n",
    "\n",
    " gb_temp.fit(X_subset, y_subset)\n",
    "\n",
    " train_score = gb_temp.score(X_subset, y_subset)\n",
    " test_score = gb_temp.score(X_fin_test, y_fin_test)\n",
    "\n",
    " lc_train_scores.append(train_score)\n",
    " lc_test_scores.append(test_score)\n",
    "\n",
    " print(f\"Training size {size*100:3.0f}%: Train={train_score:.3f}, Test={test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. COMPREHENSIVE GRADIENT BOOSTING VISUALIZATION DASHBOARD\n",
    "print(\" 4. COMPREHENSIVE GRADIENT BOOSTING VISUALIZATION DASHBOARD\")\n",
    "print(\"=\"*66)\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "fig = make_subplots(\n",
    " rows=3, cols=2,\n",
    " subplot_titles=[\n",
    " 'Algorithm Performance Comparison',\n",
    " 'Hyperparameter Optimization: Learning Rate',\n",
    " 'N_estimators vs Performance (Overfitting Analysis)',\n",
    " 'Feature Importance: Financial Risk Factors',\n",
    " 'Learning Curves: Training Size vs Performance',\n",
    " 'Confusion Matrix: Risk Level Prediction'\n",
    " ],\n",
    " specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    " [{\"secondary_y\": False}, {\"type\": \"heatmap\"}]]\n",
    ")\n",
    "\n",
    "# 1. Algorithm comparison\n",
    "algorithms = list(results.keys())\n",
    "accuracies = [results[alg]['accuracy'] for alg in algorithms]\n",
    "cv_means = [results[alg]['cv_mean'] for alg in algorithms]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=algorithms,\n",
    " y=accuracies,\n",
    " name='Test Accuracy',\n",
    " marker_color='lightblue',\n",
    " text=[f'{acc:.3f}' for acc in accuracies],\n",
    " textposition='auto'\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=algorithms,\n",
    " y=cv_means,\n",
    " name='CV Mean',\n",
    " marker_color='lightcoral',\n",
    " text=[f'{cv:.3f}' for cv in cv_means],\n",
    " textposition='auto'\n",
    " ),\n",
    " row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Learning rate optimization\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=learning_rates,\n",
    " y=lr_scores,\n",
    " mode='lines+markers',\n",
    " name='Learning Rate',\n",
    " line=dict(color='blue', width=3),\n",
    " marker=dict(size=8)\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# Highlight best learning rate\n",
    "best_lr_idx = np.argmax(lr_scores)\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=[learning_rates[best_lr_idx]],\n",
    " y=[lr_scores[best_lr_idx]],\n",
    " mode='markers',\n",
    " name='Best LR',\n",
    " marker=dict(color='red', size=12, symbol='star')\n",
    " ),\n",
    " row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. N_estimators analysis\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=train_scores_est,\n",
    " mode='lines+markers',\n",
    " name='Training Score',\n",
    " line=dict(color='blue')\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=n_estimators_range,\n",
    " y=test_scores_est,\n",
    " mode='lines+markers',\n",
    " name='Test Score',\n",
    " line=dict(color='red')\n",
    " ),\n",
    " row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Feature importance\n",
    "top_features_plot = feature_importance_gb.head(12)\n",
    "fig.add_trace(\n",
    " go.Bar(\n",
    " x=top_features_plot['importance'],\n",
    " y=top_features_plot['feature'],\n",
    " orientation='h',\n",
    " marker_color='green',\n",
    " name='Feature Importance'\n",
    " ),\n",
    " row=2, col=2\n",
    ")\n",
    "\n",
    "# 5. Learning curves\n",
    "training_sizes_pct = [size * 100 for size in train_sizes]\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=training_sizes_pct,\n",
    " y=lc_train_scores,\n",
    " mode='lines+markers',\n",
    " name='Training Score',\n",
    " line=dict(color='blue')\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    " go.Scatter(\n",
    " x=training_sizes_pct,\n",
    " y=lc_test_scores,\n",
    " mode='lines+markers',\n",
    " name='Validation Score',\n",
    " line=dict(color='red')\n",
    " ),\n",
    " row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Confusion matrix\n",
    "y_pred_final = gb_optimized.predict(X_fin_test)\n",
    "cm_financial = confusion_matrix(y_fin_test, y_pred_final)\n",
    "risk_levels = ['Low', 'Medium', 'High', 'Very_High']\n",
    "\n",
    "fig.add_trace(\n",
    " go.Heatmap(\n",
    " z=cm_financial,\n",
    " x=risk_levels,\n",
    " y=risk_levels,\n",
    " colorscale='Blues',\n",
    " text=cm_financial,\n",
    " texttemplate='%{text}',\n",
    " hovertemplate='Predicted: %{x}<br>Actual: %{y}<br>Count: %{z}<extra></extra>'\n",
    " ),\n",
    " row=3, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    " height=1200,\n",
    " title=\"Gradient Boosting Classification - Comprehensive Analysis Dashboard\",\n",
    " showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Algorithm\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Learning Rate\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Number of Estimators\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Feature Importance\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"Training Set Size (%)\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"Predicted Risk Level\", row=3, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"CV Score\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Feature\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Actual Risk Level\", row=3, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. BUSINESS INSIGHTS AND ROI ANALYSIS\n",
    "print(\" 5. BUSINESS INSIGHTS AND ROI ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Financial risk assessment business impact\n",
    "print(\"Financial Risk Assessment System ROI:\")\n",
    "loan_portfolio_value = 500_000_000 # $500M loan portfolio\n",
    "risk_assessment_accuracy = optimized_accuracy\n",
    "\n",
    "# Default rates by risk level\n",
    "default_rates = {\n",
    " 'Low': 0.01, # 1% default rate\n",
    " 'Medium': 0.05, # 5% default rate\n",
    " 'High': 0.15, # 15% default rate\n",
    " 'Very_High': 0.35 # 35% default rate\n",
    "}\n",
    "\n",
    "# Calculate risk-adjusted portfolio value\n",
    "portfolio_distribution = financial_df['risk_level'].value_counts(normalize=True).to_dict()\n",
    "expected_losses_baseline = 0\n",
    "expected_losses_improved = 0\n",
    "\n",
    "for risk_level, proportion in portfolio_distribution.items():\n",
    " portfolio_segment = loan_portfolio_value * proportion\n",
    " default_rate = default_rates[risk_level]\n",
    "\n",
    " # Baseline (without ML): assume average default rate for all\n",
    " avg_default_rate = 0.08 # 8% average default rate\n",
    " baseline_loss = portfolio_segment * avg_default_rate\n",
    " expected_losses_baseline += baseline_loss\n",
    "\n",
    " # Improved (with ML): accurate risk assessment\n",
    " # Assume we can reduce defaults by adjusting terms/rates based on risk\n",
    " risk_adjustment_factor = 0.7 if risk_assessment_accuracy > 0.8 else 0.85\n",
    " improved_loss = portfolio_segment * default_rate * risk_adjustment_factor\n",
    " expected_losses_improved += improved_loss\n",
    "\n",
    " print(f\"• {risk_level}: ${portfolio_segment:,.0f} portfolio, \"\n",
    " f\"{default_rate*100:.0f}% default rate\")\n",
    "\n",
    "loss_reduction = expected_losses_baseline - expected_losses_improved\n",
    "system_cost = 400_000 # Annual system cost\n",
    "net_benefit = loss_reduction - system_cost\n",
    "roi = net_benefit / system_cost\n",
    "\n",
    "print(f\"\\nFinancial Risk System Impact:\")\n",
    "print(f\"• Baseline expected losses: ${expected_losses_baseline:,.0f}\")\n",
    "print(f\"• Improved expected losses: ${expected_losses_improved:,.0f}\")\n",
    "print(f\"• Annual loss reduction: ${loss_reduction:,.0f}\")\n",
    "print(f\"• System cost: ${system_cost:,.0f}\")\n",
    "print(f\"• Net annual benefit: ${net_benefit:,.0f}\")\n",
    "print(f\"• ROI: {roi*100:.0f}%\")\n",
    "\n",
    "# E-commerce customer segmentation ROI\n",
    "print(f\"\\nE-commerce Customer Segmentation ROI:\")\n",
    "total_customers_ecom = 1_000_000 # 1M customers\n",
    "segmentation_accuracy_ecom = ecom_accuracy\n",
    "\n",
    "# Revenue per customer by type\n",
    "customer_revenues = {\n",
    " 'Casual': 150, # Annual revenue per casual customer\n",
    " 'Regular': 450, # Annual revenue per regular customer\n",
    " 'Premium': 1200 # Annual revenue per premium customer\n",
    "}\n",
    "\n",
    "# Marketing efficiency improvements\n",
    "marketing_improvements = {\n",
    " 'Casual': 0.15, # 15% improvement in conversion\n",
    " 'Regular': 0.25, # 25% improvement\n",
    " 'Premium': 0.40 # 40% improvement for premium targeting\n",
    "}\n",
    "\n",
    "ecom_distribution = ecommerce_df['customer_type'].value_counts(normalize=True).to_dict()\n",
    "total_revenue_improvement_ecom = 0\n",
    "\n",
    "for customer_type, proportion in ecom_distribution.items():\n",
    " customers_in_type = total_customers_ecom * proportion\n",
    " correctly_segmented = customers_in_type * segmentation_accuracy_ecom\n",
    "\n",
    " base_revenue = customer_revenues[customer_type]\n",
    " improvement_factor = marketing_improvements[customer_type]\n",
    " additional_revenue = correctly_segmented * base_revenue * improvement_factor\n",
    "\n",
    " total_revenue_improvement_ecom += additional_revenue\n",
    "\n",
    " print(f\"• {customer_type}: {customers_in_type:,.0f} customers, \"\n",
    " f\"${additional_revenue:,.0f} additional revenue\")\n",
    "\n",
    "ecom_system_cost = 200_000 # Annual system cost\n",
    "ecom_net_benefit = total_revenue_improvement_ecom - ecom_system_cost\n",
    "ecom_roi = ecom_net_benefit / ecom_system_cost\n",
    "\n",
    "print(f\"\\nE-commerce System Impact:\")\n",
    "print(f\"• Total revenue improvement: ${total_revenue_improvement_ecom:,.0f}\")\n",
    "print(f\"• System cost: ${ecom_system_cost:,.0f}\")\n",
    "print(f\"• Net annual benefit: ${ecom_net_benefit:,.0f}\")\n",
    "print(f\"• ROI: {ecom_roi*100:.0f}%\")\n",
    "\n",
    "# Combined systems ROI\n",
    "total_investment = system_cost + ecom_system_cost\n",
    "total_benefits = net_benefit + ecom_net_benefit\n",
    "combined_roi = total_benefits / total_investment\n",
    "\n",
    "print(f\"\\nCombined Gradient Boosting Systems ROI:\")\n",
    "print(f\"• Total investment: ${total_investment:,.0f}\")\n",
    "print(f\"• Total annual benefits: ${total_benefits:,.0f}\")\n",
    "print(f\"• Combined ROI: {combined_roi*100:.0f}%\")\n",
    "print(f\"• Payback period: {total_investment/total_benefits*12:.1f} months\")\n",
    "\n",
    "# Implementation guidelines\n",
    "print(f\"\\nGradient Boosting Implementation Guidelines:\")\n",
    "print(f\"• Start with learning_rate=0.1, adjust based on dataset size\")\n",
    "print(f\"• Use max_depth=6 for complex problems, 3-4 for simple ones\")\n",
    "print(f\"• Monitor train/validation curves to detect overfitting\")\n",
    "print(f\"• Implement early stopping for optimal n_estimators\")\n",
    "print(f\"• Consider XGBoost/LightGBM for large datasets\")\n",
    "print(f\"• Use feature importance for interpretability and feature selection\")\n",
    "\n",
    "print(f\"\\nCross-Reference Learning Path:\")\n",
    "print(f\"• Foundation: Tier2_DecisionTree.ipynb (tree fundamentals)\")\n",
    "print(f\"• Building On: Tier2_GradientBoosting.ipynb (basic boosting)\")\n",
    "print(f\"• Comparison: Tier5_RandomForest.ipynb (bagging vs boosting)\")\n",
    "print(f\"• Advanced: Advanced_EnsembleClassification.ipynb, Advanced_HyperparameterTuning.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}